{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288be4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from predictor import ThroughputPredictor\n",
    "from trustee import RegressionTrustee, ClassificationTrustee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3442611",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b66590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE_MS = 100 # Number of milliseconds for each time window; TCP_INFO statistics are aggregated over this period\n",
    "TIME_SERIES_LENGTH = 5 # Number of time series steps to consider for prediction\n",
    "\n",
    "def load_dataset(file_path):\n",
    "  df = pd.read_csv(file_path).dropna()\n",
    "  df.sort_values(by=['TestID', 'ElapsedTime'], inplace=True)\n",
    "  \n",
    "  df['k'] = df['ElapsedTime'] // (WINDOW_SIZE_MS * 1000)\n",
    "  agg_df = df.groupby(['TestID', 'k'], as_index=False).agg({\n",
    "    'ElapsedTime': 'max',\n",
    "    'BusyTime': 'max',\n",
    "    'BytesSent': 'max',\n",
    "    'BytesAcked': 'max',\n",
    "    'BytesRetrans': 'max',\n",
    "    'RTT': 'mean',\n",
    "    'RTTVar': 'mean',\n",
    "    'MinRTT': 'min',\n",
    "    'RWndLimited': 'max',\n",
    "    'SndBufLimited': 'max',\n",
    "    'FinalSpeed': 'max',\n",
    "  }, inplace=True)\n",
    "  \n",
    "  for i in range(1, TIME_SERIES_LENGTH):\n",
    "    agg_df[f'ElapsedTime_{i}'] = agg_df.groupby('TestID')['ElapsedTime'].shift(i).fillna(0)\n",
    "    agg_df[f'BusyTime_{i}'] = agg_df.groupby('TestID')['BusyTime'].shift(i).fillna(0)\n",
    "    agg_df[f'BytesSent_{i}'] = agg_df.groupby('TestID')['BytesSent'].shift(i).fillna(0)\n",
    "    agg_df[f'BytesAcked_{i}'] = agg_df.groupby('TestID')['BytesAcked'].shift(i).fillna(0)\n",
    "    agg_df[f'BytesRetrans_{i}'] = agg_df.groupby('TestID')['BytesRetrans'].shift(i).fillna(0)\n",
    "    agg_df[f'RTT_{i}'] = agg_df.groupby('TestID')['RTT'].shift(i).fillna(0)\n",
    "    agg_df[f'RTTVar_{i}'] = agg_df.groupby('TestID')['RTTVar'].shift(i).fillna(0)\n",
    "    agg_df[f'RWndLimited_{i}'] = agg_df.groupby('TestID')['RWndLimited'].shift(i).fillna(0)\n",
    "    agg_df[f'SndBufLimited_{i}'] = agg_df.groupby('TestID')['SndBufLimited'].shift(i).fillna(0)\n",
    "    \n",
    "  agg_df = agg_df.groupby('TestID').apply(lambda x: x.iloc[TIME_SERIES_LENGTH:]).reset_index(drop=True)\n",
    "  agg_df.drop(columns=['TestID'], inplace=True)\n",
    "  labels = agg_df.pop('FinalSpeed')\n",
    "  \n",
    "  return agg_df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522fd1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>ElapsedTime</th>\n",
       "      <th>BusyTime</th>\n",
       "      <th>BytesSent</th>\n",
       "      <th>BytesAcked</th>\n",
       "      <th>BytesRetrans</th>\n",
       "      <th>RTT</th>\n",
       "      <th>RTTVar</th>\n",
       "      <th>MinRTT</th>\n",
       "      <th>RWndLimited</th>\n",
       "      <th>...</th>\n",
       "      <th>SndBufLimited_3</th>\n",
       "      <th>ElapsedTime_4</th>\n",
       "      <th>BusyTime_4</th>\n",
       "      <th>BytesSent_4</th>\n",
       "      <th>BytesAcked_4</th>\n",
       "      <th>BytesRetrans_4</th>\n",
       "      <th>RTT_4</th>\n",
       "      <th>RTTVar_4</th>\n",
       "      <th>RWndLimited_4</th>\n",
       "      <th>SndBufLimited_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>579234</td>\n",
       "      <td>575000</td>\n",
       "      <td>212785</td>\n",
       "      <td>93673</td>\n",
       "      <td>1418</td>\n",
       "      <td>121797.25</td>\n",
       "      <td>9535.5</td>\n",
       "      <td>100816</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124037.0</td>\n",
       "      <td>119000.0</td>\n",
       "      <td>1.284700e+04</td>\n",
       "      <td>1.547000e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101050.00</td>\n",
       "      <td>28799.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>682398</td>\n",
       "      <td>678000</td>\n",
       "      <td>248235</td>\n",
       "      <td>93673</td>\n",
       "      <td>2836</td>\n",
       "      <td>163404.60</td>\n",
       "      <td>16453.2</td>\n",
       "      <td>100816</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288933.0</td>\n",
       "      <td>284000.0</td>\n",
       "      <td>4.120700e+04</td>\n",
       "      <td>1.284700e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106504.25</td>\n",
       "      <td>16665.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>786233</td>\n",
       "      <td>782000</td>\n",
       "      <td>279431</td>\n",
       "      <td>93673</td>\n",
       "      <td>5672</td>\n",
       "      <td>221776.00</td>\n",
       "      <td>23703.8</td>\n",
       "      <td>100816</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>392226.0</td>\n",
       "      <td>388000.0</td>\n",
       "      <td>8.091100e+04</td>\n",
       "      <td>4.120700e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109021.75</td>\n",
       "      <td>6359.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>891832</td>\n",
       "      <td>887000</td>\n",
       "      <td>336151</td>\n",
       "      <td>133377</td>\n",
       "      <td>8508</td>\n",
       "      <td>274418.60</td>\n",
       "      <td>9798.4</td>\n",
       "      <td>100816</td>\n",
       "      <td>15000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496415.0</td>\n",
       "      <td>492000.0</td>\n",
       "      <td>1.603190e+05</td>\n",
       "      <td>7.240300e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109827.20</td>\n",
       "      <td>4563.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>998020</td>\n",
       "      <td>993000</td>\n",
       "      <td>402797</td>\n",
       "      <td>205695</td>\n",
       "      <td>12762</td>\n",
       "      <td>216210.20</td>\n",
       "      <td>11117.8</td>\n",
       "      <td>100816</td>\n",
       "      <td>74000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579234.0</td>\n",
       "      <td>575000.0</td>\n",
       "      <td>2.127850e+05</td>\n",
       "      <td>9.367300e+04</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>121797.25</td>\n",
       "      <td>9535.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>92</td>\n",
       "      <td>9293469</td>\n",
       "      <td>9293000</td>\n",
       "      <td>1412073970</td>\n",
       "      <td>1304071780</td>\n",
       "      <td>106774202</td>\n",
       "      <td>1891.60</td>\n",
       "      <td>296.6</td>\n",
       "      <td>161</td>\n",
       "      <td>4146000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8787459.0</td>\n",
       "      <td>8788000.0</td>\n",
       "      <td>1.312769e+09</td>\n",
       "      <td>1.214030e+09</td>\n",
       "      <td>98098878.0</td>\n",
       "      <td>1579.80</td>\n",
       "      <td>218.40</td>\n",
       "      <td>3952000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>93</td>\n",
       "      <td>9314891</td>\n",
       "      <td>9315000</td>\n",
       "      <td>1418576562</td>\n",
       "      <td>1307920232</td>\n",
       "      <td>107103178</td>\n",
       "      <td>2422.00</td>\n",
       "      <td>843.0</td>\n",
       "      <td>161</td>\n",
       "      <td>4149000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8808776.0</td>\n",
       "      <td>8809000.0</td>\n",
       "      <td>1.316681e+09</td>\n",
       "      <td>1.214769e+09</td>\n",
       "      <td>98189630.0</td>\n",
       "      <td>2084.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>3959000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>95</td>\n",
       "      <td>9583439</td>\n",
       "      <td>9584000</td>\n",
       "      <td>1436655000</td>\n",
       "      <td>1324500906</td>\n",
       "      <td>108148244</td>\n",
       "      <td>1974.50</td>\n",
       "      <td>707.0</td>\n",
       "      <td>161</td>\n",
       "      <td>4364000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9099539.0</td>\n",
       "      <td>9100000.0</td>\n",
       "      <td>1.349074e+09</td>\n",
       "      <td>1.248059e+09</td>\n",
       "      <td>100265582.0</td>\n",
       "      <td>2280.40</td>\n",
       "      <td>344.00</td>\n",
       "      <td>4146000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>97</td>\n",
       "      <td>9789169</td>\n",
       "      <td>9789000</td>\n",
       "      <td>1447534832</td>\n",
       "      <td>1337624014</td>\n",
       "      <td>108847318</td>\n",
       "      <td>2000.50</td>\n",
       "      <td>429.0</td>\n",
       "      <td>161</td>\n",
       "      <td>4534000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9185733.0</td>\n",
       "      <td>9186000.0</td>\n",
       "      <td>1.375302e+09</td>\n",
       "      <td>1.271397e+09</td>\n",
       "      <td>103331298.0</td>\n",
       "      <td>1498.75</td>\n",
       "      <td>497.00</td>\n",
       "      <td>4146000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>98</td>\n",
       "      <td>9896221</td>\n",
       "      <td>9897000</td>\n",
       "      <td>1476319542</td>\n",
       "      <td>1361344318</td>\n",
       "      <td>111338744</td>\n",
       "      <td>1598.80</td>\n",
       "      <td>307.2</td>\n",
       "      <td>161</td>\n",
       "      <td>4556000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9293469.0</td>\n",
       "      <td>9293000.0</td>\n",
       "      <td>1.412074e+09</td>\n",
       "      <td>1.304072e+09</td>\n",
       "      <td>106774202.0</td>\n",
       "      <td>1891.60</td>\n",
       "      <td>296.60</td>\n",
       "      <td>4146000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22534 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  ElapsedTime  BusyTime   BytesSent  BytesAcked  BytesRetrans  \\\n",
       "0       5       579234    575000      212785       93673          1418   \n",
       "1       6       682398    678000      248235       93673          2836   \n",
       "2       7       786233    782000      279431       93673          5672   \n",
       "3       8       891832    887000      336151      133377          8508   \n",
       "4       9       998020    993000      402797      205695         12762   \n",
       "...    ..          ...       ...         ...         ...           ...   \n",
       "22529  92      9293469   9293000  1412073970  1304071780     106774202   \n",
       "22530  93      9314891   9315000  1418576562  1307920232     107103178   \n",
       "22531  95      9583439   9584000  1436655000  1324500906     108148244   \n",
       "22532  97      9789169   9789000  1447534832  1337624014     108847318   \n",
       "22533  98      9896221   9897000  1476319542  1361344318     111338744   \n",
       "\n",
       "             RTT   RTTVar  MinRTT  RWndLimited  ...  SndBufLimited_3  \\\n",
       "0      121797.25   9535.5  100816            0  ...              0.0   \n",
       "1      163404.60  16453.2  100816            0  ...              0.0   \n",
       "2      221776.00  23703.8  100816            0  ...              0.0   \n",
       "3      274418.60   9798.4  100816        15000  ...              0.0   \n",
       "4      216210.20  11117.8  100816        74000  ...              0.0   \n",
       "...          ...      ...     ...          ...  ...              ...   \n",
       "22529    1891.60    296.6     161      4146000  ...              0.0   \n",
       "22530    2422.00    843.0     161      4149000  ...              0.0   \n",
       "22531    1974.50    707.0     161      4364000  ...              0.0   \n",
       "22532    2000.50    429.0     161      4534000  ...              0.0   \n",
       "22533    1598.80    307.2     161      4556000  ...              0.0   \n",
       "\n",
       "       ElapsedTime_4  BusyTime_4   BytesSent_4  BytesAcked_4  BytesRetrans_4  \\\n",
       "0           124037.0    119000.0  1.284700e+04  1.547000e+03             0.0   \n",
       "1           288933.0    284000.0  4.120700e+04  1.284700e+04             0.0   \n",
       "2           392226.0    388000.0  8.091100e+04  4.120700e+04             0.0   \n",
       "3           496415.0    492000.0  1.603190e+05  7.240300e+04             0.0   \n",
       "4           579234.0    575000.0  2.127850e+05  9.367300e+04          1418.0   \n",
       "...              ...         ...           ...           ...             ...   \n",
       "22529      8787459.0   8788000.0  1.312769e+09  1.214030e+09      98098878.0   \n",
       "22530      8808776.0   8809000.0  1.316681e+09  1.214769e+09      98189630.0   \n",
       "22531      9099539.0   9100000.0  1.349074e+09  1.248059e+09     100265582.0   \n",
       "22532      9185733.0   9186000.0  1.375302e+09  1.271397e+09     103331298.0   \n",
       "22533      9293469.0   9293000.0  1.412074e+09  1.304072e+09     106774202.0   \n",
       "\n",
       "           RTT_4  RTTVar_4  RWndLimited_4  SndBufLimited_4  \n",
       "0      101050.00  28799.00            0.0              0.0  \n",
       "1      106504.25  16665.00            0.0              0.0  \n",
       "2      109021.75   6359.75            0.0              0.0  \n",
       "3      109827.20   4563.20            0.0              0.0  \n",
       "4      121797.25   9535.50            0.0              0.0  \n",
       "...          ...       ...            ...              ...  \n",
       "22529    1579.80    218.40      3952000.0              0.0  \n",
       "22530    2084.00    577.00      3959000.0              0.0  \n",
       "22531    2280.40    344.00      4146000.0              0.0  \n",
       "22532    1498.75    497.00      4146000.0              0.0  \n",
       "22533    1891.60    296.60      4146000.0              0.0  \n",
       "\n",
       "[22534 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = load_dataset('./dataset.csv')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edc9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = features.values, labels.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d788be",
   "metadata": {},
   "source": [
    "# Throughput Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962f3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 27108.960763447667\n",
      "Epoch 2/100, Loss: 3051.9155460729075\n",
      "Epoch 3/100, Loss: 1986.8909218538606\n",
      "Epoch 4/100, Loss: 1418.9217281902538\n",
      "Epoch 5/100, Loss: 1219.9882387060663\n",
      "Epoch 6/100, Loss: 1038.033269504988\n",
      "Epoch 7/100, Loss: 980.7039729383364\n",
      "Epoch 8/100, Loss: 895.1096270137577\n",
      "Epoch 9/100, Loss: 885.4718241599704\n",
      "Epoch 10/100, Loss: 811.0186436451957\n",
      "Epoch 11/100, Loss: 812.8274436519306\n",
      "Epoch 12/100, Loss: 770.4923785414705\n",
      "Epoch 13/100, Loss: 800.0684532964448\n",
      "Epoch 14/100, Loss: 735.7975317604885\n",
      "Epoch 15/100, Loss: 739.3380574710955\n",
      "Epoch 16/100, Loss: 704.7016607385136\n",
      "Epoch 17/100, Loss: 728.4806824528422\n",
      "Epoch 18/100, Loss: 684.2406382553476\n",
      "Epoch 19/100, Loss: 673.4279787801584\n",
      "Epoch 20/100, Loss: 719.9941114450806\n",
      "Epoch 21/100, Loss: 663.1794707257656\n",
      "Epoch 22/100, Loss: 696.9179366819757\n",
      "Epoch 23/100, Loss: 663.2619430576328\n",
      "Epoch 24/100, Loss: 669.4884744512624\n",
      "Epoch 25/100, Loss: 648.3995246200484\n",
      "Epoch 26/100, Loss: 643.5816066308631\n",
      "Epoch 27/100, Loss: 640.2847369325331\n",
      "Epoch 28/100, Loss: 656.743144559691\n",
      "Epoch 29/100, Loss: 652.346555379293\n",
      "Epoch 30/100, Loss: 660.3164628681257\n",
      "Epoch 31/100, Loss: 620.8131709490538\n",
      "Epoch 32/100, Loss: 602.7090521177946\n",
      "Epoch 33/100, Loss: 610.319372464387\n",
      "Epoch 34/100, Loss: 604.5222658641924\n",
      "Epoch 35/100, Loss: 590.7717170913611\n",
      "Epoch 36/100, Loss: 628.0245830865225\n",
      "Epoch 37/100, Loss: 606.6357100836888\n",
      "Epoch 38/100, Loss: 595.066907226193\n",
      "Epoch 39/100, Loss: 629.8017470658914\n",
      "Epoch 40/100, Loss: 579.8313430631136\n",
      "Epoch 41/100, Loss: 588.2459353190407\n",
      "Epoch 42/100, Loss: 575.7596490634625\n",
      "Epoch 43/100, Loss: 582.3386606081988\n",
      "Epoch 44/100, Loss: 601.066662440919\n",
      "Epoch 45/100, Loss: 567.579719006048\n",
      "Epoch 46/100, Loss: 531.0922088753621\n",
      "Epoch 47/100, Loss: 578.7621125299597\n",
      "Epoch 48/100, Loss: 554.0774540005301\n",
      "Epoch 49/100, Loss: 568.9356479369119\n",
      "Epoch 50/100, Loss: 536.3437000572923\n",
      "Epoch 51/100, Loss: 547.7218599360565\n",
      "Epoch 52/100, Loss: 548.2163899355921\n",
      "Epoch 53/100, Loss: 522.0199216899233\n",
      "Epoch 54/100, Loss: 554.9431859317706\n",
      "Epoch 55/100, Loss: 535.7637628076285\n",
      "Epoch 56/100, Loss: 543.8604016848075\n",
      "Epoch 57/100, Loss: 516.8237295628319\n",
      "Epoch 58/100, Loss: 503.37820819642917\n",
      "Epoch 59/100, Loss: 508.43415414053817\n",
      "Epoch 60/100, Loss: 495.95512615123096\n",
      "Epoch 61/100, Loss: 470.5587811888352\n",
      "Epoch 62/100, Loss: 535.3755474987669\n",
      "Epoch 63/100, Loss: 472.4317062117507\n",
      "Epoch 64/100, Loss: 490.2304099914268\n",
      "Epoch 65/100, Loss: 476.89384226213843\n",
      "Epoch 66/100, Loss: 487.4443554878235\n",
      "Epoch 67/100, Loss: 549.1134397599083\n",
      "Epoch 68/100, Loss: 452.704298462152\n",
      "Epoch 69/100, Loss: 450.29651591961573\n",
      "Epoch 70/100, Loss: 466.40334192468475\n",
      "Epoch 71/100, Loss: 468.4585550258174\n",
      "Epoch 72/100, Loss: 463.5425458355801\n",
      "Epoch 73/100, Loss: 452.5048471983016\n",
      "Epoch 74/100, Loss: 482.40962471246235\n",
      "Epoch 75/100, Loss: 438.6479154992055\n",
      "Epoch 76/100, Loss: 445.4624389669958\n",
      "Epoch 77/100, Loss: 442.8052050325015\n",
      "Epoch 78/100, Loss: 459.8163682777306\n",
      "Epoch 79/100, Loss: 450.9858814753577\n",
      "Epoch 80/100, Loss: 442.21594608978614\n",
      "Epoch 81/100, Loss: 455.162860804228\n",
      "Epoch 82/100, Loss: 434.7302431054338\n",
      "Epoch 83/100, Loss: 451.9640306520897\n",
      "Epoch 84/100, Loss: 441.16075052934536\n",
      "Epoch 85/100, Loss: 426.842255867398\n",
      "Epoch 86/100, Loss: 443.00817489805377\n",
      "Epoch 87/100, Loss: 434.1073388008752\n",
      "Epoch 88/100, Loss: 465.4827022776159\n",
      "Epoch 89/100, Loss: 423.2485183371008\n",
      "Epoch 90/100, Loss: 405.03803338796564\n",
      "Epoch 91/100, Loss: 435.5693591221351\n",
      "Epoch 92/100, Loss: 407.66436867237576\n",
      "Epoch 93/100, Loss: 420.77298281702986\n",
      "Epoch 94/100, Loss: 419.7034469604734\n",
      "Epoch 95/100, Loss: 401.60113595664865\n",
      "Epoch 96/100, Loss: 427.74206107347055\n",
      "Epoch 97/100, Loss: 419.20152877624815\n",
      "Epoch 98/100, Loss: 415.3516876921209\n",
      "Epoch 99/100, Loss: 404.9333043316073\n",
      "Epoch 100/100, Loss: 398.0688632119742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThroughputPredictor(\n",
       "  (stack): Sequential(\n",
       "    (0): Linear(in_features=47, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regression = ThroughputPredictor(num_features=len(features.columns))\n",
    "regression.to(device)\n",
    "regression.fit(X_train, y_train, epochs=100, batch_size=16, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ebde4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a51cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regression.predict(X_test)\n",
    "percent_error = np.abs((y_pred - y_test) / y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b139af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "ax.ecdf(percent_error, label='Percent Error')\n",
    "ax.set_xlabel('Percent Error (%)')\n",
    "ax.set_ylabel('Proportion of Samples')\n",
    "plt.xlim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8760c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAG2CAYAAACZEEfAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMllJREFUeJzt3XtYVPWixvF38AKIAl5ygLxhal5T1FTEo6XkNc305HZvM9S87MQLYqWUaOYF9WzLvKJWmpVau7yUJeqh0jTEu9XWzIzETNAyIEBRYc4fPc1pQouBGWdYfD/PM8/T/Naa5TuTDi+/9Zs1JovFYhEAAIBBebg6AAAAgDNRdgAAgKFRdgAAgKFRdgAAgKFRdgAAgKFRdgAAgKFRdgAAgKFRdgAAgKFRdgAAgKFRdgAAgKG5tOzs2bNHffv2VVBQkEwmk7Zs2WKz3WKxaPr06QoMDJS3t7fCw8N1+vRpm30uX76sIUOGyNfXV/7+/nr88ceVnZ19G58FAABwZy4tOzk5OWrZsqWWLVt20+0LFizQ4sWLFR8fr+TkZPn4+KhHjx66evWqdZ8hQ4boP//5j3bt2qVt27Zpz549Gj169O16CgAAwM2Z3OWLQE0mkzZv3qz+/ftL+nVWJygoSJMnT9aTTz4pScrMzJTZbNbatWs1ePBgnTx5Uk2bNtXBgwfVtm1bSVJCQoJ69+6t77//XkFBQa56OgAAwE2Ud3WAW0lJSVFaWprCw8OtY35+fmrfvr2SkpI0ePBgJSUlyd/f31p0JCk8PFweHh5KTk7Www8/fNNj5+XlKS8vz3q/oKBAly9fVvXq1WUymZz3pAAAgMNYLBb98ssvCgoKkofHrU9WuW3ZSUtLkySZzWabcbPZbN2WlpammjVr2mwvX768qlWrZt3nZuLi4jRz5kwHJwYAAK5w7tw51apV65bb3bbsOFNMTIyio6Ot9zMzM1WnTh2dO3dOvr6+Dv2zms/YUazHfTmzh0NzAABgNFlZWapdu7aqVKnyp/u5bdkJCAiQJKWnpyswMNA6np6erlatWln3uXjxos3jbty4ocuXL1sffzOenp7y9PQsNO7r6+vwsuPhWalYj3N0DgAAjOqvlqC47XV2goODFRAQoMTEROtYVlaWkpOTFRoaKkkKDQ1VRkaGDh8+bN3no48+UkFBgdq3b3/bMwMAAPfj0pmd7OxsffPNN9b7KSkpOnbsmKpVq6Y6deooKipKs2fPVsOGDRUcHKzY2FgFBQVZP7HVpEkT9ezZU6NGjVJ8fLyuX7+ucePGafDgwXwSCwAASHJx2Tl06JDuv/9+6/3f1tFERERo7dq1evrpp5WTk6PRo0crIyNDnTp1UkJCgry8vKyPefPNNzVu3Dh169ZNHh4eGjhwoBYvXnzbnwsAAHBPbnOdHVfKysqSn5+fMjMzHb5Wpt7UD4r1uO/m9XFoDgAAjKaoP7/dds0OAACAI1B2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoZV3dQDcXL2pHxQa+25eHxckAQCgdGNmBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGBplBwAAGJpbl538/HzFxsYqODhY3t7euuuuuzRr1ixZLBbrPhaLRdOnT1dgYKC8vb0VHh6u06dPuzA1AABwJ25ddubPn68VK1Zo6dKlOnnypObPn68FCxZoyZIl1n0WLFigxYsXKz4+XsnJyfLx8VGPHj109epVFyYHAADuoryrA/yZzz77TA899JD69OkjSapXr542bNigAwcOSPp1VmfRokWaNm2aHnroIUnSunXrZDabtWXLFg0ePNhl2QEAgHtw65mdjh07KjExUV9//bUk6fjx49q7d6969eolSUpJSVFaWprCw8Otj/Hz81P79u2VlJR0y+Pm5eUpKyvL5gYAAIzJrWd2pk6dqqysLDVu3FjlypVTfn6+5syZoyFDhkiS0tLSJElms9nmcWaz2brtZuLi4jRz5kznBQcAAG7DrWd23n77bb355ptav369jhw5otdee03/+te/9Nprr5XouDExMcrMzLTezp0756DEAADA3bj1zM5TTz2lqVOnWtfetGjRQmfPnlVcXJwiIiIUEBAgSUpPT1dgYKD1cenp6WrVqtUtj+vp6SlPT0+nZgcAAO7BrWd2cnNz5eFhG7FcuXIqKCiQJAUHBysgIECJiYnW7VlZWUpOTlZoaOhtzQoAANyTW8/s9O3bV3PmzFGdOnXUrFkzHT16VC+88IJGjBghSTKZTIqKitLs2bPVsGFDBQcHKzY2VkFBQerfv79rwwMAALfg1mVnyZIlio2N1dixY3Xx4kUFBQVpzJgxmj59unWfp59+Wjk5ORo9erQyMjLUqVMnJSQkyMvLy4XJAQCAuzBZfn854jIqKytLfn5+yszMlK+vr0OPXW/qBw471nfz+jjsWAAAlHZF/fnt1mt2AAAASoqyAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADK1EZScvL89ROQAAAJzCrrKzfft2RUREqH79+qpQoYIqVaokX19fdenSRXPmzNEPP/zgrJwAAADFUqSys3nzZjVq1EgjRoxQ+fLlNWXKFG3atEk7duzQyy+/rC5duuh///d/Vb9+ff3zn//UpUuXnJ0bAACgSMoXZacFCxboxRdfVK9eveThUbgfDRo0SJJ0/vx5LVmyRG+88YYmTZrk2KQAAADFUKSyk5SUVKSD3XnnnZo3b16JAgEAADhSiT+NlZOTo6ysLEdkAQAAcLhil50TJ06obdu2qlKliqpWraoWLVro0KFDjswGAABQYsUuO2PGjNG4ceOUnZ2tn376SQMGDFBERIQjswEAAJRYkcvOQw89pPPnz1vvX7p0Sf369VOlSpXk7++v3r17Kz093SkhAQAAiqtIC5Ql6dFHH1XXrl0VGRmp8ePHa9y4cWrWrJm6dOmi69ev66OPPtLkyZOdmRUAAMBuRZ7ZeeSRR3TgwAGdOHFCHTp0UFhYmHbu3KmwsDD913/9l3bu3Klp06Y5MysAAIDdijyzI0l+fn6Kj4/X3r17FRERoQceeECzZs1SpUqVnJUPAACgROxaoHz58mUdPnxYLVq00OHDh+Xr66uQkBB9+OGHzsoHAABQIkUuO+vXr1etWrXUp08f1a1bV9u3b9eMGTO0detWLViwQIMGDWKBMgAAcDtFLjsxMTF69dVXlZaWpsTERMXGxkqSGjdurE8++UQPPPCAQkNDnRYUAACgOIpcdrKzs3X33XdLku666y7l5ubabB81apT279/v2HQAAAAlVOQFyhEREerTp4/uu+8+HTp0SEOHDi20T82aNR0aDgAAoKSKXHZeeOEF3X///frqq680bNgwde/e3Zm5AAAAHMKuj5737dtXffv2dVYWAAAAhyvSmp2NGzcW+YDnzp3Tvn37ih0IAADAkYpUdlasWKEmTZpowYIFOnnyZKHtmZmZ+vDDD/WPf/xDrVu31k8//eTwoAAAAMVRpNNYu3fv1nvvvaclS5YoJiZGPj4+MpvN8vLy0s8//6y0tDTVqFFDw4YN05dffimz2ezs3AAAAEVS5DU7/fr1U79+/fTjjz9q7969Onv2rK5cuaIaNWooJCREISEh8vCw64LMAAAATmfXAmVJqlGjhvr37++EKPgr9aZ+UGjsu3l9XJAEAIDSg6kYAABgaJQdAABgaJQdAABgaJQdAABgaHaVnevXr+uuu+666bV2AAAA3JFdZadChQq6evWqs7IAAAA4nN2nsSIjIzV//nzduHHDGXkAAAAcyu7r7Bw8eFCJiYnauXOnWrRoIR8fH5vtmzZtclg4AACAkrK77Pj7+2vgwIHOyAIAAOBwdpedNWvWOCMHAACAU9hddn5z6dIlnTp1SpJ0991364477nBYKAAAAEexe4FyTk6ORowYocDAQHXu3FmdO3dWUFCQHn/8ceXm5jojIwAAQLHZXXaio6O1e/duvf/++8rIyFBGRoa2bt2q3bt3a/Lkyc7ICAAAUGx2n8Z699139c477+i+++6zjvXu3Vve3t4aNGiQVqxY4ch8AAAAJWL3zE5ubq7MZnOh8Zo1azrlNNb58+f16KOPqnr16vL29laLFi106NAh63aLxaLp06crMDBQ3t7eCg8P1+nTpx2eAwAAlE52l53Q0FDNmDHD5krKV65c0cyZMxUaGurQcD///LPCwsJUoUIFbd++XSdOnNDChQtVtWpV6z4LFizQ4sWLFR8fr+TkZPn4+KhHjx5c6RkAAEgqxmmsRYsWqWfPnqpVq5ZatmwpSTp+/Li8vLy0Y8cOh4abP3++ateubfNx9+DgYOt/WywWLVq0SNOmTdNDDz0kSVq3bp3MZrO2bNmiwYMHOzQPAAAofeye2WnRooVOnz6tuLg4tWrVSq1atdK8efN0+vRpNWvWzKHh3nvvPbVt21aPPPKIatasqZCQEK1evdq6PSUlRWlpaQoPD7eO+fn5qX379kpKSrrlcfPy8pSVlWVzAwAAxmTXzM7169fVuHFjbdu2TaNGjXJWJqtvv/1WK1asUHR0tJ555hkdPHhQEyZMUMWKFRUREaG0tDRJKrSGyGw2W7fdTFxcnGbOnOnU7AAAwD249beeFxQUqHXr1po7d65CQkI0evRojRo1SvHx8SU6bkxMjDIzM623c+fOOSgxAABwN279reeBgYFq2rSpzViTJk2UmpoqSQoICJAkpaen2+yTnp5u3XYznp6e8vX1tbkBAABjcutvPQ8LC7N+JcVvvv76a9WtW1fSr4uVAwIClJiYqFatWkmSsrKylJycrCeeeMJhOQAAQOnl1t96PmnSJHXs2FFz587VoEGDdODAAa1atUqrVq2SJJlMJkVFRWn27Nlq2LChgoODFRsbq6CgIPXv3/+2ZAQAAO7NrrJz48YN3X///erevfufniZylHvvvVebN29WTEyMnn/+eQUHB2vRokUaMmSIdZ+nn35aOTk5Gj16tDIyMtSpUyclJCTIy8vL6fkAAID7M1ksFos9D6hUqZJOnjxpPZVkBFlZWfLz81NmZqbD1+/Um/qBQ4/3R9/N6+PU4wMA4K6K+vPb7gXK7dq109GjR0sUDgAA4Haxe83O2LFjNXnyZH3//fdq06ZNoQXK99xzj8PCAQAAlJTdZee3r2CYMGGCdcxkMslischkMik/P99x6QAAAErI7rKTkpLijBwAAABOYXfZMdLCZAAAYHxFXqA8duxYZWdnW+9v2LBBOTk51vsZGRnq3bu3Y9MBAACUUJHLzsqVK5Wbm2u9P2bMGJuvacjLy9OOHTscmw4AAKCEilx2/ng5HjsvzwMAAOASdl9nBwAAoDSh7AAAAEOz69NY06dPV6VKlSRJ165d05w5c+Tn5ydJNut5AAAA3EWRy07nzp116tQp6/2OHTvq22+/LbQPAACAOyly2fnkk0+cGAMAAMA5WLMDAAAMjbIDAAAMjbIDAAAMjbIDAAAMze6yk5qaetOrJ1ssFqWmpjokFAAAgKPYXXaCg4N16dKlQuOXL19WcHCwQ0IBAAA4it1lx2KxyGQyFRrPzs6Wl5eXQ0IBAAA4SpGvsxMdHS1JMplMio2NtV5JWZLy8/OVnJysVq1aOTwgAABASRS57Bw9elTSrzM7X3zxhSpWrGjdVrFiRbVs2VJPPvmk4xMCAACUQJHLzscffyxJGj58uF566SX5+vo6LRQAAICj2PVFoJK0Zs0aZ+QAAABwCrvLTk5OjubNm6fExERdvHhRBQUFNtv/+OWgAAAArmR32Rk5cqR2796toUOHKjAw8KafzAIAAHAXdped7du364MPPlBYWJgz8gAAADiU3dfZqVq1qqpVq+aMLAAAAA5nd9mZNWuWpk+frtzcXGfkAQAAcCi7T2MtXLhQZ86ckdlsVr169VShQgWb7UeOHHFYOAAAgJKyu+z079/fCTEAAACcw+6yM2PGDGfkAAAAcAq71+xIUkZGhl5++WXFxMTo8uXLkn49fXX+/HmHhgMAACgpu2d2Pv/8c4WHh8vPz0/fffedRo0apWrVqmnTpk1KTU3VunXrnJETAACgWOye2YmOjtawYcN0+vRpeXl5Wcd79+6tPXv2ODQcAABASdlddg4ePKgxY8YUGr/zzjuVlpbmkFAAAACOYnfZ8fT0VFZWVqHxr7/+WnfccYdDQgEAADiK3WWnX79+ev7553X9+nVJkslkUmpqqqZMmaKBAwc6PCAAAEBJ2F12Fi5cqOzsbNWsWVNXrlxRly5d1KBBA1WpUkVz5sxxRkYAAIBis/vTWH5+ftq1a5f27dun48ePKzs7W61bt1Z4eLgz8gEAAJSI3WXnN2FhYXzzOQAAcHt2n8aaMGGCFi9eXGh86dKlioqKckQmAAAAh7G77Lz77rs3ndHp2LGj3nnnHYeEAgAAcBS7y85PP/0kPz+/QuO+vr768ccfHRIKAADAUewuOw0aNFBCQkKh8e3bt6t+/foOCQUAAOAodi9Qjo6O1rhx43Tp0iV17dpVkpSYmKiFCxdq0aJFjs4HAABQInaXnREjRigvL09z5szRrFmzJEn16tXTihUr9Nhjjzk8IAAAQEnYVXZu3Lih9evXa8CAAXriiSd06dIleXt7q3Llys7KBwAAUCJ2rdkpX768/vnPf+rq1auSpDvuuIOiAwAA3JrdC5TbtWuno0ePOiMLAACAw9m9Zmfs2LGaPHmyvv/+e7Vp00Y+Pj422++55x6HhQMAACgpu8vO4MGDJf16JeXfmEwmWSwWmUwm5efnOy4dAABACdlddlJSUpyRAwAAwCnsLjt169Z1Rg4AAACnsHuBsiS9/vrrCgsLU1BQkM6ePStJWrRokbZu3erQcAAAACVld9lZsWKFoqOj1bt3b2VkZFjX6Pj7+3MFZQAA4HbsLjtLlizR6tWr9eyzz6pcuXLW8bZt2+qLL75waDgAAICSsrvspKSkKCQkpNC4p6encnJyHBIKAADAUewuO8HBwTp27Fih8YSEBDVp0sQRmQAAABymWN96HhkZqatXr8pisejAgQPasGGD4uLi9PLLLzsjIwAAQLHZXXZGjhwpb29vTZs2Tbm5ufrHP/6hoKAgvfTSS9YLDgIAALgLu8tOXl6e+vfvryFDhig3N1fZ2dmqWbOmM7IBAACUWJHX7Fy6dEm9evVS5cqV5evrqw4dOujChQsUHQAA4NaKXHamTJmiY8eO6fnnn9e//vUvZWRkaOTIkc7MVsi8efNkMpkUFRVlHbt69aoiIyNVvXp1Va5cWQMHDlR6evptzQUAANxXkU9j7dq1S2vXrlWPHj0kSQ8++KCaNGmivLw8eXp6Oi3gbw4ePKiVK1cW+lb1SZMm6YMPPtC///1v+fn5ady4cRowYID27dvn9EwAAMD9FXlm54cfflDLli2t9xs2bChPT09duHDBKcF+Lzs7W0OGDNHq1atVtWpV63hmZqZeeeUVvfDCC+ratavatGmjNWvW6LPPPtP+/fudngsAALg/u66z8/srJv9232KxODTQzURGRqpPnz4KDw+3GT98+LCuX79uM964cWPVqVNHSUlJtzxeXl6esrKybG4AAMCYinway2KxqFGjRjKZTNax7OxshYSEyMPj/zvT5cuXHRpw48aNOnLkiA4ePFhoW1pamipWrCh/f3+bcbPZrLS0tFseMy4uTjNnznRoTgAA4J6KXHbWrFnjzBw3de7cOU2cOFG7du2Sl5eXw44bExOj6Oho6/2srCzVrl3bYccHAADuo8hlJyIiwpk5burw4cO6ePGiWrdubR3Lz8/Xnj17tHTpUu3YsUPXrl1TRkaGzexOenq6AgICbnlcT0/P27KoGgAAuJ7dFxW8nbp161bom9SHDx+uxo0ba8qUKapdu7YqVKigxMREDRw4UJJ06tQppaamKjQ01BWRAQCAm3HrslOlShU1b97cZszHx0fVq1e3jj/++OOKjo5WtWrV5Ovrq/Hjxys0NFQdOnRwRWQAAOBm3LrsFMWLL74oDw8PDRw4UHl5eerRo4eWL1/u6lgAAMBNmCy347Pjbi4rK0t+fn7KzMyUr6+vQ49db+oHDj3eH303r49Tjw8AgLsq6s9vu66zAwAAUNrYfRorPz9fa9euVWJioi5evKiCggKb7R999JHDwgEAAJSU3WVn4sSJWrt2rfr06aPmzZvbXGQQAADA3dhddjZu3Ki3335bvXv3dkYeAAAAh7J7zU7FihXVoEEDZ2QBAABwOLtndiZPnqyXXnpJS5cu5RSWm/rjJ8D4xBYAoCyzu+zs3btXH3/8sbZv365mzZqpQoUKNts3bdrksHAAAAAlZXfZ8ff318MPP+yMLAAAAA5nd9lxxbefAwAAFBcXFQQAAIZWrO/Geuedd/T2228rNTVV165ds9l25MgRhwQDAABwBLtndhYvXqzhw4fLbDbr6NGjateunapXr65vv/1WvXr1ckZGAACAYrO77CxfvlyrVq3SkiVLVLFiRT399NPatWuXJkyYoMzMTGdkBAAAKDa7y05qaqo6duwoSfL29tYvv/wiSRo6dKg2bNjg2HQAAAAlZHfZCQgI0OXLlyVJderU0f79+yVJKSkpslgsjk0HAABQQnaXna5du+q9996TJA0fPlyTJk3SAw88oL/97W9cfwcAALgduz+NtWrVKhUUFEiSIiMjVb16dX322Wfq16+fxowZ4/CAAAAAJWF32fHw8JCHx/9PCA0ePFiDBw92aCgAAABHKdZ1dj799FOtXLlSZ86c0TvvvKM777xTr7/+uoKDg9WpUydHZ8Sf+OOXfgIAAFt2r9l599131aNHD3l7e+vo0aPKy8uTJGVmZmru3LkODwgAAFASdped2bNnKz4+XqtXr7b5xvOwsDCungwAANyO3WXn1KlT6ty5c6FxPz8/ZWRkOCITAACAwxTrOjvffPNNofG9e/eqfv36DgkFAADgKHaXnVGjRmnixIlKTk6WyWTSDz/8oDfffFNPPvmknnjiCWdkBAAAKDa7P401depUFRQUqFu3bsrNzVXnzp3l6empJ598UuPHj3dGRgAAgGKzu+yYTCY9++yzeuqpp/TNN98oOztbTZs2VeXKlZ2RDwAAoESKdZ0dSapYsaKaNm3qyCwAAAAOV+SyM2LEiCLt9+qrrxY7DAAAgKMVueysXbtWdevWVUhICN9uDgAASo0il50nnnhCGzZsUEpKioYPH65HH31U1apVc2Y2AACAEivyR8+XLVumCxcu6Omnn9b777+v2rVra9CgQdqxYwczPQAAwG3ZdZ0dT09P/f3vf9euXbt04sQJNWvWTGPHjlW9evWUnZ3trIwAAADFZvdFBa0P9PCQyWSSxWJRfn6+IzMBAAA4jF0fPc/Ly9OmTZv06quvau/evXrwwQe1dOlS9ezZUx4exe5NcIF6Uz8oNPbdvD4uSAIAgHMVueyMHTtWGzduVO3atTVixAht2LBBNWrUcGY2AACAEity2YmPj1edOnVUv3597d69W7t3777pfps2bXJYOAAAgJIqctl57LHHZDKZnJkFKDZOywEAbsWuiwoCAG6PPxZ4yjtQfKwqBgAAhlbsLwIFAMCROB0NZ2FmBwAAGBplBwAAGBqnscqAm00NAwBQVjCzAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2yAwAADI2PnsOKq5cCAIyIsgM4EQUSAFyP01gAAMDQmNkBABTZH2cry+pMJbO2pQszOwAAwNCY2QHKAH4bB1CWUXYAAA5V1FM8fEkxbhfKDgDAcFhTg9+j7AAA3BalBY5A2YFT8AYFAHAXlB0YlrMLF4UOwO/xnuC+KDvAbcYbIgDcXpQd/Ck+sgwAKO0oOwBgcPzSgrLOrctOXFycNm3apK+++kre3t7q2LGj5s+fr7vvvtu6z9WrVzV58mRt3LhReXl56tGjh5YvXy6z2ezC5DAargcCo+P0KozMrcvO7t27FRkZqXvvvVc3btzQM888o+7du+vEiRPy8fGRJE2aNEkffPCB/v3vf8vPz0/jxo3TgAEDtG/fPhenBwD8hl8Yyg53LM5uXXYSEhJs7q9du1Y1a9bU4cOH1blzZ2VmZuqVV17R+vXr1bVrV0nSmjVr1KRJE+3fv18dOnRwRWyUMqVlit8d30DgWvydcH+l5f3F6Ny67PxRZmamJKlatWqSpMOHD+v69esKDw+37tO4cWPVqVNHSUlJtyw7eXl5ysvLs97PyspyYmqUJfz2CgDup9SUnYKCAkVFRSksLEzNmzeXJKWlpalixYry9/e32ddsNistLe2Wx4qLi9PMmTOdGRduyl3LiCN/Q3fX54hfMRuD0qy0/v0tNWUnMjJSX375pfbu3VviY8XExCg6Otp6PysrS7Vr1y7xcQGUHaX1TV8qu4W4NP8/Q8mUirIzbtw4bdu2TXv27FGtWrWs4wEBAbp27ZoyMjJsZnfS09MVEBBwy+N5enrK09PTmZENizeL26es/kC63fg7bQz8e8GfceuyY7FYNH78eG3evFmffPKJgoODbba3adNGFSpUUGJiogYOHChJOnXqlFJTUxUaGuqKyCihov7g4Y2tbKOguA8j/ltkUbHxuHXZiYyM1Pr167V161ZVqVLFug7Hz89P3t7e8vPz0+OPP67o6GhVq1ZNvr6+Gj9+vEJDQ/kkFuAAlAoARuDWZWfFihWSpPvuu89mfM2aNRo2bJgk6cUXX5SHh4cGDhxoc1FBAM7Bb73uzYgzLUBJuXXZsVgsf7mPl5eXli1bpmXLlt2GRABKC2alSs5di5O75oL7cuuyA8A5KAIAyhLKDgBJjv1t2RVlqjT/tl+as5d2vPZlA2UHAIDbxNkXEGWG9uYoOygxZ/9mxG9eAHD7Gem9l7ID/IGR/oHDFv9vgbKJsgOX4QcPAOB2oOwAcGulvRSX9vxwn/+H7pKjNKLsAMBf4IcMSgsWLd+ch6sDAAAAOBMzOwBui6L8xlnaZ1BKe34YE7M9zOwAAACDo+wAAABD4zQWAJfhtA9wc/zbcCzKDlCK8YYIAH+N01gAAMDQKDsAAMDQKDsAAMDQKDsAAMDQKDsAAMDQ+DQWbhs+OQQUH/9+gOJjZgcAABgaMzsASoQZBwDujrIDAH9AgcPt5Iq/b3/8M43+xaCcxgIAAIbGzA4AACi20jATStkBAKCMKw2FpSQ4jQUAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAyNsgMAAAzNMGVn2bJlqlevnry8vNS+fXsdOHDA1ZEAAIAbMETZeeuttxQdHa0ZM2boyJEjatmypXr06KGLFy+6OhoAAHAxQ5SdF154QaNGjdLw4cPVtGlTxcfHq1KlSnr11VddHQ0AALhYeVcHKKlr167p8OHDiomJsY55eHgoPDxcSUlJN31MXl6e8vLyrPczMzMlSVlZWQ7PV5CX6/BjAgBQmjjj5+vvj2uxWP50v1Jfdn788Ufl5+fLbDbbjJvNZn311Vc3fUxcXJxmzpxZaLx27dpOyQgAQFnmt8i5x//ll1/k5+d3y+2lvuwUR0xMjKKjo633CwoKdPnyZVWvXl0mk8mFycqOrKws1a5dW+fOnZOvr6+r45QpvPauw2vvOrz2ruPM195iseiXX35RUFDQn+5X6stOjRo1VK5cOaWnp9uMp6enKyAg4KaP8fT0lKenp82Yv7+/syLiT/j6+vLG4yK89q7Da+86vPau46zX/s9mdH5T6hcoV6xYUW3atFFiYqJ1rKCgQImJiQoNDXVhMgAA4A5K/cyOJEVHRysiIkJt27ZVu3bttGjRIuXk5Gj48OGujgYAAFzMEGXnb3/7my5duqTp06crLS1NrVq1UkJCQqFFy3Afnp6emjFjRqHTiXA+XnvX4bV3HV5713GH195k+avPawEAAJRipX7NDgAAwJ+h7AAAAEOj7AAAAEOj7AAAAEOj7OC2iouL07333qsqVaqoZs2a6t+/v06dOuXqWGXSvHnzZDKZFBUV5eooZcL58+f16KOPqnr16vL29laLFi106NAhV8cyvPz8fMXGxio4OFje3t666667NGvWrL/8LiXYb8+ePerbt6+CgoJkMpm0ZcsWm+0Wi0XTp09XYGCgvL29FR4ertOnT9+WbJQd3Fa7d+9WZGSk9u/fr127dun69evq3r27cnJyXB2tTDl48KBWrlype+65x9VRyoSff/5ZYWFhqlChgrZv364TJ05o4cKFqlq1qqujGd78+fO1YsUKLV26VCdPntT8+fO1YMECLVmyxNXRDCcnJ0ctW7bUsmXLbrp9wYIFWrx4seLj45WcnCwfHx/16NFDV69edXo2PnoOl7p06ZJq1qyp3bt3q3Pnzq6OUyZkZ2erdevWWr58uWbPnq1WrVpp0aJFro5laFOnTtW+ffv06aefujpKmfPggw/KbDbrlVdesY4NHDhQ3t7eeuONN1yYzNhMJpM2b96s/v37S/p1VicoKEiTJ0/Wk08+KUnKzMyU2WzW2rVrNXjwYKfmYWYHLpWZmSlJqlatmouTlB2RkZHq06ePwsPDXR2lzHjvvffUtm1bPfLII6pZs6ZCQkK0evVqV8cqEzp27KjExER9/fXXkqTjx49r79696tWrl4uTlS0pKSlKS0uzed/x8/NT+/btlZSU5PQ/3xBXUEbpVFBQoKioKIWFhal58+aujlMmbNy4UUeOHNHBgwddHaVM+fbbb7VixQpFR0frmWee0cGDBzVhwgRVrFhRERERro5naFOnTlVWVpYaN26scuXKKT8/X3PmzNGQIUNcHa1MSUtLk6RC32xgNput25yJsgOXiYyM1Jdffqm9e/e6OkqZcO7cOU2cOFG7du2Sl5eXq+OUKQUFBWrbtq3mzp0rSQoJCdGXX36p+Ph4yo6Tvf3223rzzTe1fv16NWvWTMeOHVNUVJSCgoJ47csQTmPBJcaNG6dt27bp448/Vq1atVwdp0w4fPiwLl68qNatW6t8+fIqX768du/ercWLF6t8+fLKz893dUTDCgwMVNOmTW3GmjRpotTUVBclKjueeuopTZ06VYMHD1aLFi00dOhQTZo0SXFxca6OVqYEBARIktLT023G09PTrducibKD28pisWjcuHHavHmzPvroIwUHB7s6UpnRrVs3ffHFFzp27Jj11rZtWw0ZMkTHjh1TuXLlXB3RsMLCwgpdYuHrr79W3bp1XZSo7MjNzZWHh+2PunLlyqmgoMBFicqm4OBgBQQEKDEx0TqWlZWl5ORkhYaGOv3P5zQWbqvIyEitX79eW7duVZUqVaznav38/OTt7e3idMZWpUqVQmujfHx8VL16ddZMOdmkSZPUsWNHzZ07V4MGDdKBAwe0atUqrVq1ytXRDK9v376aM2eO6tSpo2bNmuno0aN64YUXNGLECFdHM5zs7Gx988031vspKSk6duyYqlWrpjp16igqKkqzZ89Ww4YNFRwcrNjYWAUFBVk/seVUFuA2knTT25o1a1wdrUzq0qWLZeLEia6OUSa8//77lubNm1s8PT0tjRs3tqxatcrVkcqErKwsy8SJEy116tSxeHl5WerXr2959tlnLXl5ea6OZjgff/zxTd/fIyIiLBaLxVJQUGCJjY21mM1mi6enp6Vbt26WU6dO3ZZsXGcHAAAYGmt2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2AACAoVF2APwlk8mkLVu2uDrGXyppzmHDht2eq7newtChQ61fFvpXBg8erIULFzo5EWAMlB2gjBs2bJhMJlOhW8+ePV0dzaFu9hx/f3vuuef00ksvae3atS7Jd/z4cX344YeaMGFCkfafNm2a5syZo8zMTCcnA0o/vhsLgHr27Kk1a9bYjHl6eroojXNcuHDB+t9vvfWWpk+fbvPlnJUrV1blypVdEU2StGTJEj3yyCNFztC8eXPdddddeuONNxQZGenkdEDpxswOAHl6eiogIMDmVrVq1VvuP2XKFDVq1EiVKlVS/fr1FRsbq+vXr1u3P/fcc2rVqpVWrlyp2rVrq1KlSho0aJDNLMQnn3yidu3aycfHR/7+/goLC9PZs2et27du3arWrVvLy8tL9evX18yZM3Xjxg3r9tOnT6tz587y8vJS06ZNtWvXrj99jr9/bn5+fjKZTDZjlStXLnQa67777tP48eMVFRWlqlWrymw2a/Xq1crJydHw4cNVpUoVNWjQQNu3b7f5s7788kv16tVLlStXltls1tChQ/Xjjz/eMlt+fr7eeecd9e3b12Z8+fLlatiwoby8vGQ2m/Xf//3fNtv79u2rjRs3/unzBkDZAVAMVapU0dq1a3XixAm99NJLWr16tV588UWbfb755hu9/fbbev/995WQkKCjR49q7NixkqQbN26of//+6tKliz7//HMlJSVp9OjRMplMkqRPP/1Ujz32mCZOnKgTJ05o5cqVWrt2rebMmSNJKigo0IABA1SxYkUlJycrPj5eU6ZMccpzfe2111SjRg0dOHBA48eP1xNPPKFHHnlEHTt21JEjR9S9e3cNHTpUubm5kqSMjAx17dpVISEhOnTokBISEpSenq5Bgwbd8s/4/PPPlZmZqbZt21rHDh06pAkTJuj555/XqVOnlJCQoM6dO9s8rl27djpw4IDy8vKc8twBw7gtXzcKwG1FRERYypUrZ/Hx8bG5zZkzx7qPJMvmzZtveYz/+Z//sbRp08Z6f8aMGZZy5cpZvv/+e+vY9u3bLR4eHpYLFy5YfvrpJ4skyyeffHLT43Xr1s0yd+5cm7HXX3/dEhgYaLFYLJYdO3ZYypcvbzl//rzN8f8q52/WrFlj8fPzKzQeERFheeihh6z3u3TpYunUqZP1/o0bNyw+Pj6WoUOHWscuXLhgkWRJSkqyWCwWy6xZsyzdu3e3Oe65c+cskm75Dc+bN2+2lCtXzlJQUGAde/fddy2+vr6WrKysWz6P48ePWyRZvvvuuz99vkBZx5odALr//vu1YsUKm7Fq1ardcv+33npLixcv1pkzZ5Sdna0bN27I19fXZp86derozjvvtN4PDQ1VQUGBTp06pS5dumjYsGHq0aOHHnjgAYWHh2vQoEEKDAyU9Oti3X379llncqRfT/VcvXpVubm5OnnypGrXrq2goCCb4zvDPffcY/3vcuXKqXr16mrRooV1zGw2S5IuXrxozf7xxx/fdO3NmTNn1KhRo0LjV65ckaenp3VmS5IeeOAB1a1bV/Xr11fPnj3Vs2dPPfzww6pUqZJ1H29vb0myzioBuDlOYwGQj4+PGjRoYHO7VdlJSkrSkCFD1Lt3b23btk1Hjx7Vs88+q2vXrtn1Z65Zs0ZJSUnq2LGj3nrrLTVq1Ej79++XJGVnZ2vmzJk6duyY9fbFF1/o9OnT8vLyKvHztUeFChVs7ptMJpux3wpKQUGBpF+z9+3b1yb7sWPHrGuMbqZGjRrKzc21eQ2rVKmiI0eOaMOGDQoMDNT06dPVsmVLZWRkWPe5fPmyJOmOO+5wyHMFjIqZHQB2+eyzz1S3bl09++yz1rHfLyz+TWpqqn744Qfr7Mv+/fvl4eGhu+++27pPSEiIQkJCFBMTo9DQUK1fv14dOnRQ69atderUKTVo0OCmGZo0aaJz587pwoUL1tmg34qSq7Vu3Vrvvvuu6tWrp/Lli/YW26pVK0nSiRMnrP8tSeXLl1d4eLjCw8M1Y8YM+fv766OPPtKAAQMk/boQulatWqpRo4ajnwZgKMzsAFBeXp7S0tJsbrf69FDDhg2VmpqqjRs36syZM1q8eLE2b95caD8vLy9FRETo+PHj+vTTTzVhwgQNGjRIAQEBSklJUUxMjJKSknT27Fnt3LlTp0+fVpMmTSRJ06dP17p16zRz5kz95z//0cmTJ7Vx40ZNmzZNkhQeHq5GjRrZHP/35cuVIiMjdfnyZf3973/XwYMHdebMGe3YsUPDhw9Xfn7+TR9zxx13qHXr1tq7d691bNu2bVq8eLGOHTums2fPat26dSooKLApi59++qm6d+/u9OcElHaUHQBKSEhQYGCgza1Tp0433bdfv36aNGmSxo0bp1atWumzzz5TbGxsof0aNGigAQMGqHfv3urevbvuueceLV++XJJUqVIlffXVVxo4cKAaNWqk0aNHKzIyUmPGjJEk9ejRQ9u2bdPOnTt17733qkOHDnrxxRdVt25dSZKHh4c2b96sK1euqF27dho5cqTN+h5XCgoK0r59+5Sfn6/u3burRYsWioqKkr+/vzw8bv2WO3LkSL355pvW+/7+/tq0aZO6du2qJk2aKD4+Xhs2bFCzZs0kSVevXtWWLVs0atQopz8noLQzWSwWi6tDADCW5557Tlu2bNGxY8dcHaXUuHLliu6++2699dZbRVpsvWLFCm3evFk7d+68DemA0o2ZHQBwA97e3lq3bt2fXnzw9ypUqKAlS5Y4ORVgDCxQBgA3cd999xV535EjRzovCGAwnMYCAACGxmksAABgaJQdAABgaJQdAABgaJQdAABgaJQdAABgaJQdAABgaJQdAABgaJQdAABgaJQdAABgaP8HFtoD0eAdes4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_vals, inv = np.unique(X_test[::, 0], return_inverse=True)\n",
    "ax = plt.axes()\n",
    "ax.bar(k_vals * WINDOW_SIZE_MS/1000.0, np.bincount(inv, weights=percent_error) / np.bincount(inv), width=WINDOW_SIZE_MS/1000.0, align='edge')\n",
    "ax.set_xlabel('Elapsed Time (s)')\n",
    "ax.set_ylabel('Mean Percent Error (%)')\n",
    "ax.set_ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6341b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trustee_report(model, regression: bool, X, y, scaler, feature_names):\n",
    "  trustee = RegressionTrustee(expert=model) if regression else ClassificationTrustee(expert=model)\n",
    "  trustee.fit(scaler.transform(X) if scaler else X, y, num_iter=50, num_stability_iter=10, samples_size=0.3, top_k=5, verbose=True, predict_method_name='predict_trustee')\n",
    "  dt, pruned_dt, agreement, reward = trustee.explain()\n",
    "  print(f\"Model explanation training (agreement, fidelity): ({agreement}, {reward})\")\n",
    "  print(f\"Model Explanation size: {dt.tree_.node_count}\")\n",
    "  print(f\"Top-k Prunned Model explanation size: {pruned_dt.tree_.node_count}\")\n",
    "  \n",
    "  dot_data = tree.export_graphviz(\n",
    "    pruned_dt,\n",
    "    feature_names=feature_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    "  )\n",
    "  graph = graphviz.Source(dot_data)\n",
    "  display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ab901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training dataset using ThroughputPredictor(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=47, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.9977921395827527\n",
      "Initializing Trustee outer-loop with 10 iterations\n",
      "########## Outer-loop Iteration 0/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (15773, 15773) entries\n",
      "Student model 0-0 trained with depth 30 and 3310 leaves:\n",
      "Student model score: 0.9977173594148473\n",
      "Student model 0-0 fidelity: 0.9977173592050128\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (17193, 17193) entries\n",
      "Student model 0-1 trained with depth 33 and 3258 leaves:\n",
      "Student model score: 0.9923809018949267\n",
      "Student model 0-1 fidelity: 0.9923809019480038\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (18613, 18613) entries\n",
      "Student model 0-2 trained with depth 34 and 3215 leaves:\n",
      "Student model score: 0.9962993964792851\n",
      "Student model 0-2 fidelity: 0.9962993963636563\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (20033, 20033) entries\n",
      "Student model 0-3 trained with depth 31 and 3167 leaves:\n",
      "Student model score: 0.9959758959795746\n",
      "Student model 0-3 fidelity: 0.9959758961832548\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (21453, 21453) entries\n",
      "Student model 0-4 trained with depth 33 and 3174 leaves:\n",
      "Student model score: 0.9974597245511487\n",
      "Student model 0-4 fidelity: 0.9974597246068628\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (22873, 22873) entries\n",
      "Student model 0-5 trained with depth 34 and 3128 leaves:\n",
      "Student model score: 0.9979526948381425\n",
      "Student model 0-5 fidelity: 0.9979526951542759\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (24293, 24293) entries\n",
      "Student model 0-6 trained with depth 31 and 3106 leaves:\n",
      "Student model score: 0.9976897414700728\n",
      "Student model 0-6 fidelity: 0.9976897416787511\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (25713, 25713) entries\n",
      "Student model 0-7 trained with depth 34 and 3071 leaves:\n",
      "Student model score: 0.9948466344585996\n",
      "Student model 0-7 fidelity: 0.9948466344416027\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (27133, 27133) entries\n",
      "Student model 0-8 trained with depth 29 and 3099 leaves:\n",
      "Student model score: 0.9982561882288541\n",
      "Student model 0-8 fidelity: 0.9982561878081861\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (28553, 28553) entries\n",
      "Student model 0-9 trained with depth 34 and 3040 leaves:\n",
      "Student model score: 0.9972743543053427\n",
      "Student model 0-9 fidelity: 0.9972743547776636\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (29973, 29973) entries\n",
      "Student model 0-10 trained with depth 30 and 3038 leaves:\n",
      "Student model score: 0.9949207622027659\n",
      "Student model 0-10 fidelity: 0.9949207624400058\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (31393, 31393) entries\n",
      "Student model 0-11 trained with depth 31 and 3023 leaves:\n",
      "Student model score: 0.9935979313866045\n",
      "Student model 0-11 fidelity: 0.9935979309292592\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (32813, 32813) entries\n",
      "Student model 0-12 trained with depth 35 and 2996 leaves:\n",
      "Student model score: 0.996740946104073\n",
      "Student model 0-12 fidelity: 0.9967409462037967\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (34233, 34233) entries\n",
      "Student model 0-13 trained with depth 39 and 2999 leaves:\n",
      "Student model score: 0.9963753207833864\n",
      "Student model 0-13 fidelity: 0.9963753209365418\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (35653, 35653) entries\n",
      "Student model 0-14 trained with depth 36 and 2966 leaves:\n",
      "Student model score: 0.992688379044098\n",
      "Student model 0-14 fidelity: 0.9926883787976699\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (37073, 37073) entries\n",
      "Student model 0-15 trained with depth 33 and 2956 leaves:\n",
      "Student model score: 0.9950675391694651\n",
      "Student model 0-15 fidelity: 0.9950675388961806\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (38493, 38493) entries\n",
      "Student model 0-16 trained with depth 33 and 2975 leaves:\n",
      "Student model score: 0.9950427579670404\n",
      "Student model 0-16 fidelity: 0.9950427576660434\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (39913, 39913) entries\n",
      "Student model 0-17 trained with depth 30 and 2959 leaves:\n",
      "Student model score: 0.9955794413526969\n",
      "Student model 0-17 fidelity: 0.9955794410613199\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (41333, 41333) entries\n",
      "Student model 0-18 trained with depth 31 and 2970 leaves:\n",
      "Student model score: 0.9921649386026212\n",
      "Student model 0-18 fidelity: 0.9921649386779283\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (42753, 42753) entries\n",
      "Student model 0-19 trained with depth 30 and 2937 leaves:\n",
      "Student model score: 0.9982044828188169\n",
      "Student model 0-19 fidelity: 0.9982044829581114\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (44173, 44173) entries\n",
      "Student model 0-20 trained with depth 34 and 2919 leaves:\n",
      "Student model score: 0.9961093705727382\n",
      "Student model 0-20 fidelity: 0.9961093706490995\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (45593, 45593) entries\n",
      "Student model 0-21 trained with depth 35 and 2926 leaves:\n",
      "Student model score: 0.9983146821411277\n",
      "Student model 0-21 fidelity: 0.9983146822767957\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (47013, 47013) entries\n",
      "Student model 0-22 trained with depth 31 and 2912 leaves:\n",
      "Student model score: 0.9862551697036331\n",
      "Student model 0-22 fidelity: 0.9862551693431739\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (48433, 48433) entries\n",
      "Student model 0-23 trained with depth 31 and 2912 leaves:\n",
      "Student model score: 0.9977189201353517\n",
      "Student model 0-23 fidelity: 0.9977189201303465\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (49853, 49853) entries\n",
      "Student model 0-24 trained with depth 36 and 2912 leaves:\n",
      "Student model score: 0.9937163853720123\n",
      "Student model 0-24 fidelity: 0.9937163855497658\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (51273, 51273) entries\n",
      "Student model 0-25 trained with depth 34 and 2908 leaves:\n",
      "Student model score: 0.9971036941850786\n",
      "Student model 0-25 fidelity: 0.9971036941799374\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (52693, 52693) entries\n",
      "Student model 0-26 trained with depth 30 and 2895 leaves:\n",
      "Student model score: 0.9914574900040884\n",
      "Student model 0-26 fidelity: 0.9914574896805555\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (54113, 54113) entries\n",
      "Student model 0-27 trained with depth 31 and 2883 leaves:\n",
      "Student model score: 0.9947730152728704\n",
      "Student model 0-27 fidelity: 0.9947730153080395\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (55533, 55533) entries\n",
      "Student model 0-28 trained with depth 32 and 2889 leaves:\n",
      "Student model score: 0.9974572505711767\n",
      "Student model 0-28 fidelity: 0.9974572507637262\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (56953, 56953) entries\n",
      "Student model 0-29 trained with depth 34 and 2911 leaves:\n",
      "Student model score: 0.9947964994378771\n",
      "Student model 0-29 fidelity: 0.9947964995550189\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (58373, 58373) entries\n",
      "Student model 0-30 trained with depth 31 and 2924 leaves:\n",
      "Student model score: 0.997101848573936\n",
      "Student model 0-30 fidelity: 0.9971018485107954\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (59793, 59793) entries\n",
      "Student model 0-31 trained with depth 32 and 2908 leaves:\n",
      "Student model score: 0.9972282459182413\n",
      "Student model 0-31 fidelity: 0.9972282459630475\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (61213, 61213) entries\n",
      "Student model 0-32 trained with depth 31 and 2907 leaves:\n",
      "Student model score: 0.9973129372341504\n",
      "Student model 0-32 fidelity: 0.9973129372553357\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (62633, 62633) entries\n",
      "Student model 0-33 trained with depth 33 and 2865 leaves:\n",
      "Student model score: 0.9964009716402265\n",
      "Student model 0-33 fidelity: 0.9964009715178458\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (64053, 64053) entries\n",
      "Student model 0-34 trained with depth 32 and 2896 leaves:\n",
      "Student model score: 0.9889528524011746\n",
      "Student model 0-34 fidelity: 0.9889528523709915\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (65473, 65473) entries\n",
      "Student model 0-35 trained with depth 30 and 2853 leaves:\n",
      "Student model score: 0.9881379796241082\n",
      "Student model 0-35 fidelity: 0.9881379799614768\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (66893, 66893) entries\n",
      "Student model 0-36 trained with depth 32 and 2879 leaves:\n",
      "Student model score: 0.9925143784510907\n",
      "Student model 0-36 fidelity: 0.992514378162201\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (68313, 68313) entries\n",
      "Student model 0-37 trained with depth 30 and 2855 leaves:\n",
      "Student model score: 0.9971001990063457\n",
      "Student model 0-37 fidelity: 0.9971001989230491\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (69733, 69733) entries\n",
      "Student model 0-38 trained with depth 30 and 2870 leaves:\n",
      "Student model score: 0.9973409862176691\n",
      "Student model 0-38 fidelity: 0.9973409860297918\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (71153, 71153) entries\n",
      "Student model 0-39 trained with depth 34 and 2875 leaves:\n",
      "Student model score: 0.9912642367917452\n",
      "Student model 0-39 fidelity: 0.9912642368266016\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (72573, 72573) entries\n",
      "Student model 0-40 trained with depth 37 and 2844 leaves:\n",
      "Student model score: 0.9918720754266013\n",
      "Student model 0-40 fidelity: 0.9918720754184402\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (73993, 73993) entries\n",
      "Student model 0-41 trained with depth 33 and 2847 leaves:\n",
      "Student model score: 0.9858275786738954\n",
      "Student model 0-41 fidelity: 0.9858275787788234\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (75413, 75413) entries\n",
      "Student model 0-42 trained with depth 31 and 2859 leaves:\n",
      "Student model score: 0.9984400357333406\n",
      "Student model 0-42 fidelity: 0.9984400357880643\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (76833, 76833) entries\n",
      "Student model 0-43 trained with depth 33 and 2829 leaves:\n",
      "Student model score: 0.9920744415376263\n",
      "Student model 0-43 fidelity: 0.9920744416297647\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (78253, 78253) entries\n",
      "Student model 0-44 trained with depth 31 and 2854 leaves:\n",
      "Student model score: 0.9937318348713864\n",
      "Student model 0-44 fidelity: 0.9937318346751308\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (79673, 79673) entries\n",
      "Student model 0-45 trained with depth 29 and 2877 leaves:\n",
      "Student model score: 0.9975054575845467\n",
      "Student model 0-45 fidelity: 0.9975054573113581\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (81093, 81093) entries\n",
      "Student model 0-46 trained with depth 34 and 2875 leaves:\n",
      "Student model score: 0.995337681467306\n",
      "Student model 0-46 fidelity: 0.995337681598444\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (82513, 82513) entries\n",
      "Student model 0-47 trained with depth 36 and 2856 leaves:\n",
      "Student model score: 0.9916545379700531\n",
      "Student model 0-47 fidelity: 0.9916545382595513\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (83933, 83933) entries\n",
      "Student model 0-48 trained with depth 32 and 2873 leaves:\n",
      "Student model score: 0.9974699210590724\n",
      "Student model 0-48 fidelity: 0.9974699210914267\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (85353, 85353) entries\n",
      "Student model 0-49 trained with depth 34 and 2847 leaves:\n",
      "Student model score: 0.9974589768547593\n",
      "Student model 0-49 fidelity: 0.9974589767890915\n",
      "########## Outer-loop Iteration 1/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (86773, 86773) entries\n",
      "Student model 1-0 trained with depth 30 and 2841 leaves:\n",
      "Student model score: 0.994053200675077\n",
      "Student model 1-0 fidelity: 0.9940532001566128\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (88193, 88193) entries\n",
      "Student model 1-1 trained with depth 30 and 2846 leaves:\n",
      "Student model score: 0.9978223976803231\n",
      "Student model 1-1 fidelity: 0.9978223979837206\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (89613, 89613) entries\n",
      "Student model 1-2 trained with depth 38 and 2817 leaves:\n",
      "Student model score: 0.9981024583397724\n",
      "Student model 1-2 fidelity: 0.9981024582185841\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (91033, 91033) entries\n",
      "Student model 1-3 trained with depth 31 and 2820 leaves:\n",
      "Student model score: 0.9955490389454179\n",
      "Student model 1-3 fidelity: 0.9955490389837022\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (92453, 92453) entries\n",
      "Student model 1-4 trained with depth 30 and 2808 leaves:\n",
      "Student model score: 0.988451287839126\n",
      "Student model 1-4 fidelity: 0.9884512878963165\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (93873, 93873) entries\n",
      "Student model 1-5 trained with depth 35 and 2854 leaves:\n",
      "Student model score: 0.9977704182313625\n",
      "Student model 1-5 fidelity: 0.9977704181443781\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (95293, 95293) entries\n",
      "Student model 1-6 trained with depth 30 and 2859 leaves:\n",
      "Student model score: 0.9961992337936232\n",
      "Student model 1-6 fidelity: 0.9961992333980692\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (96713, 96713) entries\n",
      "Student model 1-7 trained with depth 31 and 2851 leaves:\n",
      "Student model score: 0.9968798554304329\n",
      "Student model 1-7 fidelity: 0.9968798556269597\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (98133, 98133) entries\n",
      "Student model 1-8 trained with depth 34 and 2834 leaves:\n",
      "Student model score: 0.9976088368621869\n",
      "Student model 1-8 fidelity: 0.997608836854358\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (99553, 99553) entries\n",
      "Student model 1-9 trained with depth 31 and 2816 leaves:\n",
      "Student model score: 0.9988121642628704\n",
      "Student model 1-9 fidelity: 0.9988121639620623\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (100973, 100973) entries\n",
      "Student model 1-10 trained with depth 33 and 2816 leaves:\n",
      "Student model score: 0.998539603190346\n",
      "Student model 1-10 fidelity: 0.9985396030670974\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (102393, 102393) entries\n",
      "Student model 1-11 trained with depth 35 and 2841 leaves:\n",
      "Student model score: 0.9828821495832057\n",
      "Student model 1-11 fidelity: 0.9828821494283883\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (103813, 103813) entries\n",
      "Student model 1-12 trained with depth 32 and 2789 leaves:\n",
      "Student model score: 0.9914475284393279\n",
      "Student model 1-12 fidelity: 0.9914475281701708\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (105233, 105233) entries\n",
      "Student model 1-13 trained with depth 32 and 2852 leaves:\n",
      "Student model score: 0.9969170349478231\n",
      "Student model 1-13 fidelity: 0.996917034835573\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (106653, 106653) entries\n",
      "Student model 1-14 trained with depth 30 and 2830 leaves:\n",
      "Student model score: 0.9986170622249302\n",
      "Student model 1-14 fidelity: 0.9986170622497645\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (108073, 108073) entries\n",
      "Student model 1-15 trained with depth 28 and 2807 leaves:\n",
      "Student model score: 0.9944280543729882\n",
      "Student model 1-15 fidelity: 0.9944280544821051\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (109493, 109493) entries\n",
      "Student model 1-16 trained with depth 33 and 2835 leaves:\n",
      "Student model score: 0.99729804223096\n",
      "Student model 1-16 fidelity: 0.9972980421148269\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (110913, 110913) entries\n",
      "Student model 1-17 trained with depth 32 and 2836 leaves:\n",
      "Student model score: 0.9938439460951941\n",
      "Student model 1-17 fidelity: 0.9938439461076376\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (112333, 112333) entries\n",
      "Student model 1-18 trained with depth 43 and 2823 leaves:\n",
      "Student model score: 0.9984980945783285\n",
      "Student model 1-18 fidelity: 0.9984980945640324\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (113753, 113753) entries\n",
      "Student model 1-19 trained with depth 34 and 2833 leaves:\n",
      "Student model score: 0.9923532716859003\n",
      "Student model 1-19 fidelity: 0.9923532716098276\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (115173, 115173) entries\n",
      "Student model 1-20 trained with depth 30 and 2814 leaves:\n",
      "Student model score: 0.9977698439074406\n",
      "Student model 1-20 fidelity: 0.9977698436975712\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (116593, 116593) entries\n",
      "Student model 1-21 trained with depth 34 and 2825 leaves:\n",
      "Student model score: 0.9979430581762279\n",
      "Student model 1-21 fidelity: 0.9979430580286983\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (118013, 118013) entries\n",
      "Student model 1-22 trained with depth 31 and 2801 leaves:\n",
      "Student model score: 0.9966181280623234\n",
      "Student model 1-22 fidelity: 0.9966181277418429\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (119433, 119433) entries\n",
      "Student model 1-23 trained with depth 30 and 2823 leaves:\n",
      "Student model score: 0.997972734218486\n",
      "Student model 1-23 fidelity: 0.9979727342249619\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (120853, 120853) entries\n",
      "Student model 1-24 trained with depth 34 and 2785 leaves:\n",
      "Student model score: 0.9986075357509292\n",
      "Student model 1-24 fidelity: 0.9986075359187576\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (122273, 122273) entries\n",
      "Student model 1-25 trained with depth 31 and 2806 leaves:\n",
      "Student model score: 0.9972910584623846\n",
      "Student model 1-25 fidelity: 0.997291058420681\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (123693, 123693) entries\n",
      "Student model 1-26 trained with depth 32 and 2847 leaves:\n",
      "Student model score: 0.996614496568298\n",
      "Student model 1-26 fidelity: 0.9966144965300728\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (125113, 125113) entries\n",
      "Student model 1-27 trained with depth 33 and 2790 leaves:\n",
      "Student model score: 0.9987291439963375\n",
      "Student model 1-27 fidelity: 0.9987291440105808\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (126533, 126533) entries\n",
      "Student model 1-28 trained with depth 32 and 2809 leaves:\n",
      "Student model score: 0.9967568296265196\n",
      "Student model 1-28 fidelity: 0.9967568296104394\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (127953, 127953) entries\n",
      "Student model 1-29 trained with depth 31 and 2813 leaves:\n",
      "Student model score: 0.9980603794714231\n",
      "Student model 1-29 fidelity: 0.998060379514183\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (129373, 129373) entries\n",
      "Student model 1-30 trained with depth 31 and 2826 leaves:\n",
      "Student model score: 0.996912495936922\n",
      "Student model 1-30 fidelity: 0.9969124959418665\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (130793, 130793) entries\n",
      "Student model 1-31 trained with depth 32 and 2807 leaves:\n",
      "Student model score: 0.9941411628530971\n",
      "Student model 1-31 fidelity: 0.9941411625676548\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (132213, 132213) entries\n",
      "Student model 1-32 trained with depth 33 and 2858 leaves:\n",
      "Student model score: 0.9921903749026254\n",
      "Student model 1-32 fidelity: 0.9921903748277199\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (133633, 133633) entries\n",
      "Student model 1-33 trained with depth 31 and 2801 leaves:\n",
      "Student model score: 0.9972335701979131\n",
      "Student model 1-33 fidelity: 0.9972335700761057\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (135053, 135053) entries\n",
      "Student model 1-34 trained with depth 31 and 2837 leaves:\n",
      "Student model score: 0.9931420369640936\n",
      "Student model 1-34 fidelity: 0.9931420369794768\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (136473, 136473) entries\n",
      "Student model 1-35 trained with depth 38 and 2842 leaves:\n",
      "Student model score: 0.9963338840051585\n",
      "Student model 1-35 fidelity: 0.9963338841289788\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (137893, 137893) entries\n",
      "Student model 1-36 trained with depth 33 and 2815 leaves:\n",
      "Student model score: 0.9956677040843599\n",
      "Student model 1-36 fidelity: 0.9956677044145281\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (139313, 139313) entries\n",
      "Student model 1-37 trained with depth 30 and 2806 leaves:\n",
      "Student model score: 0.9927222110454956\n",
      "Student model 1-37 fidelity: 0.9927222110956705\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (140733, 140733) entries\n",
      "Student model 1-38 trained with depth 36 and 2835 leaves:\n",
      "Student model score: 0.9942283174934924\n",
      "Student model 1-38 fidelity: 0.9942283175293092\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (142153, 142153) entries\n",
      "Student model 1-39 trained with depth 32 and 2805 leaves:\n",
      "Student model score: 0.9849190790688\n",
      "Student model 1-39 fidelity: 0.9849190791388025\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (143573, 143573) entries\n",
      "Student model 1-40 trained with depth 33 and 2761 leaves:\n",
      "Student model score: 0.9856164212524038\n",
      "Student model 1-40 fidelity: 0.9856164212251627\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (144993, 144993) entries\n",
      "Student model 1-41 trained with depth 36 and 2804 leaves:\n",
      "Student model score: 0.9908120367518776\n",
      "Student model 1-41 fidelity: 0.9908120366580331\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (146413, 146413) entries\n",
      "Student model 1-42 trained with depth 30 and 2810 leaves:\n",
      "Student model score: 0.9975304643185856\n",
      "Student model 1-42 fidelity: 0.9975304645065423\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (147833, 147833) entries\n",
      "Student model 1-43 trained with depth 31 and 2794 leaves:\n",
      "Student model score: 0.9952134516373253\n",
      "Student model 1-43 fidelity: 0.9952134514438751\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (149253, 149253) entries\n",
      "Student model 1-44 trained with depth 33 and 2814 leaves:\n",
      "Student model score: 0.9970491458194984\n",
      "Student model 1-44 fidelity: 0.9970491458150804\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (150673, 150673) entries\n",
      "Student model 1-45 trained with depth 33 and 2789 leaves:\n",
      "Student model score: 0.996855061668535\n",
      "Student model 1-45 fidelity: 0.996855061841371\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (152093, 152093) entries\n",
      "Student model 1-46 trained with depth 33 and 2829 leaves:\n",
      "Student model score: 0.997889887452633\n",
      "Student model 1-46 fidelity: 0.9978898874481984\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (153513, 153513) entries\n",
      "Student model 1-47 trained with depth 31 and 2786 leaves:\n",
      "Student model score: 0.9969425301368379\n",
      "Student model 1-47 fidelity: 0.9969425301275403\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (154933, 154933) entries\n",
      "Student model 1-48 trained with depth 32 and 2791 leaves:\n",
      "Student model score: 0.9960488144399391\n",
      "Student model 1-48 fidelity: 0.9960488147947444\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (156353, 156353) entries\n",
      "Student model 1-49 trained with depth 35 and 2823 leaves:\n",
      "Student model score: 0.9975208566084723\n",
      "Student model 1-49 fidelity: 0.9975208566403199\n",
      "########## Outer-loop Iteration 2/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (157773, 157773) entries\n",
      "Student model 2-0 trained with depth 33 and 2803 leaves:\n",
      "Student model score: 0.9975654750623183\n",
      "Student model 2-0 fidelity: 0.997565475015558\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (159193, 159193) entries\n",
      "Student model 2-1 trained with depth 34 and 2797 leaves:\n",
      "Student model score: 0.9967687100220426\n",
      "Student model 2-1 fidelity: 0.9967687100077058\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (160613, 160613) entries\n",
      "Student model 2-2 trained with depth 31 and 2810 leaves:\n",
      "Student model score: 0.9973583701258771\n",
      "Student model 2-2 fidelity: 0.9973583704191112\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (162033, 162033) entries\n",
      "Student model 2-3 trained with depth 36 and 2782 leaves:\n",
      "Student model score: 0.9875352925774987\n",
      "Student model 2-3 fidelity: 0.9875352925612368\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (163453, 163453) entries\n",
      "Student model 2-4 trained with depth 32 and 2825 leaves:\n",
      "Student model score: 0.9980229587323919\n",
      "Student model 2-4 fidelity: 0.9980229587988947\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (164873, 164873) entries\n",
      "Student model 2-5 trained with depth 32 and 2801 leaves:\n",
      "Student model score: 0.9975353813001925\n",
      "Student model 2-5 fidelity: 0.9975353811692437\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (166293, 166293) entries\n",
      "Student model 2-6 trained with depth 33 and 2783 leaves:\n",
      "Student model score: 0.9936278972539996\n",
      "Student model 2-6 fidelity: 0.9936278972416681\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (167713, 167713) entries\n",
      "Student model 2-7 trained with depth 36 and 2782 leaves:\n",
      "Student model score: 0.9956079904373796\n",
      "Student model 2-7 fidelity: 0.9956079904209227\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (169133, 169133) entries\n",
      "Student model 2-8 trained with depth 33 and 2824 leaves:\n",
      "Student model score: 0.9939389468285686\n",
      "Student model 2-8 fidelity: 0.9939389469217444\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (170553, 170553) entries\n",
      "Student model 2-9 trained with depth 32 and 2802 leaves:\n",
      "Student model score: 0.9943700431043809\n",
      "Student model 2-9 fidelity: 0.9943700433924206\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (171973, 171973) entries\n",
      "Student model 2-10 trained with depth 33 and 2837 leaves:\n",
      "Student model score: 0.9979618155576566\n",
      "Student model 2-10 fidelity: 0.9979618155385017\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (173393, 173393) entries\n",
      "Student model 2-11 trained with depth 33 and 2806 leaves:\n",
      "Student model score: 0.984724080940727\n",
      "Student model 2-11 fidelity: 0.9847240808290306\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (174813, 174813) entries\n",
      "Student model 2-12 trained with depth 30 and 2806 leaves:\n",
      "Student model score: 0.996350824578472\n",
      "Student model 2-12 fidelity: 0.9963508246459571\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (176233, 176233) entries\n",
      "Student model 2-13 trained with depth 29 and 2789 leaves:\n",
      "Student model score: 0.9867885551934185\n",
      "Student model 2-13 fidelity: 0.9867885551713337\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (177653, 177653) entries\n",
      "Student model 2-14 trained with depth 40 and 2821 leaves:\n",
      "Student model score: 0.994150199390975\n",
      "Student model 2-14 fidelity: 0.9941501993532758\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (179073, 179073) entries\n",
      "Student model 2-15 trained with depth 32 and 2822 leaves:\n",
      "Student model score: 0.9977125214129389\n",
      "Student model 2-15 fidelity: 0.9977125215419026\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (180493, 180493) entries\n",
      "Student model 2-16 trained with depth 36 and 2808 leaves:\n",
      "Student model score: 0.9982420482242024\n",
      "Student model 2-16 fidelity: 0.9982420481999809\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (181913, 181913) entries\n",
      "Student model 2-17 trained with depth 34 and 2821 leaves:\n",
      "Student model score: 0.9975140566725547\n",
      "Student model 2-17 fidelity: 0.9975140566147952\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (183333, 183333) entries\n",
      "Student model 2-18 trained with depth 35 and 2796 leaves:\n",
      "Student model score: 0.9970877619761442\n",
      "Student model 2-18 fidelity: 0.9970877619708272\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (184753, 184753) entries\n",
      "Student model 2-19 trained with depth 31 and 2811 leaves:\n",
      "Student model score: 0.99816414886337\n",
      "Student model 2-19 fidelity: 0.9981641488657435\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (186173, 186173) entries\n",
      "Student model 2-20 trained with depth 35 and 2829 leaves:\n",
      "Student model score: 0.9961241774396284\n",
      "Student model 2-20 fidelity: 0.9961241771200577\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (187593, 187593) entries\n",
      "Student model 2-21 trained with depth 33 and 2782 leaves:\n",
      "Student model score: 0.9969719922740164\n",
      "Student model 2-21 fidelity: 0.9969719922487303\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (189013, 189013) entries\n",
      "Student model 2-22 trained with depth 31 and 2787 leaves:\n",
      "Student model score: 0.991485432627363\n",
      "Student model 2-22 fidelity: 0.9914854326470113\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (190433, 190433) entries\n",
      "Student model 2-23 trained with depth 33 and 2795 leaves:\n",
      "Student model score: 0.9954945881542779\n",
      "Student model 2-23 fidelity: 0.9954945881344935\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (191853, 191853) entries\n",
      "Student model 2-24 trained with depth 32 and 2790 leaves:\n",
      "Student model score: 0.995831260895588\n",
      "Student model 2-24 fidelity: 0.995831260971454\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (193273, 193273) entries\n",
      "Student model 2-25 trained with depth 37 and 2776 leaves:\n",
      "Student model score: 0.9971818910319465\n",
      "Student model 2-25 fidelity: 0.9971818910068345\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (194693, 194693) entries\n",
      "Student model 2-26 trained with depth 32 and 2804 leaves:\n",
      "Student model score: 0.9976855775738944\n",
      "Student model 2-26 fidelity: 0.9976855773858803\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (196113, 196113) entries\n",
      "Student model 2-27 trained with depth 35 and 2786 leaves:\n",
      "Student model score: 0.9949560322747758\n",
      "Student model 2-27 fidelity: 0.9949560324044211\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (197533, 197533) entries\n",
      "Student model 2-28 trained with depth 34 and 2783 leaves:\n",
      "Student model score: 0.9947292269600801\n",
      "Student model 2-28 fidelity: 0.9947292269531072\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (198953, 198953) entries\n",
      "Student model 2-29 trained with depth 34 and 2786 leaves:\n",
      "Student model score: 0.99725028828965\n",
      "Student model 2-29 fidelity: 0.9972502882893205\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (200373, 200373) entries\n",
      "Student model 2-30 trained with depth 32 and 2800 leaves:\n",
      "Student model score: 0.9974033111367152\n",
      "Student model 2-30 fidelity: 0.9974033112104503\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (201793, 201793) entries\n",
      "Student model 2-31 trained with depth 34 and 2820 leaves:\n",
      "Student model score: 0.9970389771234696\n",
      "Student model 2-31 fidelity: 0.9970389771373409\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (203213, 203213) entries\n",
      "Student model 2-32 trained with depth 34 and 2787 leaves:\n",
      "Student model score: 0.9906488778155179\n",
      "Student model 2-32 fidelity: 0.9906488783616522\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (204633, 204633) entries\n",
      "Student model 2-33 trained with depth 28 and 2797 leaves:\n",
      "Student model score: 0.9983871583862217\n",
      "Student model 2-33 fidelity: 0.998387158225924\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (206053, 206053) entries\n",
      "Student model 2-34 trained with depth 31 and 2798 leaves:\n",
      "Student model score: 0.9975853837928431\n",
      "Student model 2-34 fidelity: 0.9975853839217694\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (207473, 207473) entries\n",
      "Student model 2-35 trained with depth 32 and 2807 leaves:\n",
      "Student model score: 0.9922524800135278\n",
      "Student model 2-35 fidelity: 0.9922524799864377\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (208893, 208893) entries\n",
      "Student model 2-36 trained with depth 34 and 2774 leaves:\n",
      "Student model score: 0.9976193326163533\n",
      "Student model 2-36 fidelity: 0.9976193326545582\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (210313, 210313) entries\n",
      "Student model 2-37 trained with depth 30 and 2795 leaves:\n",
      "Student model score: 0.9930514414200976\n",
      "Student model 2-37 fidelity: 0.9930514414123062\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (211733, 211733) entries\n",
      "Student model 2-38 trained with depth 33 and 2806 leaves:\n",
      "Student model score: 0.9906916108023252\n",
      "Student model 2-38 fidelity: 0.9906916107427943\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (213153, 213153) entries\n",
      "Student model 2-39 trained with depth 32 and 2820 leaves:\n",
      "Student model score: 0.9986253945715337\n",
      "Student model 2-39 fidelity: 0.9986253945695721\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (214573, 214573) entries\n",
      "Student model 2-40 trained with depth 32 and 2809 leaves:\n",
      "Student model score: 0.9973351778789903\n",
      "Student model 2-40 fidelity: 0.9973351778786953\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (215993, 215993) entries\n",
      "Student model 2-41 trained with depth 33 and 2809 leaves:\n",
      "Student model score: 0.9910692536491988\n",
      "Student model 2-41 fidelity: 0.9910692534204378\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (217413, 217413) entries\n",
      "Student model 2-42 trained with depth 33 and 2797 leaves:\n",
      "Student model score: 0.998653091640103\n",
      "Student model 2-42 fidelity: 0.9986530917566536\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (218833, 218833) entries\n",
      "Student model 2-43 trained with depth 35 and 2794 leaves:\n",
      "Student model score: 0.9931648680249605\n",
      "Student model 2-43 fidelity: 0.9931648680310855\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (220253, 220253) entries\n",
      "Student model 2-44 trained with depth 35 and 2803 leaves:\n",
      "Student model score: 0.9916929884572382\n",
      "Student model 2-44 fidelity: 0.9916929883886824\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (221673, 221673) entries\n",
      "Student model 2-45 trained with depth 28 and 2793 leaves:\n",
      "Student model score: 0.9924604709749892\n",
      "Student model 2-45 fidelity: 0.9924604708518289\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (223093, 223093) entries\n",
      "Student model 2-46 trained with depth 33 and 2807 leaves:\n",
      "Student model score: 0.9974172327154895\n",
      "Student model 2-46 fidelity: 0.9974172327135974\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (224513, 224513) entries\n",
      "Student model 2-47 trained with depth 28 and 2751 leaves:\n",
      "Student model score: 0.9986334675324584\n",
      "Student model 2-47 fidelity: 0.9986334674633395\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (225933, 225933) entries\n",
      "Student model 2-48 trained with depth 32 and 2798 leaves:\n",
      "Student model score: 0.9947281051386095\n",
      "Student model 2-48 fidelity: 0.9947281051447943\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (227353, 227353) entries\n",
      "Student model 2-49 trained with depth 33 and 2772 leaves:\n",
      "Student model score: 0.9981945436182629\n",
      "Student model 2-49 fidelity: 0.9981945435565863\n",
      "########## Outer-loop Iteration 3/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (228773, 228773) entries\n",
      "Student model 3-0 trained with depth 31 and 2794 leaves:\n",
      "Student model score: 0.9945462201916158\n",
      "Student model 3-0 fidelity: 0.9945462201130105\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (230193, 230193) entries\n",
      "Student model 3-1 trained with depth 28 and 2794 leaves:\n",
      "Student model score: 0.9966648242614115\n",
      "Student model 3-1 fidelity: 0.9966648242143895\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (231613, 231613) entries\n",
      "Student model 3-2 trained with depth 36 and 2792 leaves:\n",
      "Student model score: 0.9940983731841853\n",
      "Student model 3-2 fidelity: 0.9940983729869103\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (233033, 233033) entries\n",
      "Student model 3-3 trained with depth 32 and 2788 leaves:\n",
      "Student model score: 0.9953154172102484\n",
      "Student model 3-3 fidelity: 0.9953154170720977\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (234453, 234453) entries\n",
      "Student model 3-4 trained with depth 33 and 2758 leaves:\n",
      "Student model score: 0.9865935295250635\n",
      "Student model 3-4 fidelity: 0.9865935296341343\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (235873, 235873) entries\n",
      "Student model 3-5 trained with depth 31 and 2788 leaves:\n",
      "Student model score: 0.9966975703494759\n",
      "Student model 3-5 fidelity: 0.9966975703394453\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (237293, 237293) entries\n",
      "Student model 3-6 trained with depth 30 and 2776 leaves:\n",
      "Student model score: 0.9937097046454614\n",
      "Student model 3-6 fidelity: 0.9937097047258759\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (238713, 238713) entries\n",
      "Student model 3-7 trained with depth 31 and 2797 leaves:\n",
      "Student model score: 0.9968012041151723\n",
      "Student model 3-7 fidelity: 0.9968012036223397\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (240133, 240133) entries\n",
      "Student model 3-8 trained with depth 34 and 2804 leaves:\n",
      "Student model score: 0.9915591943314208\n",
      "Student model 3-8 fidelity: 0.9915591943627095\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (241553, 241553) entries\n",
      "Student model 3-9 trained with depth 29 and 2761 leaves:\n",
      "Student model score: 0.9975920541821204\n",
      "Student model 3-9 fidelity: 0.9975920541866706\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (242973, 242973) entries\n",
      "Student model 3-10 trained with depth 34 and 2769 leaves:\n",
      "Student model score: 0.9982921511808311\n",
      "Student model 3-10 fidelity: 0.9982921511966552\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (244393, 244393) entries\n",
      "Student model 3-11 trained with depth 34 and 2820 leaves:\n",
      "Student model score: 0.9970922620354491\n",
      "Student model 3-11 fidelity: 0.9970922619993604\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (245813, 245813) entries\n",
      "Student model 3-12 trained with depth 33 and 2776 leaves:\n",
      "Student model score: 0.996558876914446\n",
      "Student model 3-12 fidelity: 0.9965588769231982\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (247233, 247233) entries\n",
      "Student model 3-13 trained with depth 30 and 2806 leaves:\n",
      "Student model score: 0.9958154785619068\n",
      "Student model 3-13 fidelity: 0.995815478564184\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (248653, 248653) entries\n",
      "Student model 3-14 trained with depth 28 and 2797 leaves:\n",
      "Student model score: 0.997542277382279\n",
      "Student model 3-14 fidelity: 0.9975422773862735\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (250073, 250073) entries\n",
      "Student model 3-15 trained with depth 35 and 2767 leaves:\n",
      "Student model score: 0.9973244404357302\n",
      "Student model 3-15 fidelity: 0.9973244404104805\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (251493, 251493) entries\n",
      "Student model 3-16 trained with depth 35 and 2812 leaves:\n",
      "Student model score: 0.9979109711716153\n",
      "Student model 3-16 fidelity: 0.9979109711745362\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (252913, 252913) entries\n",
      "Student model 3-17 trained with depth 33 and 2789 leaves:\n",
      "Student model score: 0.9981014731732087\n",
      "Student model 3-17 fidelity: 0.9981014731852774\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (254333, 254333) entries\n",
      "Student model 3-18 trained with depth 34 and 2802 leaves:\n",
      "Student model score: 0.9974203448276694\n",
      "Student model 3-18 fidelity: 0.9974203449716373\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (255753, 255753) entries\n",
      "Student model 3-19 trained with depth 38 and 2770 leaves:\n",
      "Student model score: 0.9986074241733026\n",
      "Student model 3-19 fidelity: 0.9986074241660187\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (257173, 257173) entries\n",
      "Student model 3-20 trained with depth 33 and 2775 leaves:\n",
      "Student model score: 0.9891399047585451\n",
      "Student model 3-20 fidelity: 0.9891399048069206\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (258593, 258593) entries\n",
      "Student model 3-21 trained with depth 33 and 2782 leaves:\n",
      "Student model score: 0.9955319080835048\n",
      "Student model 3-21 fidelity: 0.9955319081111518\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (260013, 260013) entries\n",
      "Student model 3-22 trained with depth 34 and 2796 leaves:\n",
      "Student model score: 0.9925284380429404\n",
      "Student model 3-22 fidelity: 0.9925284381358476\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (261433, 261433) entries\n",
      "Student model 3-23 trained with depth 35 and 2783 leaves:\n",
      "Student model score: 0.9948188874812609\n",
      "Student model 3-23 fidelity: 0.9948188874773491\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (262853, 262853) entries\n",
      "Student model 3-24 trained with depth 34 and 2769 leaves:\n",
      "Student model score: 0.9978894581812314\n",
      "Student model 3-24 fidelity: 0.997889457966394\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (264273, 264273) entries\n",
      "Student model 3-25 trained with depth 29 and 2765 leaves:\n",
      "Student model score: 0.9944541779174004\n",
      "Student model 3-25 fidelity: 0.9944541779102468\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (265693, 265693) entries\n",
      "Student model 3-26 trained with depth 30 and 2787 leaves:\n",
      "Student model score: 0.9972027437844362\n",
      "Student model 3-26 fidelity: 0.9972027438000841\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (267113, 267113) entries\n",
      "Student model 3-27 trained with depth 33 and 2809 leaves:\n",
      "Student model score: 0.9919901448174019\n",
      "Student model 3-27 fidelity: 0.9919901448355212\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (268533, 268533) entries\n",
      "Student model 3-28 trained with depth 30 and 2780 leaves:\n",
      "Student model score: 0.9979489064548367\n",
      "Student model 3-28 fidelity: 0.9979489064580875\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (269953, 269953) entries\n",
      "Student model 3-29 trained with depth 37 and 2802 leaves:\n",
      "Student model score: 0.9985425925777294\n",
      "Student model 3-29 fidelity: 0.9985425925662599\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (271373, 271373) entries\n",
      "Student model 3-30 trained with depth 34 and 2783 leaves:\n",
      "Student model score: 0.9974813395046505\n",
      "Student model 3-30 fidelity: 0.9974813394917033\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (272793, 272793) entries\n",
      "Student model 3-31 trained with depth 33 and 2789 leaves:\n",
      "Student model score: 0.9942920768818851\n",
      "Student model 3-31 fidelity: 0.9942920768874349\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (274213, 274213) entries\n",
      "Student model 3-32 trained with depth 33 and 2785 leaves:\n",
      "Student model score: 0.9972004195146293\n",
      "Student model 3-32 fidelity: 0.9972004195195466\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (275633, 275633) entries\n",
      "Student model 3-33 trained with depth 34 and 2742 leaves:\n",
      "Student model score: 0.9930890439944356\n",
      "Student model 3-33 fidelity: 0.9930890439718211\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (277053, 277053) entries\n",
      "Student model 3-34 trained with depth 32 and 2803 leaves:\n",
      "Student model score: 0.9970661097435637\n",
      "Student model 3-34 fidelity: 0.9970661095740906\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (278473, 278473) entries\n",
      "Student model 3-35 trained with depth 32 and 2796 leaves:\n",
      "Student model score: 0.9925070154969788\n",
      "Student model 3-35 fidelity: 0.9925070156175542\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (279893, 279893) entries\n",
      "Student model 3-36 trained with depth 39 and 2784 leaves:\n",
      "Student model score: 0.9948381768896463\n",
      "Student model 3-36 fidelity: 0.994838176877933\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (281313, 281313) entries\n",
      "Student model 3-37 trained with depth 34 and 2773 leaves:\n",
      "Student model score: 0.991636927914302\n",
      "Student model 3-37 fidelity: 0.9916369279451871\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (282733, 282733) entries\n",
      "Student model 3-38 trained with depth 33 and 2772 leaves:\n",
      "Student model score: 0.9956698125401758\n",
      "Student model 3-38 fidelity: 0.9956698125638787\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (284153, 284153) entries\n",
      "Student model 3-39 trained with depth 30 and 2789 leaves:\n",
      "Student model score: 0.9954575884805158\n",
      "Student model 3-39 fidelity: 0.9954575885039507\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (285573, 285573) entries\n",
      "Student model 3-40 trained with depth 37 and 2772 leaves:\n",
      "Student model score: 0.9947778226513319\n",
      "Student model 3-40 fidelity: 0.994777822657423\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (286993, 286993) entries\n",
      "Student model 3-41 trained with depth 31 and 2765 leaves:\n",
      "Student model score: 0.9927777482546117\n",
      "Student model 3-41 fidelity: 0.992777748206486\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (288413, 288413) entries\n",
      "Student model 3-42 trained with depth 32 and 2765 leaves:\n",
      "Student model score: 0.9941663616119276\n",
      "Student model 3-42 fidelity: 0.994166361612134\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (289833, 289833) entries\n",
      "Student model 3-43 trained with depth 29 and 2784 leaves:\n",
      "Student model score: 0.9919123285647763\n",
      "Student model 3-43 fidelity: 0.9919123285195164\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (291253, 291253) entries\n",
      "Student model 3-44 trained with depth 34 and 2786 leaves:\n",
      "Student model score: 0.998131451832336\n",
      "Student model 3-44 fidelity: 0.9981314518771349\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (292673, 292673) entries\n",
      "Student model 3-45 trained with depth 40 and 2752 leaves:\n",
      "Student model score: 0.9942472314941649\n",
      "Student model 3-45 fidelity: 0.9942472316340585\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (294093, 294093) entries\n",
      "Student model 3-46 trained with depth 31 and 2766 leaves:\n",
      "Student model score: 0.9941941289386865\n",
      "Student model 3-46 fidelity: 0.9941941289408409\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (295513, 295513) entries\n",
      "Student model 3-47 trained with depth 33 and 2777 leaves:\n",
      "Student model score: 0.9988333983038291\n",
      "Student model 3-47 fidelity: 0.9988333983113294\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (296933, 296933) entries\n",
      "Student model 3-48 trained with depth 30 and 2804 leaves:\n",
      "Student model score: 0.9923707018569278\n",
      "Student model 3-48 fidelity: 0.9923707017121932\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (298353, 298353) entries\n",
      "Student model 3-49 trained with depth 35 and 2765 leaves:\n",
      "Student model score: 0.9947259035644622\n",
      "Student model 3-49 fidelity: 0.9947259035446336\n",
      "########## Outer-loop Iteration 4/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (299773, 299773) entries\n",
      "Student model 4-0 trained with depth 34 and 2801 leaves:\n",
      "Student model score: 0.9970719988213892\n",
      "Student model 4-0 fidelity: 0.9970719988523825\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (301193, 301193) entries\n",
      "Student model 4-1 trained with depth 33 and 2740 leaves:\n",
      "Student model score: 0.9936373599302362\n",
      "Student model 4-1 fidelity: 0.9936373598810743\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (302613, 302613) entries\n",
      "Student model 4-2 trained with depth 32 and 2765 leaves:\n",
      "Student model score: 0.9927339000972449\n",
      "Student model 4-2 fidelity: 0.9927339000806052\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (304033, 304033) entries\n",
      "Student model 4-3 trained with depth 30 and 2810 leaves:\n",
      "Student model score: 0.9979399119246106\n",
      "Student model 4-3 fidelity: 0.9979399120674065\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (305453, 305453) entries\n",
      "Student model 4-4 trained with depth 31 and 2784 leaves:\n",
      "Student model score: 0.9961135657939534\n",
      "Student model 4-4 fidelity: 0.996113565873487\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (306873, 306873) entries\n",
      "Student model 4-5 trained with depth 32 and 2791 leaves:\n",
      "Student model score: 0.9944928028645029\n",
      "Student model 4-5 fidelity: 0.9944928029256537\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (308293, 308293) entries\n",
      "Student model 4-6 trained with depth 35 and 2796 leaves:\n",
      "Student model score: 0.9974998411376993\n",
      "Student model 4-6 fidelity: 0.9974998410600779\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (309713, 309713) entries\n",
      "Student model 4-7 trained with depth 31 and 2794 leaves:\n",
      "Student model score: 0.9982318500295737\n",
      "Student model 4-7 fidelity: 0.9982318503521339\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (311133, 311133) entries\n",
      "Student model 4-8 trained with depth 36 and 2784 leaves:\n",
      "Student model score: 0.9986886377087786\n",
      "Student model 4-8 fidelity: 0.998688637445821\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (312553, 312553) entries\n",
      "Student model 4-9 trained with depth 38 and 2777 leaves:\n",
      "Student model score: 0.9934289084837713\n",
      "Student model 4-9 fidelity: 0.9934289085319249\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (313973, 313973) entries\n",
      "Student model 4-10 trained with depth 30 and 2783 leaves:\n",
      "Student model score: 0.9985227182722228\n",
      "Student model 4-10 fidelity: 0.9985227182679005\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (315393, 315393) entries\n",
      "Student model 4-11 trained with depth 32 and 2798 leaves:\n",
      "Student model score: 0.998853968735564\n",
      "Student model 4-11 fidelity: 0.9988539687969157\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (316813, 316813) entries\n",
      "Student model 4-12 trained with depth 42 and 2791 leaves:\n",
      "Student model score: 0.9939176954445375\n",
      "Student model 4-12 fidelity: 0.9939176954598768\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (318233, 318233) entries\n",
      "Student model 4-13 trained with depth 31 and 2762 leaves:\n",
      "Student model score: 0.9941000741137306\n",
      "Student model 4-13 fidelity: 0.9941000740647989\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (319653, 319653) entries\n",
      "Student model 4-14 trained with depth 35 and 2797 leaves:\n",
      "Student model score: 0.99670678549203\n",
      "Student model 4-14 fidelity: 0.9967067854932836\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (321073, 321073) entries\n",
      "Student model 4-15 trained with depth 30 and 2784 leaves:\n",
      "Student model score: 0.9968728840126589\n",
      "Student model 4-15 fidelity: 0.9968728839532183\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (322493, 322493) entries\n",
      "Student model 4-16 trained with depth 31 and 2769 leaves:\n",
      "Student model score: 0.9939073535997442\n",
      "Student model 4-16 fidelity: 0.9939073536033047\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (323913, 323913) entries\n",
      "Student model 4-17 trained with depth 33 and 2787 leaves:\n",
      "Student model score: 0.9859602499415376\n",
      "Student model 4-17 fidelity: 0.9859602498339733\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (325333, 325333) entries\n",
      "Student model 4-18 trained with depth 34 and 2760 leaves:\n",
      "Student model score: 0.9776540188404178\n",
      "Student model 4-18 fidelity: 0.9776540188883711\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (326753, 326753) entries\n",
      "Student model 4-19 trained with depth 34 and 2760 leaves:\n",
      "Student model score: 0.9946847804590216\n",
      "Student model 4-19 fidelity: 0.99468478041884\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (328173, 328173) entries\n",
      "Student model 4-20 trained with depth 30 and 2796 leaves:\n",
      "Student model score: 0.9910479799664526\n",
      "Student model 4-20 fidelity: 0.9910479797894635\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (329593, 329593) entries\n",
      "Student model 4-21 trained with depth 32 and 2791 leaves:\n",
      "Student model score: 0.9985504377858334\n",
      "Student model 4-21 fidelity: 0.9985504377950077\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (331013, 331013) entries\n",
      "Student model 4-22 trained with depth 33 and 2761 leaves:\n",
      "Student model score: 0.9983137156494227\n",
      "Student model 4-22 fidelity: 0.9983137156496136\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (332433, 332433) entries\n",
      "Student model 4-23 trained with depth 35 and 2776 leaves:\n",
      "Student model score: 0.9976965613219079\n",
      "Student model 4-23 fidelity: 0.9976965612015237\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (333853, 333853) entries\n",
      "Student model 4-24 trained with depth 35 and 2785 leaves:\n",
      "Student model score: 0.989803701485874\n",
      "Student model 4-24 fidelity: 0.9898037014928398\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (335273, 335273) entries\n",
      "Student model 4-25 trained with depth 30 and 2776 leaves:\n",
      "Student model score: 0.9959051422160635\n",
      "Student model 4-25 fidelity: 0.9959051422182446\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (336693, 336693) entries\n",
      "Student model 4-26 trained with depth 33 and 2761 leaves:\n",
      "Student model score: 0.9970999942398149\n",
      "Student model 4-26 fidelity: 0.997099994295521\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (338113, 338113) entries\n",
      "Student model 4-27 trained with depth 29 and 2751 leaves:\n",
      "Student model score: 0.9973466817811538\n",
      "Student model 4-27 fidelity: 0.9973466817150536\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (339533, 339533) entries\n",
      "Student model 4-28 trained with depth 31 and 2761 leaves:\n",
      "Student model score: 0.9955219284140997\n",
      "Student model 4-28 fidelity: 0.9955219283968009\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (340953, 340953) entries\n",
      "Student model 4-29 trained with depth 32 and 2763 leaves:\n",
      "Student model score: 0.9955388406542737\n",
      "Student model 4-29 fidelity: 0.9955388406550378\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (342373, 342373) entries\n",
      "Student model 4-30 trained with depth 36 and 2746 leaves:\n",
      "Student model score: 0.996112448835607\n",
      "Student model 4-30 fidelity: 0.9961124487121299\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (343793, 343793) entries\n",
      "Student model 4-31 trained with depth 31 and 2813 leaves:\n",
      "Student model score: 0.9986561452657876\n",
      "Student model 4-31 fidelity: 0.9986561452611115\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (345213, 345213) entries\n",
      "Student model 4-32 trained with depth 36 and 2766 leaves:\n",
      "Student model score: 0.997651576118695\n",
      "Student model 4-32 fidelity: 0.9976515761167025\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (346633, 346633) entries\n",
      "Student model 4-33 trained with depth 31 and 2770 leaves:\n",
      "Student model score: 0.9987110274262658\n",
      "Student model 4-33 fidelity: 0.9987110274564479\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (348053, 348053) entries\n",
      "Student model 4-34 trained with depth 33 and 2765 leaves:\n",
      "Student model score: 0.9965036110883834\n",
      "Student model 4-34 fidelity: 0.9965036110642844\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (349473, 349473) entries\n",
      "Student model 4-35 trained with depth 36 and 2766 leaves:\n",
      "Student model score: 0.9982922072091314\n",
      "Student model 4-35 fidelity: 0.9982922072895996\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (350893, 350893) entries\n",
      "Student model 4-36 trained with depth 30 and 2758 leaves:\n",
      "Student model score: 0.9961865641306862\n",
      "Student model 4-36 fidelity: 0.9961865641114792\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (352313, 352313) entries\n",
      "Student model 4-37 trained with depth 40 and 2769 leaves:\n",
      "Student model score: 0.9967665265807562\n",
      "Student model 4-37 fidelity: 0.996766526491758\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (353733, 353733) entries\n",
      "Student model 4-38 trained with depth 30 and 2802 leaves:\n",
      "Student model score: 0.9971996697397656\n",
      "Student model 4-38 fidelity: 0.9971996697409394\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (355153, 355153) entries\n",
      "Student model 4-39 trained with depth 33 and 2762 leaves:\n",
      "Student model score: 0.9977175204312492\n",
      "Student model 4-39 fidelity: 0.9977175204292743\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (356573, 356573) entries\n",
      "Student model 4-40 trained with depth 37 and 2770 leaves:\n",
      "Student model score: 0.9943407727933073\n",
      "Student model 4-40 fidelity: 0.9943407727544639\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (357993, 357993) entries\n",
      "Student model 4-41 trained with depth 38 and 2777 leaves:\n",
      "Student model score: 0.9943479102269459\n",
      "Student model 4-41 fidelity: 0.9943479103513333\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (359413, 359413) entries\n",
      "Student model 4-42 trained with depth 35 and 2789 leaves:\n",
      "Student model score: 0.9976450187615341\n",
      "Student model 4-42 fidelity: 0.9976450187650235\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (360833, 360833) entries\n",
      "Student model 4-43 trained with depth 33 and 2787 leaves:\n",
      "Student model score: 0.9969134733152142\n",
      "Student model 4-43 fidelity: 0.9969134733124799\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (362253, 362253) entries\n",
      "Student model 4-44 trained with depth 33 and 2755 leaves:\n",
      "Student model score: 0.9939312252439377\n",
      "Student model 4-44 fidelity: 0.9939312252307412\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (363673, 363673) entries\n",
      "Student model 4-45 trained with depth 31 and 2716 leaves:\n",
      "Student model score: 0.9984104064158197\n",
      "Student model 4-45 fidelity: 0.9984104063549718\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (365093, 365093) entries\n",
      "Student model 4-46 trained with depth 30 and 2790 leaves:\n",
      "Student model score: 0.9975578073141825\n",
      "Student model 4-46 fidelity: 0.9975578073083152\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (366513, 366513) entries\n",
      "Student model 4-47 trained with depth 35 and 2750 leaves:\n",
      "Student model score: 0.9984515576242047\n",
      "Student model 4-47 fidelity: 0.9984515575022642\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (367933, 367933) entries\n",
      "Student model 4-48 trained with depth 42 and 2753 leaves:\n",
      "Student model score: 0.9916637861161619\n",
      "Student model 4-48 fidelity: 0.9916637862301414\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (369353, 369353) entries\n",
      "Student model 4-49 trained with depth 28 and 2771 leaves:\n",
      "Student model score: 0.9910714670581559\n",
      "Student model 4-49 fidelity: 0.9910714671221369\n",
      "########## Outer-loop Iteration 5/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (370773, 370773) entries\n",
      "Student model 5-0 trained with depth 32 and 2769 leaves:\n",
      "Student model score: 0.9976923472080782\n",
      "Student model 5-0 fidelity: 0.9976923471396562\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (372193, 372193) entries\n",
      "Student model 5-1 trained with depth 38 and 2782 leaves:\n",
      "Student model score: 0.9961847582708548\n",
      "Student model 5-1 fidelity: 0.9961847582173672\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (373613, 373613) entries\n",
      "Student model 5-2 trained with depth 32 and 2796 leaves:\n",
      "Student model score: 0.9944587611214947\n",
      "Student model 5-2 fidelity: 0.9944587611252227\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (375033, 375033) entries\n",
      "Student model 5-3 trained with depth 30 and 2797 leaves:\n",
      "Student model score: 0.9979023517903367\n",
      "Student model 5-3 fidelity: 0.9979023517834754\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (376453, 376453) entries\n",
      "Student model 5-4 trained with depth 32 and 2781 leaves:\n",
      "Student model score: 0.9976873114981736\n",
      "Student model 5-4 fidelity: 0.9976873114846517\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (377873, 377873) entries\n",
      "Student model 5-5 trained with depth 35 and 2760 leaves:\n",
      "Student model score: 0.9947245172390169\n",
      "Student model 5-5 fidelity: 0.9947245171600182\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (379293, 379293) entries\n",
      "Student model 5-6 trained with depth 31 and 2797 leaves:\n",
      "Student model score: 0.9973264954380326\n",
      "Student model 5-6 fidelity: 0.9973264952718572\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (380713, 380713) entries\n",
      "Student model 5-7 trained with depth 30 and 2794 leaves:\n",
      "Student model score: 0.9976621330810265\n",
      "Student model 5-7 fidelity: 0.9976621327764933\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (382133, 382133) entries\n",
      "Student model 5-8 trained with depth 31 and 2761 leaves:\n",
      "Student model score: 0.9973076872702191\n",
      "Student model 5-8 fidelity: 0.997307687270219\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (383553, 383553) entries\n",
      "Student model 5-9 trained with depth 38 and 2726 leaves:\n",
      "Student model score: 0.9972854791495631\n",
      "Student model 5-9 fidelity: 0.9972854793345566\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (384973, 384973) entries\n",
      "Student model 5-10 trained with depth 31 and 2807 leaves:\n",
      "Student model score: 0.9974475260463881\n",
      "Student model 5-10 fidelity: 0.997447526038232\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (386393, 386393) entries\n",
      "Student model 5-11 trained with depth 34 and 2804 leaves:\n",
      "Student model score: 0.9978917677563891\n",
      "Student model 5-11 fidelity: 0.9978917677590936\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (387813, 387813) entries\n",
      "Student model 5-12 trained with depth 32 and 2790 leaves:\n",
      "Student model score: 0.9979274385018974\n",
      "Student model 5-12 fidelity: 0.9979274384694768\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (389233, 389233) entries\n",
      "Student model 5-13 trained with depth 34 and 2757 leaves:\n",
      "Student model score: 0.9981899895519587\n",
      "Student model 5-13 fidelity: 0.9981899895785312\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (390653, 390653) entries\n",
      "Student model 5-14 trained with depth 34 and 2789 leaves:\n",
      "Student model score: 0.9943987388817424\n",
      "Student model 5-14 fidelity: 0.9943987389413476\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (392073, 392073) entries\n",
      "Student model 5-15 trained with depth 34 and 2783 leaves:\n",
      "Student model score: 0.9946325224790462\n",
      "Student model 5-15 fidelity: 0.9946325223364619\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (393493, 393493) entries\n",
      "Student model 5-16 trained with depth 36 and 2769 leaves:\n",
      "Student model score: 0.998328048140497\n",
      "Student model 5-16 fidelity: 0.9983280481428114\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (394913, 394913) entries\n",
      "Student model 5-17 trained with depth 35 and 2728 leaves:\n",
      "Student model score: 0.9883288158263105\n",
      "Student model 5-17 fidelity: 0.9883288158265308\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (396333, 396333) entries\n",
      "Student model 5-18 trained with depth 33 and 2800 leaves:\n",
      "Student model score: 0.9851682964425547\n",
      "Student model 5-18 fidelity: 0.9851682964900493\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (397753, 397753) entries\n",
      "Student model 5-19 trained with depth 31 and 2801 leaves:\n",
      "Student model score: 0.9985803759898219\n",
      "Student model 5-19 fidelity: 0.998580376042324\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (399173, 399173) entries\n",
      "Student model 5-20 trained with depth 32 and 2754 leaves:\n",
      "Student model score: 0.9919243181688847\n",
      "Student model 5-20 fidelity: 0.991924318134173\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (400593, 400593) entries\n",
      "Student model 5-21 trained with depth 36 and 2775 leaves:\n",
      "Student model score: 0.9975505572119749\n",
      "Student model 5-21 fidelity: 0.9975505572187547\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (402013, 402013) entries\n",
      "Student model 5-22 trained with depth 36 and 2769 leaves:\n",
      "Student model score: 0.9954347519249367\n",
      "Student model 5-22 fidelity: 0.995434752421416\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (403433, 403433) entries\n",
      "Student model 5-23 trained with depth 29 and 2783 leaves:\n",
      "Student model score: 0.998594132984179\n",
      "Student model 5-23 fidelity: 0.9985941329462449\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (404853, 404853) entries\n",
      "Student model 5-24 trained with depth 32 and 2773 leaves:\n",
      "Student model score: 0.9978209762318381\n",
      "Student model 5-24 fidelity: 0.997820976333184\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (406273, 406273) entries\n",
      "Student model 5-25 trained with depth 33 and 2777 leaves:\n",
      "Student model score: 0.9946980177126442\n",
      "Student model 5-25 fidelity: 0.9946980177024535\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (407693, 407693) entries\n",
      "Student model 5-26 trained with depth 29 and 2772 leaves:\n",
      "Student model score: 0.9976220224067137\n",
      "Student model 5-26 fidelity: 0.9976220225375367\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (409113, 409113) entries\n",
      "Student model 5-27 trained with depth 37 and 2764 leaves:\n",
      "Student model score: 0.9976987985139885\n",
      "Student model 5-27 fidelity: 0.9976987985147563\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (410533, 410533) entries\n",
      "Student model 5-28 trained with depth 33 and 2804 leaves:\n",
      "Student model score: 0.9951455314506661\n",
      "Student model 5-28 fidelity: 0.9951455314470864\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (411953, 411953) entries\n",
      "Student model 5-29 trained with depth 32 and 2754 leaves:\n",
      "Student model score: 0.9974276547330106\n",
      "Student model 5-29 fidelity: 0.9974276548269004\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (413373, 413373) entries\n",
      "Student model 5-30 trained with depth 31 and 2778 leaves:\n",
      "Student model score: 0.9946448933907246\n",
      "Student model 5-30 fidelity: 0.9946448934370903\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (414793, 414793) entries\n",
      "Student model 5-31 trained with depth 30 and 2748 leaves:\n",
      "Student model score: 0.9977774541378985\n",
      "Student model 5-31 fidelity: 0.9977774541035729\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (416213, 416213) entries\n",
      "Student model 5-32 trained with depth 31 and 2794 leaves:\n",
      "Student model score: 0.9988374809919991\n",
      "Student model 5-32 fidelity: 0.998837480998937\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (417633, 417633) entries\n",
      "Student model 5-33 trained with depth 31 and 2756 leaves:\n",
      "Student model score: 0.9973379418909685\n",
      "Student model 5-33 fidelity: 0.9973379419347818\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (419053, 419053) entries\n",
      "Student model 5-34 trained with depth 32 and 2801 leaves:\n",
      "Student model score: 0.9963329432830075\n",
      "Student model 5-34 fidelity: 0.9963329431761389\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (420473, 420473) entries\n",
      "Student model 5-35 trained with depth 29 and 2787 leaves:\n",
      "Student model score: 0.9954356210943308\n",
      "Student model 5-35 fidelity: 0.9954356211515584\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (421893, 421893) entries\n",
      "Student model 5-36 trained with depth 32 and 2748 leaves:\n",
      "Student model score: 0.9976643712383193\n",
      "Student model 5-36 fidelity: 0.9976643712154104\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (423313, 423313) entries\n",
      "Student model 5-37 trained with depth 36 and 2782 leaves:\n",
      "Student model score: 0.9938201977017802\n",
      "Student model 5-37 fidelity: 0.9938201977022239\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (424733, 424733) entries\n",
      "Student model 5-38 trained with depth 33 and 2758 leaves:\n",
      "Student model score: 0.991543425929289\n",
      "Student model 5-38 fidelity: 0.9915434259008625\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (426153, 426153) entries\n",
      "Student model 5-39 trained with depth 32 and 2772 leaves:\n",
      "Student model score: 0.9916121957936193\n",
      "Student model 5-39 fidelity: 0.9916121957688853\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (427573, 427573) entries\n",
      "Student model 5-40 trained with depth 33 and 2777 leaves:\n",
      "Student model score: 0.9873965408236353\n",
      "Student model 5-40 fidelity: 0.9873965408096589\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (428993, 428993) entries\n",
      "Student model 5-41 trained with depth 32 and 2757 leaves:\n",
      "Student model score: 0.9983286082604573\n",
      "Student model 5-41 fidelity: 0.9983286082697458\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (430413, 430413) entries\n",
      "Student model 5-42 trained with depth 34 and 2742 leaves:\n",
      "Student model score: 0.9979597754995759\n",
      "Student model 5-42 fidelity: 0.9979597755231127\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (431833, 431833) entries\n",
      "Student model 5-43 trained with depth 43 and 2756 leaves:\n",
      "Student model score: 0.9926026344260095\n",
      "Student model 5-43 fidelity: 0.992602634432044\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (433253, 433253) entries\n",
      "Student model 5-44 trained with depth 34 and 2762 leaves:\n",
      "Student model score: 0.9892626396783348\n",
      "Student model 5-44 fidelity: 0.9892626396962476\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (434673, 434673) entries\n",
      "Student model 5-45 trained with depth 32 and 2803 leaves:\n",
      "Student model score: 0.9951005207511824\n",
      "Student model 5-45 fidelity: 0.9951005206166398\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (436093, 436093) entries\n",
      "Student model 5-46 trained with depth 30 and 2776 leaves:\n",
      "Student model score: 0.9908167023032738\n",
      "Student model 5-46 fidelity: 0.9908167022748501\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (437513, 437513) entries\n",
      "Student model 5-47 trained with depth 30 and 2756 leaves:\n",
      "Student model score: 0.9968781384186292\n",
      "Student model 5-47 fidelity: 0.9968781379682993\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (438933, 438933) entries\n",
      "Student model 5-48 trained with depth 34 and 2774 leaves:\n",
      "Student model score: 0.9972934007512438\n",
      "Student model 5-48 fidelity: 0.9972934007572701\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (440353, 440353) entries\n",
      "Student model 5-49 trained with depth 32 and 2766 leaves:\n",
      "Student model score: 0.9949254446247007\n",
      "Student model 5-49 fidelity: 0.9949254445410236\n",
      "########## Outer-loop Iteration 6/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (441773, 441773) entries\n",
      "Student model 6-0 trained with depth 33 and 2780 leaves:\n",
      "Student model score: 0.9972997549379768\n",
      "Student model 6-0 fidelity: 0.997299754993685\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (443193, 443193) entries\n",
      "Student model 6-1 trained with depth 37 and 2793 leaves:\n",
      "Student model score: 0.99337670586343\n",
      "Student model 6-1 fidelity: 0.9933767058209657\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (444613, 444613) entries\n",
      "Student model 6-2 trained with depth 32 and 2762 leaves:\n",
      "Student model score: 0.9903936607954507\n",
      "Student model 6-2 fidelity: 0.9903936607023714\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (446033, 446033) entries\n",
      "Student model 6-3 trained with depth 34 and 2757 leaves:\n",
      "Student model score: 0.9971367233279324\n",
      "Student model 6-3 fidelity: 0.9971367233275952\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (447453, 447453) entries\n",
      "Student model 6-4 trained with depth 32 and 2799 leaves:\n",
      "Student model score: 0.9971337358157522\n",
      "Student model 6-4 fidelity: 0.9971337358032513\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (448873, 448873) entries\n",
      "Student model 6-5 trained with depth 34 and 2775 leaves:\n",
      "Student model score: 0.9955430580703626\n",
      "Student model 6-5 fidelity: 0.9955430580878085\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (450293, 450293) entries\n",
      "Student model 6-6 trained with depth 30 and 2767 leaves:\n",
      "Student model score: 0.9978028701835965\n",
      "Student model 6-6 fidelity: 0.997802870181887\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (451713, 451713) entries\n",
      "Student model 6-7 trained with depth 32 and 2779 leaves:\n",
      "Student model score: 0.9954157452204796\n",
      "Student model 6-7 fidelity: 0.9954157451759448\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (453133, 453133) entries\n",
      "Student model 6-8 trained with depth 32 and 2800 leaves:\n",
      "Student model score: 0.9979769317883361\n",
      "Student model 6-8 fidelity: 0.9979769318692717\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (454553, 454553) entries\n",
      "Student model 6-9 trained with depth 33 and 2774 leaves:\n",
      "Student model score: 0.9978830795117873\n",
      "Student model 6-9 fidelity: 0.9978830794497726\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (455973, 455973) entries\n",
      "Student model 6-10 trained with depth 29 and 2758 leaves:\n",
      "Student model score: 0.9970798794922691\n",
      "Student model 6-10 fidelity: 0.9970798794881232\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (457393, 457393) entries\n",
      "Student model 6-11 trained with depth 35 and 2775 leaves:\n",
      "Student model score: 0.988350992634179\n",
      "Student model 6-11 fidelity: 0.9883509926388717\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (458813, 458813) entries\n",
      "Student model 6-12 trained with depth 31 and 2764 leaves:\n",
      "Student model score: 0.9981982055174555\n",
      "Student model 6-12 fidelity: 0.9981982054891901\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (460233, 460233) entries\n",
      "Student model 6-13 trained with depth 34 and 2788 leaves:\n",
      "Student model score: 0.994472750820229\n",
      "Student model 6-13 fidelity: 0.9944727508183363\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (461653, 461653) entries\n",
      "Student model 6-14 trained with depth 31 and 2774 leaves:\n",
      "Student model score: 0.9945197004172083\n",
      "Student model 6-14 fidelity: 0.9945197001730692\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (463073, 463073) entries\n",
      "Student model 6-15 trained with depth 39 and 2766 leaves:\n",
      "Student model score: 0.9973565475191302\n",
      "Student model 6-15 fidelity: 0.9973565476328332\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (464493, 464493) entries\n",
      "Student model 6-16 trained with depth 32 and 2773 leaves:\n",
      "Student model score: 0.9939273610106559\n",
      "Student model 6-16 fidelity: 0.9939273609114635\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (465913, 465913) entries\n",
      "Student model 6-17 trained with depth 29 and 2761 leaves:\n",
      "Student model score: 0.9971763571072633\n",
      "Student model 6-17 fidelity: 0.9971763571071315\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (467333, 467333) entries\n",
      "Student model 6-18 trained with depth 40 and 2804 leaves:\n",
      "Student model score: 0.9975956121787529\n",
      "Student model 6-18 fidelity: 0.9975956122213719\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (468753, 468753) entries\n",
      "Student model 6-19 trained with depth 33 and 2761 leaves:\n",
      "Student model score: 0.998605787973125\n",
      "Student model 6-19 fidelity: 0.9986057880316755\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (470173, 470173) entries\n",
      "Student model 6-20 trained with depth 35 and 2780 leaves:\n",
      "Student model score: 0.9942538214627155\n",
      "Student model 6-20 fidelity: 0.9942538214694785\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (471593, 471593) entries\n",
      "Student model 6-21 trained with depth 32 and 2775 leaves:\n",
      "Student model score: 0.9916754499175067\n",
      "Student model 6-21 fidelity: 0.9916754499132202\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (473013, 473013) entries\n",
      "Student model 6-22 trained with depth 31 and 2780 leaves:\n",
      "Student model score: 0.9911031082074863\n",
      "Student model 6-22 fidelity: 0.9911031081975176\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (474433, 474433) entries\n",
      "Student model 6-23 trained with depth 33 and 2804 leaves:\n",
      "Student model score: 0.9925422473126935\n",
      "Student model 6-23 fidelity: 0.9925422473825503\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (475853, 475853) entries\n",
      "Student model 6-24 trained with depth 33 and 2760 leaves:\n",
      "Student model score: 0.9944428613640515\n",
      "Student model 6-24 fidelity: 0.9944428614270749\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (477273, 477273) entries\n",
      "Student model 6-25 trained with depth 31 and 2772 leaves:\n",
      "Student model score: 0.9950312719386791\n",
      "Student model 6-25 fidelity: 0.995031271917778\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (478693, 478693) entries\n",
      "Student model 6-26 trained with depth 33 and 2779 leaves:\n",
      "Student model score: 0.9971918305616301\n",
      "Student model 6-26 fidelity: 0.9971918307129839\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (480113, 480113) entries\n",
      "Student model 6-27 trained with depth 32 and 2762 leaves:\n",
      "Student model score: 0.9983989383284182\n",
      "Student model 6-27 fidelity: 0.9983989383277407\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (481533, 481533) entries\n",
      "Student model 6-28 trained with depth 29 and 2775 leaves:\n",
      "Student model score: 0.9978655979820188\n",
      "Student model 6-28 fidelity: 0.997865597973974\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (482953, 482953) entries\n",
      "Student model 6-29 trained with depth 32 and 2778 leaves:\n",
      "Student model score: 0.9940301981127624\n",
      "Student model 6-29 fidelity: 0.9940301981096056\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (484373, 484373) entries\n",
      "Student model 6-30 trained with depth 32 and 2798 leaves:\n",
      "Student model score: 0.9975046506743286\n",
      "Student model 6-30 fidelity: 0.9975046506736851\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (485793, 485793) entries\n",
      "Student model 6-31 trained with depth 30 and 2793 leaves:\n",
      "Student model score: 0.9893967231804386\n",
      "Student model 6-31 fidelity: 0.9893967232654908\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (487213, 487213) entries\n",
      "Student model 6-32 trained with depth 35 and 2764 leaves:\n",
      "Student model score: 0.9948294427462716\n",
      "Student model 6-32 fidelity: 0.9948294427379012\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (488633, 488633) entries\n",
      "Student model 6-33 trained with depth 32 and 2755 leaves:\n",
      "Student model score: 0.997272036078652\n",
      "Student model 6-33 fidelity: 0.997272036013578\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (490053, 490053) entries\n",
      "Student model 6-34 trained with depth 34 and 2756 leaves:\n",
      "Student model score: 0.9870557734252563\n",
      "Student model 6-34 fidelity: 0.9870557734305503\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (491473, 491473) entries\n",
      "Student model 6-35 trained with depth 33 and 2738 leaves:\n",
      "Student model score: 0.9927912346576008\n",
      "Student model 6-35 fidelity: 0.9927912346441631\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (492893, 492893) entries\n",
      "Student model 6-36 trained with depth 35 and 2771 leaves:\n",
      "Student model score: 0.9955476337197507\n",
      "Student model 6-36 fidelity: 0.9955476337219814\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (494313, 494313) entries\n",
      "Student model 6-37 trained with depth 29 and 2773 leaves:\n",
      "Student model score: 0.9986389679628673\n",
      "Student model 6-37 fidelity: 0.99863896796486\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (495733, 495733) entries\n",
      "Student model 6-38 trained with depth 35 and 2769 leaves:\n",
      "Student model score: 0.9957980866188392\n",
      "Student model 6-38 fidelity: 0.995798086598904\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (497153, 497153) entries\n",
      "Student model 6-39 trained with depth 31 and 2776 leaves:\n",
      "Student model score: 0.9884684798803974\n",
      "Student model 6-39 fidelity: 0.9884684797761952\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (498573, 498573) entries\n",
      "Student model 6-40 trained with depth 30 and 2753 leaves:\n",
      "Student model score: 0.9893154049110588\n",
      "Student model 6-40 fidelity: 0.9893154048622652\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (499993, 499993) entries\n",
      "Student model 6-41 trained with depth 30 and 2787 leaves:\n",
      "Student model score: 0.9979690972256382\n",
      "Student model 6-41 fidelity: 0.9979690973301135\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (501413, 501413) entries\n",
      "Student model 6-42 trained with depth 34 and 2773 leaves:\n",
      "Student model score: 0.9968372602756882\n",
      "Student model 6-42 fidelity: 0.9968372602726936\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (502833, 502833) entries\n",
      "Student model 6-43 trained with depth 40 and 2782 leaves:\n",
      "Student model score: 0.9982565660798945\n",
      "Student model 6-43 fidelity: 0.998256566050633\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (504253, 504253) entries\n",
      "Student model 6-44 trained with depth 32 and 2743 leaves:\n",
      "Student model score: 0.9866182000022585\n",
      "Student model 6-44 fidelity: 0.9866182000019726\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (505673, 505673) entries\n",
      "Student model 6-45 trained with depth 35 and 2759 leaves:\n",
      "Student model score: 0.9935298249622436\n",
      "Student model 6-45 fidelity: 0.9935298249683092\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (507093, 507093) entries\n",
      "Student model 6-46 trained with depth 33 and 2758 leaves:\n",
      "Student model score: 0.9982918958650929\n",
      "Student model 6-46 fidelity: 0.998291895749848\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (508513, 508513) entries\n",
      "Student model 6-47 trained with depth 34 and 2732 leaves:\n",
      "Student model score: 0.997247384035205\n",
      "Student model 6-47 fidelity: 0.9972473837978049\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (509933, 509933) entries\n",
      "Student model 6-48 trained with depth 32 and 2740 leaves:\n",
      "Student model score: 0.9943854665356918\n",
      "Student model 6-48 fidelity: 0.9943854669727403\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (511353, 511353) entries\n",
      "Student model 6-49 trained with depth 32 and 2804 leaves:\n",
      "Student model score: 0.9921105832381693\n",
      "Student model 6-49 fidelity: 0.9921105833226797\n",
      "########## Outer-loop Iteration 7/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (512773, 512773) entries\n",
      "Student model 7-0 trained with depth 37 and 2771 leaves:\n",
      "Student model score: 0.9951967959064875\n",
      "Student model 7-0 fidelity: 0.9951967956137222\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (514193, 514193) entries\n",
      "Student model 7-1 trained with depth 32 and 2803 leaves:\n",
      "Student model score: 0.984665244883749\n",
      "Student model 7-1 fidelity: 0.9846652448772024\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (515613, 515613) entries\n",
      "Student model 7-2 trained with depth 35 and 2769 leaves:\n",
      "Student model score: 0.994691756585605\n",
      "Student model 7-2 fidelity: 0.9946917565298684\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (517033, 517033) entries\n",
      "Student model 7-3 trained with depth 32 and 2780 leaves:\n",
      "Student model score: 0.9990013730486494\n",
      "Student model 7-3 fidelity: 0.9990013730358499\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (518453, 518453) entries\n",
      "Student model 7-4 trained with depth 37 and 2791 leaves:\n",
      "Student model score: 0.9957529474404718\n",
      "Student model 7-4 fidelity: 0.9957529474974485\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (519873, 519873) entries\n",
      "Student model 7-5 trained with depth 28 and 2788 leaves:\n",
      "Student model score: 0.9952023957884959\n",
      "Student model 7-5 fidelity: 0.9952023957163287\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (521293, 521293) entries\n",
      "Student model 7-6 trained with depth 29 and 2750 leaves:\n",
      "Student model score: 0.9968111624037663\n",
      "Student model 7-6 fidelity: 0.9968111623869735\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (522713, 522713) entries\n",
      "Student model 7-7 trained with depth 34 and 2771 leaves:\n",
      "Student model score: 0.9961801188054575\n",
      "Student model 7-7 fidelity: 0.9961801186795533\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (524133, 524133) entries\n",
      "Student model 7-8 trained with depth 34 and 2797 leaves:\n",
      "Student model score: 0.9956977213883066\n",
      "Student model 7-8 fidelity: 0.9956977213940331\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (525553, 525553) entries\n",
      "Student model 7-9 trained with depth 32 and 2763 leaves:\n",
      "Student model score: 0.9977291231647292\n",
      "Student model 7-9 fidelity: 0.9977291231614711\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (526973, 526973) entries\n",
      "Student model 7-10 trained with depth 32 and 2726 leaves:\n",
      "Student model score: 0.9952474069274749\n",
      "Student model 7-10 fidelity: 0.9952474070433641\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (528393, 528393) entries\n",
      "Student model 7-11 trained with depth 31 and 2750 leaves:\n",
      "Student model score: 0.9949510470987823\n",
      "Student model 7-11 fidelity: 0.9949510470975328\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (529813, 529813) entries\n",
      "Student model 7-12 trained with depth 31 and 2770 leaves:\n",
      "Student model score: 0.9952606202162155\n",
      "Student model 7-12 fidelity: 0.9952606199761562\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (531233, 531233) entries\n",
      "Student model 7-13 trained with depth 35 and 2772 leaves:\n",
      "Student model score: 0.9942361833373999\n",
      "Student model 7-13 fidelity: 0.9942361834176626\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (532653, 532653) entries\n",
      "Student model 7-14 trained with depth 33 and 2776 leaves:\n",
      "Student model score: 0.9987638180135916\n",
      "Student model 7-14 fidelity: 0.9987638180150056\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (534073, 534073) entries\n",
      "Student model 7-15 trained with depth 30 and 2798 leaves:\n",
      "Student model score: 0.9928528559123475\n",
      "Student model 7-15 fidelity: 0.992852855935377\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (535493, 535493) entries\n",
      "Student model 7-16 trained with depth 32 and 2743 leaves:\n",
      "Student model score: 0.9906786162752528\n",
      "Student model 7-16 fidelity: 0.9906786162665873\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (536913, 536913) entries\n",
      "Student model 7-17 trained with depth 32 and 2745 leaves:\n",
      "Student model score: 0.9977736738632776\n",
      "Student model 7-17 fidelity: 0.9977736739508507\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (538333, 538333) entries\n",
      "Student model 7-18 trained with depth 35 and 2800 leaves:\n",
      "Student model score: 0.9977611152855587\n",
      "Student model 7-18 fidelity: 0.9977611152860613\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (539753, 539753) entries\n",
      "Student model 7-19 trained with depth 38 and 2796 leaves:\n",
      "Student model score: 0.9873301731194125\n",
      "Student model 7-19 fidelity: 0.987330173127079\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (541173, 541173) entries\n",
      "Student model 7-20 trained with depth 31 and 2773 leaves:\n",
      "Student model score: 0.9945576099632026\n",
      "Student model 7-20 fidelity: 0.9945576099898105\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (542593, 542593) entries\n",
      "Student model 7-21 trained with depth 35 and 2754 leaves:\n",
      "Student model score: 0.9935066103886637\n",
      "Student model 7-21 fidelity: 0.9935066104323568\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (544013, 544013) entries\n",
      "Student model 7-22 trained with depth 32 and 2761 leaves:\n",
      "Student model score: 0.9984527309974575\n",
      "Student model 7-22 fidelity: 0.998452731049012\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (545433, 545433) entries\n",
      "Student model 7-23 trained with depth 36 and 2772 leaves:\n",
      "Student model score: 0.9950858227755253\n",
      "Student model 7-23 fidelity: 0.9950858227754357\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (546853, 546853) entries\n",
      "Student model 7-24 trained with depth 36 and 2749 leaves:\n",
      "Student model score: 0.99697846124852\n",
      "Student model 7-24 fidelity: 0.9969784617054546\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (548273, 548273) entries\n",
      "Student model 7-25 trained with depth 32 and 2773 leaves:\n",
      "Student model score: 0.99775960858039\n",
      "Student model 7-25 fidelity: 0.9977596085696145\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (549693, 549693) entries\n",
      "Student model 7-26 trained with depth 34 and 2760 leaves:\n",
      "Student model score: 0.9980129078035429\n",
      "Student model 7-26 fidelity: 0.9980129078040889\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (551113, 551113) entries\n",
      "Student model 7-27 trained with depth 31 and 2781 leaves:\n",
      "Student model score: 0.9960512540844082\n",
      "Student model 7-27 fidelity: 0.9960512540788338\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (552533, 552533) entries\n",
      "Student model 7-28 trained with depth 32 and 2775 leaves:\n",
      "Student model score: 0.997012562752564\n",
      "Student model 7-28 fidelity: 0.9970125627511792\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (553953, 553953) entries\n",
      "Student model 7-29 trained with depth 31 and 2764 leaves:\n",
      "Student model score: 0.9973209042326103\n",
      "Student model 7-29 fidelity: 0.9973209042289244\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (555373, 555373) entries\n",
      "Student model 7-30 trained with depth 29 and 2742 leaves:\n",
      "Student model score: 0.9966678956769695\n",
      "Student model 7-30 fidelity: 0.9966678956477564\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (556793, 556793) entries\n",
      "Student model 7-31 trained with depth 37 and 2731 leaves:\n",
      "Student model score: 0.9922191723527877\n",
      "Student model 7-31 fidelity: 0.9922191724282101\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (558213, 558213) entries\n",
      "Student model 7-32 trained with depth 31 and 2824 leaves:\n",
      "Student model score: 0.9980594528617881\n",
      "Student model 7-32 fidelity: 0.9980594528509874\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (559633, 559633) entries\n",
      "Student model 7-33 trained with depth 34 and 2783 leaves:\n",
      "Student model score: 0.9939741797857021\n",
      "Student model 7-33 fidelity: 0.9939741797914423\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (561053, 561053) entries\n",
      "Student model 7-34 trained with depth 31 and 2746 leaves:\n",
      "Student model score: 0.9949019708828832\n",
      "Student model 7-34 fidelity: 0.994901970911633\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (562473, 562473) entries\n",
      "Student model 7-35 trained with depth 31 and 2773 leaves:\n",
      "Student model score: 0.9973567136228485\n",
      "Student model 7-35 fidelity: 0.9973567136133322\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (563893, 563893) entries\n",
      "Student model 7-36 trained with depth 36 and 2775 leaves:\n",
      "Student model score: 0.9981256301041188\n",
      "Student model 7-36 fidelity: 0.998125630104013\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (565313, 565313) entries\n",
      "Student model 7-37 trained with depth 34 and 2802 leaves:\n",
      "Student model score: 0.9914223539860414\n",
      "Student model 7-37 fidelity: 0.9914223539903679\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (566733, 566733) entries\n",
      "Student model 7-38 trained with depth 30 and 2735 leaves:\n",
      "Student model score: 0.9975130091060109\n",
      "Student model 7-38 fidelity: 0.9975130091848642\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (568153, 568153) entries\n",
      "Student model 7-39 trained with depth 32 and 2743 leaves:\n",
      "Student model score: 0.9922839368607316\n",
      "Student model 7-39 fidelity: 0.9922839369034753\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (569573, 569573) entries\n",
      "Student model 7-40 trained with depth 34 and 2758 leaves:\n",
      "Student model score: 0.9975361773857216\n",
      "Student model 7-40 fidelity: 0.9975361773896111\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (570993, 570993) entries\n",
      "Student model 7-41 trained with depth 32 and 2814 leaves:\n",
      "Student model score: 0.9946583567482636\n",
      "Student model 7-41 fidelity: 0.9946583566799946\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (572413, 572413) entries\n",
      "Student model 7-42 trained with depth 36 and 2755 leaves:\n",
      "Student model score: 0.9977343671883095\n",
      "Student model 7-42 fidelity: 0.9977343671879513\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (573833, 573833) entries\n",
      "Student model 7-43 trained with depth 35 and 2789 leaves:\n",
      "Student model score: 0.9947738809894743\n",
      "Student model 7-43 fidelity: 0.9947738807263252\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (575253, 575253) entries\n",
      "Student model 7-44 trained with depth 40 and 2747 leaves:\n",
      "Student model score: 0.9923649901207323\n",
      "Student model 7-44 fidelity: 0.9923649904063049\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (576673, 576673) entries\n",
      "Student model 7-45 trained with depth 35 and 2753 leaves:\n",
      "Student model score: 0.9970930102235305\n",
      "Student model 7-45 fidelity: 0.9970930102069834\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (578093, 578093) entries\n",
      "Student model 7-46 trained with depth 28 and 2766 leaves:\n",
      "Student model score: 0.9963723028426479\n",
      "Student model 7-46 fidelity: 0.9963723028428925\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (579513, 579513) entries\n",
      "Student model 7-47 trained with depth 33 and 2747 leaves:\n",
      "Student model score: 0.9946244154299158\n",
      "Student model 7-47 fidelity: 0.994624415415394\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (580933, 580933) entries\n",
      "Student model 7-48 trained with depth 34 and 2757 leaves:\n",
      "Student model score: 0.9966707474392632\n",
      "Student model 7-48 fidelity: 0.9966707474407468\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (582353, 582353) entries\n",
      "Student model 7-49 trained with depth 33 and 2769 leaves:\n",
      "Student model score: 0.9969720542089052\n",
      "Student model 7-49 fidelity: 0.9969720541989483\n",
      "########## Outer-loop Iteration 8/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (583773, 583773) entries\n",
      "Student model 8-0 trained with depth 32 and 2821 leaves:\n",
      "Student model score: 0.998090649171919\n",
      "Student model 8-0 fidelity: 0.9980906491715675\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (585193, 585193) entries\n",
      "Student model 8-1 trained with depth 31 and 2732 leaves:\n",
      "Student model score: 0.989367501268549\n",
      "Student model 8-1 fidelity: 0.989367501252822\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (586613, 586613) entries\n",
      "Student model 8-2 trained with depth 33 and 2790 leaves:\n",
      "Student model score: 0.9941689809638515\n",
      "Student model 8-2 fidelity: 0.99416898090858\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (588033, 588033) entries\n",
      "Student model 8-3 trained with depth 28 and 2768 leaves:\n",
      "Student model score: 0.9965929964822092\n",
      "Student model 8-3 fidelity: 0.9965929964485287\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (589453, 589453) entries\n",
      "Student model 8-4 trained with depth 32 and 2769 leaves:\n",
      "Student model score: 0.9980448823404706\n",
      "Student model 8-4 fidelity: 0.9980448823504933\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (590873, 590873) entries\n",
      "Student model 8-5 trained with depth 31 and 2771 leaves:\n",
      "Student model score: 0.9977773512888692\n",
      "Student model 8-5 fidelity: 0.9977773512602007\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (592293, 592293) entries\n",
      "Student model 8-6 trained with depth 34 and 2736 leaves:\n",
      "Student model score: 0.9980457377970833\n",
      "Student model 8-6 fidelity: 0.9980457378419055\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (593713, 593713) entries\n",
      "Student model 8-7 trained with depth 32 and 2789 leaves:\n",
      "Student model score: 0.9965700116416558\n",
      "Student model 8-7 fidelity: 0.9965700116388543\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (595133, 595133) entries\n",
      "Student model 8-8 trained with depth 33 and 2754 leaves:\n",
      "Student model score: 0.9959157352558398\n",
      "Student model 8-8 fidelity: 0.9959157353332755\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (596553, 596553) entries\n",
      "Student model 8-9 trained with depth 31 and 2751 leaves:\n",
      "Student model score: 0.9971578633395983\n",
      "Student model 8-9 fidelity: 0.997157863394958\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (597973, 597973) entries\n",
      "Student model 8-10 trained with depth 34 and 2784 leaves:\n",
      "Student model score: 0.9891215558376191\n",
      "Student model 8-10 fidelity: 0.9891215558378001\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (599393, 599393) entries\n",
      "Student model 8-11 trained with depth 33 and 2768 leaves:\n",
      "Student model score: 0.9969515182574253\n",
      "Student model 8-11 fidelity: 0.9969515182582076\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (600813, 600813) entries\n",
      "Student model 8-12 trained with depth 34 and 2768 leaves:\n",
      "Student model score: 0.9984190329398851\n",
      "Student model 8-12 fidelity: 0.9984190329386959\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (602233, 602233) entries\n",
      "Student model 8-13 trained with depth 31 and 2779 leaves:\n",
      "Student model score: 0.9977942477695633\n",
      "Student model 8-13 fidelity: 0.9977942478731676\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (603653, 603653) entries\n",
      "Student model 8-14 trained with depth 30 and 2794 leaves:\n",
      "Student model score: 0.9956441942158742\n",
      "Student model 8-14 fidelity: 0.9956441942161524\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (605073, 605073) entries\n",
      "Student model 8-15 trained with depth 31 and 2750 leaves:\n",
      "Student model score: 0.9980116671358098\n",
      "Student model 8-15 fidelity: 0.9980116671362202\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (606493, 606493) entries\n",
      "Student model 8-16 trained with depth 29 and 2757 leaves:\n",
      "Student model score: 0.9979263074945501\n",
      "Student model 8-16 fidelity: 0.9979263073914706\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (607913, 607913) entries\n",
      "Student model 8-17 trained with depth 31 and 2752 leaves:\n",
      "Student model score: 0.9947777911045206\n",
      "Student model 8-17 fidelity: 0.994777791037682\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (609333, 609333) entries\n",
      "Student model 8-18 trained with depth 30 and 2741 leaves:\n",
      "Student model score: 0.9979002765518313\n",
      "Student model 8-18 fidelity: 0.9979002765505233\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (610753, 610753) entries\n",
      "Student model 8-19 trained with depth 32 and 2756 leaves:\n",
      "Student model score: 0.9983357211494929\n",
      "Student model 8-19 fidelity: 0.9983357211557574\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (612173, 612173) entries\n",
      "Student model 8-20 trained with depth 31 and 2734 leaves:\n",
      "Student model score: 0.9978378854514788\n",
      "Student model 8-20 fidelity: 0.9978378854515806\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (613593, 613593) entries\n",
      "Student model 8-21 trained with depth 34 and 2739 leaves:\n",
      "Student model score: 0.9947465344945089\n",
      "Student model 8-21 fidelity: 0.9947465343375523\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (615013, 615013) entries\n",
      "Student model 8-22 trained with depth 29 and 2784 leaves:\n",
      "Student model score: 0.995315966840085\n",
      "Student model 8-22 fidelity: 0.9953159668534591\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (616433, 616433) entries\n",
      "Student model 8-23 trained with depth 32 and 2760 leaves:\n",
      "Student model score: 0.990247427210815\n",
      "Student model 8-23 fidelity: 0.9902474272411564\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (617853, 617853) entries\n",
      "Student model 8-24 trained with depth 33 and 2773 leaves:\n",
      "Student model score: 0.9973996305651068\n",
      "Student model 8-24 fidelity: 0.9973996306421457\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (619273, 619273) entries\n",
      "Student model 8-25 trained with depth 31 and 2784 leaves:\n",
      "Student model score: 0.9976995773803243\n",
      "Student model 8-25 fidelity: 0.9976995773736151\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (620693, 620693) entries\n",
      "Student model 8-26 trained with depth 36 and 2777 leaves:\n",
      "Student model score: 0.9938622891691147\n",
      "Student model 8-26 fidelity: 0.9938622891998743\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (622113, 622113) entries\n",
      "Student model 8-27 trained with depth 30 and 2768 leaves:\n",
      "Student model score: 0.9977956338312396\n",
      "Student model 8-27 fidelity: 0.9977956338516584\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (623533, 623533) entries\n",
      "Student model 8-28 trained with depth 33 and 2768 leaves:\n",
      "Student model score: 0.9944870405734537\n",
      "Student model 8-28 fidelity: 0.9944870405732633\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (624953, 624953) entries\n",
      "Student model 8-29 trained with depth 32 and 2746 leaves:\n",
      "Student model score: 0.9942844540788068\n",
      "Student model 8-29 fidelity: 0.9942844540963385\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (626373, 626373) entries\n",
      "Student model 8-30 trained with depth 30 and 2734 leaves:\n",
      "Student model score: 0.9983781521155427\n",
      "Student model 8-30 fidelity: 0.9983781521167594\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (627793, 627793) entries\n",
      "Student model 8-31 trained with depth 36 and 2766 leaves:\n",
      "Student model score: 0.9929990903874645\n",
      "Student model 8-31 fidelity: 0.9929990903940176\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (629213, 629213) entries\n",
      "Student model 8-32 trained with depth 33 and 2789 leaves:\n",
      "Student model score: 0.9972352127631722\n",
      "Student model 8-32 fidelity: 0.9972352127633227\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (630633, 630633) entries\n",
      "Student model 8-33 trained with depth 35 and 2751 leaves:\n",
      "Student model score: 0.9913740588728139\n",
      "Student model 8-33 fidelity: 0.991374058911281\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (632053, 632053) entries\n",
      "Student model 8-34 trained with depth 36 and 2750 leaves:\n",
      "Student model score: 0.9975169898039407\n",
      "Student model 8-34 fidelity: 0.9975169898040022\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (633473, 633473) entries\n",
      "Student model 8-35 trained with depth 32 and 2764 leaves:\n",
      "Student model score: 0.9929630711228531\n",
      "Student model 8-35 fidelity: 0.9929630708942675\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (634893, 634893) entries\n",
      "Student model 8-36 trained with depth 32 and 2751 leaves:\n",
      "Student model score: 0.9908401365058418\n",
      "Student model 8-36 fidelity: 0.9908401365105064\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (636313, 636313) entries\n",
      "Student model 8-37 trained with depth 36 and 2759 leaves:\n",
      "Student model score: 0.9975311220084933\n",
      "Student model 8-37 fidelity: 0.9975311220097366\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (637733, 637733) entries\n",
      "Student model 8-38 trained with depth 31 and 2751 leaves:\n",
      "Student model score: 0.9976140623609354\n",
      "Student model 8-38 fidelity: 0.997614062357074\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (639153, 639153) entries\n",
      "Student model 8-39 trained with depth 31 and 2773 leaves:\n",
      "Student model score: 0.9940817044456068\n",
      "Student model 8-39 fidelity: 0.9940817044389977\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (640573, 640573) entries\n",
      "Student model 8-40 trained with depth 34 and 2798 leaves:\n",
      "Student model score: 0.9959381882621713\n",
      "Student model 8-40 fidelity: 0.9959381882839827\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (641993, 641993) entries\n",
      "Student model 8-41 trained with depth 47 and 2795 leaves:\n",
      "Student model score: 0.9977474468808121\n",
      "Student model 8-41 fidelity: 0.9977474468880403\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (643413, 643413) entries\n",
      "Student model 8-42 trained with depth 33 and 2794 leaves:\n",
      "Student model score: 0.997769950936325\n",
      "Student model 8-42 fidelity: 0.9977699509359746\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (644833, 644833) entries\n",
      "Student model 8-43 trained with depth 38 and 2755 leaves:\n",
      "Student model score: 0.9938536344531316\n",
      "Student model 8-43 fidelity: 0.9938536343553108\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (646253, 646253) entries\n",
      "Student model 8-44 trained with depth 33 and 2759 leaves:\n",
      "Student model score: 0.9973474879990606\n",
      "Student model 8-44 fidelity: 0.9973474879525568\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (647673, 647673) entries\n",
      "Student model 8-45 trained with depth 31 and 2771 leaves:\n",
      "Student model score: 0.9946697682622667\n",
      "Student model 8-45 fidelity: 0.9946697682315572\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (649093, 649093) entries\n",
      "Student model 8-46 trained with depth 30 and 2742 leaves:\n",
      "Student model score: 0.9971257874063599\n",
      "Student model 8-46 fidelity: 0.9971257874014996\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (650513, 650513) entries\n",
      "Student model 8-47 trained with depth 32 and 2732 leaves:\n",
      "Student model score: 0.9932304421354834\n",
      "Student model 8-47 fidelity: 0.9932304421956881\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (651933, 651933) entries\n",
      "Student model 8-48 trained with depth 35 and 2798 leaves:\n",
      "Student model score: 0.9968846388204078\n",
      "Student model 8-48 fidelity: 0.9968846388204002\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (653353, 653353) entries\n",
      "Student model 8-49 trained with depth 34 and 2763 leaves:\n",
      "Student model score: 0.997887662460062\n",
      "Student model 8-49 fidelity: 0.9978876624603498\n",
      "########## Outer-loop Iteration 9/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (654773, 654773) entries\n",
      "Student model 9-0 trained with depth 31 and 2772 leaves:\n",
      "Student model score: 0.9857099711469585\n",
      "Student model 9-0 fidelity: 0.9857099711505024\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (656193, 656193) entries\n",
      "Student model 9-1 trained with depth 35 and 2791 leaves:\n",
      "Student model score: 0.9979010526067829\n",
      "Student model 9-1 fidelity: 0.9979010526041036\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (657613, 657613) entries\n",
      "Student model 9-2 trained with depth 32 and 2774 leaves:\n",
      "Student model score: 0.993411725132814\n",
      "Student model 9-2 fidelity: 0.9934117251311665\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (659033, 659033) entries\n",
      "Student model 9-3 trained with depth 32 and 2752 leaves:\n",
      "Student model score: 0.9980480861452707\n",
      "Student model 9-3 fidelity: 0.9980480861323778\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (660453, 660453) entries\n",
      "Student model 9-4 trained with depth 35 and 2776 leaves:\n",
      "Student model score: 0.9980308915511614\n",
      "Student model 9-4 fidelity: 0.9980308915119893\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (661873, 661873) entries\n",
      "Student model 9-5 trained with depth 33 and 2779 leaves:\n",
      "Student model score: 0.9986229679073446\n",
      "Student model 9-5 fidelity: 0.9986229678910331\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (663293, 663293) entries\n",
      "Student model 9-6 trained with depth 34 and 2782 leaves:\n",
      "Student model score: 0.9963538675375269\n",
      "Student model 9-6 fidelity: 0.9963538675356688\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (664713, 664713) entries\n",
      "Student model 9-7 trained with depth 31 and 2742 leaves:\n",
      "Student model score: 0.996770001859838\n",
      "Student model 9-7 fidelity: 0.9967700018695129\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (666133, 666133) entries\n",
      "Student model 9-8 trained with depth 35 and 2790 leaves:\n",
      "Student model score: 0.9989319207166865\n",
      "Student model 9-8 fidelity: 0.998931920714153\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (667553, 667553) entries\n",
      "Student model 9-9 trained with depth 32 and 2769 leaves:\n",
      "Student model score: 0.9983298539136176\n",
      "Student model 9-9 fidelity: 0.9983298539096649\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (668973, 668973) entries\n",
      "Student model 9-10 trained with depth 34 and 2747 leaves:\n",
      "Student model score: 0.9918860783996665\n",
      "Student model 9-10 fidelity: 0.9918860784737434\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (670393, 670393) entries\n",
      "Student model 9-11 trained with depth 32 and 2748 leaves:\n",
      "Student model score: 0.9981872120434042\n",
      "Student model 9-11 fidelity: 0.9981872120505813\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (671813, 671813) entries\n",
      "Student model 9-12 trained with depth 35 and 2765 leaves:\n",
      "Student model score: 0.9978383842744795\n",
      "Student model 9-12 fidelity: 0.9978383843733717\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (673233, 673233) entries\n",
      "Student model 9-13 trained with depth 31 and 2773 leaves:\n",
      "Student model score: 0.9979729752163111\n",
      "Student model 9-13 fidelity: 0.9979729752080038\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (674653, 674653) entries\n",
      "Student model 9-14 trained with depth 34 and 2802 leaves:\n",
      "Student model score: 0.9962096443095518\n",
      "Student model 9-14 fidelity: 0.9962096443121408\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (676073, 676073) entries\n",
      "Student model 9-15 trained with depth 30 and 2761 leaves:\n",
      "Student model score: 0.9916327406521608\n",
      "Student model 9-15 fidelity: 0.9916327407221206\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (677493, 677493) entries\n",
      "Student model 9-16 trained with depth 32 and 2768 leaves:\n",
      "Student model score: 0.9963902028731441\n",
      "Student model 9-16 fidelity: 0.9963902029298232\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (678913, 678913) entries\n",
      "Student model 9-17 trained with depth 33 and 2802 leaves:\n",
      "Student model score: 0.9944903732521518\n",
      "Student model 9-17 fidelity: 0.9944903732521344\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (680333, 680333) entries\n",
      "Student model 9-18 trained with depth 34 and 2772 leaves:\n",
      "Student model score: 0.9976222018286477\n",
      "Student model 9-18 fidelity: 0.9976222018287284\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (681753, 681753) entries\n",
      "Student model 9-19 trained with depth 35 and 2781 leaves:\n",
      "Student model score: 0.9887807445643425\n",
      "Student model 9-19 fidelity: 0.988780744341204\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (683173, 683173) entries\n",
      "Student model 9-20 trained with depth 29 and 2773 leaves:\n",
      "Student model score: 0.995821671699153\n",
      "Student model 9-20 fidelity: 0.9958216717010528\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (684593, 684593) entries\n",
      "Student model 9-21 trained with depth 30 and 2767 leaves:\n",
      "Student model score: 0.997774555698432\n",
      "Student model 9-21 fidelity: 0.9977745557209806\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (686013, 686013) entries\n",
      "Student model 9-22 trained with depth 33 and 2751 leaves:\n",
      "Student model score: 0.9961824798730898\n",
      "Student model 9-22 fidelity: 0.9961824798782147\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (687433, 687433) entries\n",
      "Student model 9-23 trained with depth 30 and 2788 leaves:\n",
      "Student model score: 0.9947492465927792\n",
      "Student model 9-23 fidelity: 0.9947492466005428\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (688853, 688853) entries\n",
      "Student model 9-24 trained with depth 34 and 2776 leaves:\n",
      "Student model score: 0.9971329636073675\n",
      "Student model 9-24 fidelity: 0.9971329636122235\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (690273, 690273) entries\n",
      "Student model 9-25 trained with depth 34 and 2739 leaves:\n",
      "Student model score: 0.9979665529976054\n",
      "Student model 9-25 fidelity: 0.9979665529968594\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (691693, 691693) entries\n",
      "Student model 9-26 trained with depth 33 and 2785 leaves:\n",
      "Student model score: 0.9976188377142335\n",
      "Student model 9-26 fidelity: 0.9976188377151205\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (693113, 693113) entries\n",
      "Student model 9-27 trained with depth 36 and 2752 leaves:\n",
      "Student model score: 0.9968627596377396\n",
      "Student model 9-27 fidelity: 0.9968627596368529\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (694533, 694533) entries\n",
      "Student model 9-28 trained with depth 31 and 2748 leaves:\n",
      "Student model score: 0.9976187580669557\n",
      "Student model 9-28 fidelity: 0.9976187580543145\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (695953, 695953) entries\n",
      "Student model 9-29 trained with depth 31 and 2756 leaves:\n",
      "Student model score: 0.9990776941691462\n",
      "Student model 9-29 fidelity: 0.9990776942009358\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (697373, 697373) entries\n",
      "Student model 9-30 trained with depth 35 and 2768 leaves:\n",
      "Student model score: 0.9910151560583045\n",
      "Student model 9-30 fidelity: 0.9910151560565973\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (698793, 698793) entries\n",
      "Student model 9-31 trained with depth 33 and 2785 leaves:\n",
      "Student model score: 0.9973403873561133\n",
      "Student model 9-31 fidelity: 0.9973403873707274\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (700213, 700213) entries\n",
      "Student model 9-32 trained with depth 31 and 2759 leaves:\n",
      "Student model score: 0.9971440818798161\n",
      "Student model 9-32 fidelity: 0.997144081879745\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (701633, 701633) entries\n",
      "Student model 9-33 trained with depth 33 and 2770 leaves:\n",
      "Student model score: 0.9972348022322528\n",
      "Student model 9-33 fidelity: 0.9972348023453985\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (703053, 703053) entries\n",
      "Student model 9-34 trained with depth 34 and 2774 leaves:\n",
      "Student model score: 0.9972371726274721\n",
      "Student model 9-34 fidelity: 0.9972371726278475\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (704473, 704473) entries\n",
      "Student model 9-35 trained with depth 32 and 2749 leaves:\n",
      "Student model score: 0.9977040765437373\n",
      "Student model 9-35 fidelity: 0.9977040765429381\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (705893, 705893) entries\n",
      "Student model 9-36 trained with depth 35 and 2780 leaves:\n",
      "Student model score: 0.9943861879981033\n",
      "Student model 9-36 fidelity: 0.9943861879993595\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (707313, 707313) entries\n",
      "Student model 9-37 trained with depth 36 and 2762 leaves:\n",
      "Student model score: 0.997435035655879\n",
      "Student model 9-37 fidelity: 0.9974350356536201\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (708733, 708733) entries\n",
      "Student model 9-38 trained with depth 33 and 2791 leaves:\n",
      "Student model score: 0.9916741980809457\n",
      "Student model 9-38 fidelity: 0.9916741980383847\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (710153, 710153) entries\n",
      "Student model 9-39 trained with depth 32 and 2788 leaves:\n",
      "Student model score: 0.9952141416288295\n",
      "Student model 9-39 fidelity: 0.9952141417284787\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (711573, 711573) entries\n",
      "Student model 9-40 trained with depth 35 and 2758 leaves:\n",
      "Student model score: 0.9975862642416248\n",
      "Student model 9-40 fidelity: 0.9975862642415624\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (712993, 712993) entries\n",
      "Student model 9-41 trained with depth 38 and 2769 leaves:\n",
      "Student model score: 0.9948003889834335\n",
      "Student model 9-41 fidelity: 0.994800388976902\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (714413, 714413) entries\n",
      "Student model 9-42 trained with depth 35 and 2760 leaves:\n",
      "Student model score: 0.9977938441497392\n",
      "Student model 9-42 fidelity: 0.9977938441495552\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (715833, 715833) entries\n",
      "Student model 9-43 trained with depth 34 and 2798 leaves:\n",
      "Student model score: 0.9973155589279722\n",
      "Student model 9-43 fidelity: 0.9973155588461468\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (717253, 717253) entries\n",
      "Student model 9-44 trained with depth 31 and 2760 leaves:\n",
      "Student model score: 0.9978827414180462\n",
      "Student model 9-44 fidelity: 0.9978827414228865\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (718673, 718673) entries\n",
      "Student model 9-45 trained with depth 29 and 2759 leaves:\n",
      "Student model score: 0.9970726785061659\n",
      "Student model 9-45 fidelity: 0.9970726784837456\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (720093, 720093) entries\n",
      "Student model 9-46 trained with depth 32 and 2758 leaves:\n",
      "Student model score: 0.9966864014471406\n",
      "Student model 9-46 fidelity: 0.9966864014517932\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (721513, 721513) entries\n",
      "Student model 9-47 trained with depth 31 and 2795 leaves:\n",
      "Student model score: 0.9972825995212888\n",
      "Student model 9-47 fidelity: 0.9972825998753092\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (722933, 722933) entries\n",
      "Student model 9-48 trained with depth 34 and 2752 leaves:\n",
      "Student model score: 0.9975483143776783\n",
      "Student model 9-48 fidelity: 0.9975483143852627\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (724353, 724353) entries\n",
      "Student model 9-49 trained with depth 36 and 2752 leaves:\n",
      "Student model score: 0.9946971362690332\n",
      "Student model 9-49 fidelity: 0.9946971362620387\n",
      "Model explanation training (agreement, fidelity): (0.9498761680315593, 0.9986530917566536)\n",
      "Model Explanation size: 5593\n",
      "Top-k Prunned Model explanation size: 77\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"1937pt\" height=\"2037pt\"\n",
       " viewBox=\"0.00 0.00 1937.00 2037.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2033)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-2033 1933,-2033 1933,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#f9e2d1\" stroke=\"black\" d=\"M964.5,-2029C964.5,-2029 769.5,-2029 769.5,-2029 763.5,-2029 757.5,-2023 757.5,-2017 757.5,-2017 757.5,-1973 757.5,-1973 757.5,-1967 763.5,-1961 769.5,-1961 769.5,-1961 964.5,-1961 964.5,-1961 970.5,-1961 976.5,-1967 976.5,-1973 976.5,-1973 976.5,-2017 976.5,-2017 976.5,-2023 970.5,-2029 964.5,-2029\"/>\n",
       "<text text-anchor=\"start\" x=\"773.5\" y=\"-2013.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent â‰¤ 119203548.0</text>\n",
       "<text text-anchor=\"start\" x=\"765.5\" y=\"-1998.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 175319.506</text>\n",
       "<text text-anchor=\"start\" x=\"809.5\" y=\"-1983.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3311</text>\n",
       "<text text-anchor=\"start\" x=\"808\" y=\"-1968.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 232.366</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#fefbf9\" stroke=\"black\" d=\"M846,-1925C846,-1925 660,-1925 660,-1925 654,-1925 648,-1919 648,-1913 648,-1913 648,-1869 648,-1869 648,-1863 654,-1857 660,-1857 660,-1857 846,-1857 846,-1857 852,-1857 858,-1863 858,-1869 858,-1869 858,-1913 858,-1913 858,-1919 852,-1925 846,-1925\"/>\n",
       "<text text-anchor=\"start\" x=\"704.5\" y=\"-1909.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ 6352.6</text>\n",
       "<text text-anchor=\"start\" x=\"656\" y=\"-1894.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 10714.981</text>\n",
       "<text text-anchor=\"start\" x=\"695.5\" y=\"-1879.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2600</text>\n",
       "<text text-anchor=\"start\" x=\"698.5\" y=\"-1864.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 31.409</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M829.99,-1960.88C819.73,-1951.71 808.49,-1941.65 797.84,-1932.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"800,-1929.36 790.22,-1925.3 795.34,-1934.58 800,-1929.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"791.6\" y=\"-1946.56\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 76 -->\n",
       "<g id=\"node77\" class=\"node\">\n",
       "<title>76</title>\n",
       "<path fill=\"#e5833c\" stroke=\"black\" d=\"M1074,-1917.5C1074,-1917.5 888,-1917.5 888,-1917.5 882,-1917.5 876,-1911.5 876,-1905.5 876,-1905.5 876,-1876.5 876,-1876.5 876,-1870.5 882,-1864.5 888,-1864.5 888,-1864.5 1074,-1864.5 1074,-1864.5 1080,-1864.5 1086,-1870.5 1086,-1876.5 1086,-1876.5 1086,-1905.5 1086,-1905.5 1086,-1911.5 1080,-1917.5 1074,-1917.5\"/>\n",
       "<text text-anchor=\"start\" x=\"884\" y=\"-1902.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 89543.615</text>\n",
       "<text text-anchor=\"start\" x=\"928\" y=\"-1887.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 711</text>\n",
       "<text text-anchor=\"start\" x=\"922\" y=\"-1872.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 967.233</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;76 -->\n",
       "<g id=\"edge76\" class=\"edge\">\n",
       "<title>0&#45;&gt;76</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M904.01,-1960.88C917.16,-1949.12 931.94,-1935.89 945.01,-1924.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"947.36,-1926.8 952.48,-1917.52 942.69,-1921.58 947.36,-1926.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"951.09\" y=\"-1938.78\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#e6843e\" stroke=\"black\" d=\"M722.5,-1813.5C722.5,-1813.5 563.5,-1813.5 563.5,-1813.5 557.5,-1813.5 551.5,-1807.5 551.5,-1801.5 551.5,-1801.5 551.5,-1772.5 551.5,-1772.5 551.5,-1766.5 557.5,-1760.5 563.5,-1760.5 563.5,-1760.5 722.5,-1760.5 722.5,-1760.5 728.5,-1760.5 734.5,-1766.5 734.5,-1772.5 734.5,-1772.5 734.5,-1801.5 734.5,-1801.5 734.5,-1807.5 728.5,-1813.5 722.5,-1813.5\"/>\n",
       "<text text-anchor=\"start\" x=\"559.5\" y=\"-1798.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 71.682</text>\n",
       "<text text-anchor=\"start\" x=\"594.5\" y=\"-1783.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\n",
       "<text text-anchor=\"start\" x=\"584\" y=\"-1768.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 959.154</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M717.29,-1856.88C704.71,-1845.23 690.6,-1832.14 678.08,-1820.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"680.23,-1817.75 670.52,-1813.52 675.47,-1822.89 680.23,-1817.75\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#fefcfa\" stroke=\"black\" d=\"M963,-1821C963,-1821 765,-1821 765,-1821 759,-1821 753,-1815 753,-1809 753,-1809 753,-1765 753,-1765 753,-1759 759,-1753 765,-1753 765,-1753 963,-1753 963,-1753 969,-1753 975,-1759 975,-1765 975,-1765 975,-1809 975,-1809 975,-1815 969,-1821 963,-1821\"/>\n",
       "<text text-anchor=\"start\" x=\"761\" y=\"-1805.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesRetrans_3 â‰¤ 3224532.0</text>\n",
       "<text text-anchor=\"start\" x=\"771.5\" y=\"-1790.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 5753.304</text>\n",
       "<text text-anchor=\"start\" x=\"806.5\" y=\"-1775.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2585</text>\n",
       "<text text-anchor=\"start\" x=\"809.5\" y=\"-1760.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 26.025</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M789.04,-1856.88C799.02,-1847.71 809.97,-1837.65 820.34,-1828.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"822.77,-1830.64 827.76,-1821.3 818.03,-1825.49 822.77,-1830.64\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#fefcfb\" stroke=\"black\" d=\"M845.5,-1717C845.5,-1717 668.5,-1717 668.5,-1717 662.5,-1717 656.5,-1711 656.5,-1705 656.5,-1705 656.5,-1661 656.5,-1661 656.5,-1655 662.5,-1649 668.5,-1649 668.5,-1649 845.5,-1649 845.5,-1649 851.5,-1649 857.5,-1655 857.5,-1661 857.5,-1661 857.5,-1705 857.5,-1705 857.5,-1711 851.5,-1717 845.5,-1717\"/>\n",
       "<text text-anchor=\"start\" x=\"668\" y=\"-1701.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent â‰¤ 12666004.5</text>\n",
       "<text text-anchor=\"start\" x=\"664.5\" y=\"-1686.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2549.946</text>\n",
       "<text text-anchor=\"start\" x=\"699.5\" y=\"-1671.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2576</text>\n",
       "<text text-anchor=\"start\" x=\"702.5\" y=\"-1656.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 22.677</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M829.26,-1752.88C819.73,-1743.8 809.29,-1733.85 799.38,-1724.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"801.58,-1721.67 791.93,-1717.3 796.75,-1726.73 801.58,-1721.67\"/>\n",
       "</g>\n",
       "<!-- 75 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>75</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M1056,-1709.5C1056,-1709.5 888,-1709.5 888,-1709.5 882,-1709.5 876,-1703.5 876,-1697.5 876,-1697.5 876,-1668.5 876,-1668.5 876,-1662.5 882,-1656.5 888,-1656.5 888,-1656.5 1056,-1656.5 1056,-1656.5 1062,-1656.5 1068,-1662.5 1068,-1668.5 1068,-1668.5 1068,-1697.5 1068,-1697.5 1068,-1703.5 1062,-1709.5 1056,-1709.5\"/>\n",
       "<text text-anchor=\"start\" x=\"884\" y=\"-1694.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 781.806</text>\n",
       "<text text-anchor=\"start\" x=\"928\" y=\"-1679.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"913\" y=\"-1664.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 984.479</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;75 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>3&#45;&gt;75</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M899.06,-1752.88C911.41,-1741.23 925.26,-1728.14 937.55,-1716.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"940.11,-1718.93 944.98,-1709.52 935.31,-1713.84 940.11,-1718.93\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M738.5,-1613C738.5,-1613 553.5,-1613 553.5,-1613 547.5,-1613 541.5,-1607 541.5,-1601 541.5,-1601 541.5,-1557 541.5,-1557 541.5,-1551 547.5,-1545 553.5,-1545 553.5,-1545 738.5,-1545 738.5,-1545 744.5,-1545 750.5,-1551 750.5,-1557 750.5,-1557 750.5,-1601 750.5,-1601 750.5,-1607 744.5,-1613 738.5,-1613\"/>\n",
       "<text text-anchor=\"start\" x=\"549.5\" y=\"-1597.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_1 â‰¤ 895956.5</text>\n",
       "<text text-anchor=\"start\" x=\"558\" y=\"-1582.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 428.781</text>\n",
       "<text text-anchor=\"start\" x=\"588.5\" y=\"-1567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2265</text>\n",
       "<text text-anchor=\"start\" x=\"596\" y=\"-1552.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.637</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M720.96,-1648.88C710.98,-1639.71 700.03,-1629.65 689.66,-1620.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"691.97,-1617.49 682.24,-1613.3 687.23,-1622.64 691.97,-1617.49\"/>\n",
       "</g>\n",
       "<!-- 74 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>74</title>\n",
       "<path fill=\"#fcefe6\" stroke=\"black\" d=\"M957.5,-1605.5C957.5,-1605.5 780.5,-1605.5 780.5,-1605.5 774.5,-1605.5 768.5,-1599.5 768.5,-1593.5 768.5,-1593.5 768.5,-1564.5 768.5,-1564.5 768.5,-1558.5 774.5,-1552.5 780.5,-1552.5 780.5,-1552.5 957.5,-1552.5 957.5,-1552.5 963.5,-1552.5 969.5,-1558.5 969.5,-1564.5 969.5,-1564.5 969.5,-1593.5 969.5,-1593.5 969.5,-1599.5 963.5,-1605.5 957.5,-1605.5\"/>\n",
       "<text text-anchor=\"start\" x=\"776.5\" y=\"-1590.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 6107.233</text>\n",
       "<text text-anchor=\"start\" x=\"816\" y=\"-1575.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 311</text>\n",
       "<text text-anchor=\"start\" x=\"810\" y=\"-1560.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 124.929</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;74 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>4&#45;&gt;74</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M793.36,-1648.88C806.16,-1637.23 820.53,-1624.14 833.28,-1612.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"835.94,-1614.84 840.98,-1605.52 831.23,-1609.67 835.94,-1614.84\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#fef9f5\" stroke=\"black\" d=\"M629.5,-1501.5C629.5,-1501.5 452.5,-1501.5 452.5,-1501.5 446.5,-1501.5 440.5,-1495.5 440.5,-1489.5 440.5,-1489.5 440.5,-1460.5 440.5,-1460.5 440.5,-1454.5 446.5,-1448.5 452.5,-1448.5 452.5,-1448.5 629.5,-1448.5 629.5,-1448.5 635.5,-1448.5 641.5,-1454.5 641.5,-1460.5 641.5,-1460.5 641.5,-1489.5 641.5,-1489.5 641.5,-1495.5 635.5,-1501.5 629.5,-1501.5\"/>\n",
       "<text text-anchor=\"start\" x=\"448.5\" y=\"-1486.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 5124.761</text>\n",
       "<text text-anchor=\"start\" x=\"488\" y=\"-1471.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 121</text>\n",
       "<text text-anchor=\"start\" x=\"486.5\" y=\"-1456.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 49.894</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M611.91,-1544.88C599.91,-1533.23 586.44,-1520.14 574.49,-1508.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"576.88,-1505.98 567.27,-1501.52 572,-1511 576.88,-1505.98\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M830.5,-1509C830.5,-1509 671.5,-1509 671.5,-1509 665.5,-1509 659.5,-1503 659.5,-1497 659.5,-1497 659.5,-1453 659.5,-1453 659.5,-1447 665.5,-1441 671.5,-1441 671.5,-1441 830.5,-1441 830.5,-1441 836.5,-1441 842.5,-1447 842.5,-1453 842.5,-1453 842.5,-1497 842.5,-1497 842.5,-1503 836.5,-1509 830.5,-1509\"/>\n",
       "<text text-anchor=\"start\" x=\"677\" y=\"-1493.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_4 â‰¤ 136993.523</text>\n",
       "<text text-anchor=\"start\" x=\"667.5\" y=\"-1478.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 62.272</text>\n",
       "<text text-anchor=\"start\" x=\"693.5\" y=\"-1463.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2144</text>\n",
       "<text text-anchor=\"start\" x=\"701\" y=\"-1448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.308</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M680.09,-1544.88C689.44,-1535.8 699.69,-1525.85 709.41,-1516.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"711.99,-1518.78 716.72,-1509.3 707.11,-1513.76 711.99,-1518.78\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M676,-1405C676,-1405 508,-1405 508,-1405 502,-1405 496,-1399 496,-1393 496,-1393 496,-1349 496,-1349 496,-1343 502,-1337 508,-1337 508,-1337 676,-1337 676,-1337 682,-1337 688,-1343 688,-1349 688,-1349 688,-1393 688,-1393 688,-1399 682,-1405 676,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"533.5\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_3 â‰¤ 84.1</text>\n",
       "<text text-anchor=\"start\" x=\"504\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 192.875</text>\n",
       "<text text-anchor=\"start\" x=\"539\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 606</text>\n",
       "<text text-anchor=\"start\" x=\"537.5\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.128</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M699.38,-1440.88C684.3,-1431.21 667.7,-1420.56 652.15,-1410.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"653.94,-1407.58 643.64,-1405.12 650.16,-1413.47 653.94,-1407.58\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M986,-1405C986,-1405 836,-1405 836,-1405 830,-1405 824,-1399 824,-1393 824,-1393 824,-1349 824,-1349 824,-1343 830,-1337 836,-1337 836,-1337 986,-1337 986,-1337 992,-1337 998,-1343 998,-1349 998,-1349 998,-1393 998,-1393 998,-1399 992,-1405 986,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"837\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_2 â‰¤ 870261.812</text>\n",
       "<text text-anchor=\"start\" x=\"832\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.799</text>\n",
       "<text text-anchor=\"start\" x=\"853.5\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1538</text>\n",
       "<text text-anchor=\"start\" x=\"861\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.803</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;23 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>7&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M802.95,-1440.88C818.11,-1431.21 834.82,-1420.56 850.47,-1410.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"852.49,-1413.45 859.04,-1405.12 848.73,-1407.55 852.49,-1413.45\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#f9e4d5\" stroke=\"black\" d=\"M453,-1293.5C453,-1293.5 321,-1293.5 321,-1293.5 315,-1293.5 309,-1287.5 309,-1281.5 309,-1281.5 309,-1252.5 309,-1252.5 309,-1246.5 315,-1240.5 321,-1240.5 321,-1240.5 453,-1240.5 453,-1240.5 459,-1240.5 465,-1246.5 465,-1252.5 465,-1252.5 465,-1281.5 465,-1281.5 465,-1287.5 459,-1293.5 453,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"317\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"332.5\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 213.16</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M525.44,-1336.88C500.35,-1324.4 471.98,-1310.28 447.48,-1298.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"448.8,-1294.84 438.29,-1293.52 445.68,-1301.11 448.8,-1294.84\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M689,-1301C689,-1301 495,-1301 495,-1301 489,-1301 483,-1295 483,-1289 483,-1289 483,-1245 483,-1245 483,-1239 489,-1233 495,-1233 495,-1233 689,-1233 689,-1233 695,-1233 701,-1239 701,-1245 701,-1245 701,-1289 701,-1289 701,-1295 695,-1301 689,-1301\"/>\n",
       "<text text-anchor=\"start\" x=\"491\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_1 â‰¤ 1396089.5</text>\n",
       "<text text-anchor=\"start\" x=\"504\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 124.946</text>\n",
       "<text text-anchor=\"start\" x=\"539\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 605</text>\n",
       "<text text-anchor=\"start\" x=\"542\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.792</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M592,-1336.88C592,-1328.78 592,-1319.98 592,-1311.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"595.5,-1311.3 592,-1301.3 588.5,-1311.3 595.5,-1311.3\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#fefcfa\" stroke=\"black\" d=\"M475,-1189.5C475,-1189.5 307,-1189.5 307,-1189.5 301,-1189.5 295,-1183.5 295,-1177.5 295,-1177.5 295,-1148.5 295,-1148.5 295,-1142.5 301,-1136.5 307,-1136.5 307,-1136.5 475,-1136.5 475,-1136.5 481,-1136.5 487,-1142.5 487,-1148.5 487,-1148.5 487,-1177.5 487,-1177.5 487,-1183.5 481,-1189.5 475,-1189.5\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-1174.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1775.21</text>\n",
       "<text text-anchor=\"start\" x=\"342.5\" y=\"-1159.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n",
       "<text text-anchor=\"start\" x=\"336.5\" y=\"-1144.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 27.092</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M526.74,-1232.88C502.14,-1220.4 474.32,-1206.28 450.3,-1194.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"451.79,-1190.92 441.29,-1189.52 448.62,-1197.17 451.79,-1190.92\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M667,-1197C667,-1197 517,-1197 517,-1197 511,-1197 505,-1191 505,-1185 505,-1185 505,-1141 505,-1141 505,-1135 511,-1129 517,-1129 517,-1129 667,-1129 667,-1129 673,-1129 679,-1135 679,-1141 679,-1141 679,-1185 679,-1185 679,-1191 673,-1197 667,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"522.5\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_4 â‰¤ 21944.375</text>\n",
       "<text text-anchor=\"start\" x=\"513\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 4.109</text>\n",
       "<text text-anchor=\"start\" x=\"539\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 570</text>\n",
       "<text text-anchor=\"start\" x=\"546.5\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.73</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M592,-1232.88C592,-1224.78 592,-1215.98 592,-1207.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"595.5,-1207.3 592,-1197.3 588.5,-1207.3 595.5,-1207.3\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M472,-1093C472,-1093 298,-1093 298,-1093 292,-1093 286,-1087 286,-1081 286,-1081 286,-1037 286,-1037 286,-1031 292,-1025 298,-1025 298,-1025 472,-1025 472,-1025 478,-1025 484,-1031 484,-1037 484,-1037 484,-1081 484,-1081 484,-1087 478,-1093 472,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"294\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime_2 â‰¤ 2374000.0</text>\n",
       "<text text-anchor=\"start\" x=\"306\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.574</text>\n",
       "<text text-anchor=\"start\" x=\"332\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 244</text>\n",
       "<text text-anchor=\"start\" x=\"335\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.531</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.79,-1128.88C504.53,-1118.9 482.15,-1107.87 461.35,-1097.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.74,-1094.41 452.22,-1093.12 459.65,-1100.68 462.74,-1094.41\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M667,-1085.5C667,-1085.5 517,-1085.5 517,-1085.5 511,-1085.5 505,-1079.5 505,-1073.5 505,-1073.5 505,-1044.5 505,-1044.5 505,-1038.5 511,-1032.5 517,-1032.5 517,-1032.5 667,-1032.5 667,-1032.5 673,-1032.5 679,-1038.5 679,-1044.5 679,-1044.5 679,-1073.5 679,-1073.5 679,-1079.5 673,-1085.5 667,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"513\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 5.915</text>\n",
       "<text text-anchor=\"start\" x=\"539\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 326</text>\n",
       "<text text-anchor=\"start\" x=\"546.5\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.13</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>12&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M592,-1128.88C592,-1118.33 592,-1106.6 592,-1095.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"595.5,-1095.52 592,-1085.52 588.5,-1095.52 595.5,-1095.52\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M257.5,-981.5C257.5,-981.5 116.5,-981.5 116.5,-981.5 110.5,-981.5 104.5,-975.5 104.5,-969.5 104.5,-969.5 104.5,-940.5 104.5,-940.5 104.5,-934.5 110.5,-928.5 116.5,-928.5 116.5,-928.5 257.5,-928.5 257.5,-928.5 263.5,-928.5 269.5,-934.5 269.5,-940.5 269.5,-940.5 269.5,-969.5 269.5,-969.5 269.5,-975.5 263.5,-981.5 257.5,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"112.5\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.24</text>\n",
       "<text text-anchor=\"start\" x=\"138.5\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 24</text>\n",
       "<text text-anchor=\"start\" x=\"137\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.483</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M320.72,-1024.88C296.59,-1012.46 269.32,-998.41 245.72,-986.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.03,-982.99 236.54,-981.52 243.83,-989.21 247.03,-982.99\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M470,-989C470,-989 300,-989 300,-989 294,-989 288,-983 288,-977 288,-977 288,-933 288,-933 288,-927 294,-921 300,-921 300,-921 470,-921 470,-921 476,-921 482,-927 482,-933 482,-933 482,-977 482,-977 482,-983 476,-989 470,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"296\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesAcked â‰¤ 4751803.0</text>\n",
       "<text text-anchor=\"start\" x=\"306\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.368</text>\n",
       "<text text-anchor=\"start\" x=\"332\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 220</text>\n",
       "<text text-anchor=\"start\" x=\"335\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.646</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385,-1024.88C385,-1016.78 385,-1007.98 385,-999.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"388.5,-999.3 385,-989.3 381.5,-999.3 388.5,-999.3\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M247,-877.5C247,-877.5 97,-877.5 97,-877.5 91,-877.5 85,-871.5 85,-865.5 85,-865.5 85,-836.5 85,-836.5 85,-830.5 91,-824.5 97,-824.5 97,-824.5 247,-824.5 247,-824.5 253,-824.5 259,-830.5 259,-836.5 259,-836.5 259,-865.5 259,-865.5 259,-871.5 253,-877.5 247,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"93\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.461</text>\n",
       "<text text-anchor=\"start\" x=\"123.5\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 29</text>\n",
       "<text text-anchor=\"start\" x=\"117.5\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.325</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M315.85,-920.88C289.66,-908.35 260.03,-894.16 234.5,-881.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.82,-878.68 225.29,-877.52 232.8,-885 235.82,-878.68\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M483,-885C483,-885 289,-885 289,-885 283,-885 277,-879 277,-873 277,-873 277,-829 277,-829 277,-823 283,-817 289,-817 289,-817 483,-817 483,-817 489,-817 495,-823 495,-829 495,-829 495,-873 495,-873 495,-879 489,-885 483,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"285\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_4 â‰¤ 8793193.5</text>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.121</text>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 191</text>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.542</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.32,-920.88C385.4,-912.78 385.49,-903.98 385.57,-895.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"389.08,-895.33 385.67,-885.3 382.08,-895.26 389.08,-895.33\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M280,-781C280,-781 86,-781 86,-781 80,-781 74,-775 74,-769 74,-769 74,-725 74,-725 74,-719 80,-713 86,-713 86,-713 280,-713 280,-713 286,-713 292,-719 292,-725 292,-725 292,-769 292,-769 292,-775 286,-781 280,-781\"/>\n",
       "<text text-anchor=\"start\" x=\"82\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_3 â‰¤ 6497897.0</text>\n",
       "<text text-anchor=\"start\" x=\"104\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.115</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 163</text>\n",
       "<text text-anchor=\"start\" x=\"133\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.482</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M320.09,-816.88C300.22,-806.9 278.27,-795.87 257.87,-785.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"259.43,-782.49 248.92,-781.12 256.29,-788.74 259.43,-782.49\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M472,-773.5C472,-773.5 322,-773.5 322,-773.5 316,-773.5 310,-767.5 310,-761.5 310,-761.5 310,-732.5 310,-732.5 310,-726.5 316,-720.5 322,-720.5 322,-720.5 472,-720.5 472,-720.5 478,-720.5 484,-726.5 484,-732.5 484,-732.5 484,-761.5 484,-761.5 484,-767.5 478,-773.5 472,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"318\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.012</text>\n",
       "<text text-anchor=\"start\" x=\"348.5\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 28</text>\n",
       "<text text-anchor=\"start\" x=\"347\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.895</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M389.57,-816.88C390.72,-806.22 392,-794.35 393.17,-783.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"396.66,-783.84 394.25,-773.52 389.7,-783.09 396.66,-783.84\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M162,-669.5C162,-669.5 12,-669.5 12,-669.5 6,-669.5 0,-663.5 0,-657.5 0,-657.5 0,-628.5 0,-628.5 0,-622.5 6,-616.5 12,-616.5 12,-616.5 162,-616.5 162,-616.5 168,-616.5 174,-622.5 174,-628.5 174,-628.5 174,-657.5 174,-657.5 174,-663.5 168,-669.5 162,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.112</text>\n",
       "<text text-anchor=\"start\" x=\"38.5\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 88</text>\n",
       "<text text-anchor=\"start\" x=\"37\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.619</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.83,-712.88C140.96,-701.34 128.78,-688.39 117.93,-676.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.42,-674.4 111.02,-669.52 115.32,-679.2 120.42,-674.4\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M354,-669.5C354,-669.5 204,-669.5 204,-669.5 198,-669.5 192,-663.5 192,-657.5 192,-657.5 192,-628.5 192,-628.5 192,-622.5 198,-616.5 204,-616.5 204,-616.5 354,-616.5 354,-616.5 360,-616.5 366,-622.5 366,-628.5 366,-628.5 366,-657.5 366,-657.5 366,-663.5 360,-669.5 354,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.071</text>\n",
       "<text text-anchor=\"start\" x=\"230.5\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 75</text>\n",
       "<text text-anchor=\"start\" x=\"229\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.321</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>18&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.17,-712.88C225.04,-701.34 237.22,-688.39 248.07,-676.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.68,-679.2 254.98,-669.52 245.58,-674.4 250.68,-679.2\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M986,-1301C986,-1301 836,-1301 836,-1301 830,-1301 824,-1295 824,-1289 824,-1289 824,-1245 824,-1245 824,-1239 830,-1233 836,-1233 836,-1233 986,-1233 986,-1233 992,-1233 998,-1239 998,-1245 998,-1245 998,-1289 998,-1289 998,-1295 992,-1301 986,-1301\"/>\n",
       "<text text-anchor=\"start\" x=\"837\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_2 â‰¤ 306382.203</text>\n",
       "<text text-anchor=\"start\" x=\"832\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.372</text>\n",
       "<text text-anchor=\"start\" x=\"853.5\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1151</text>\n",
       "<text text-anchor=\"start\" x=\"861\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.568</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>23&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M911,-1336.88C911,-1328.78 911,-1319.98 911,-1311.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"914.5,-1311.3 911,-1301.3 907.5,-1311.3 914.5,-1311.3\"/>\n",
       "</g>\n",
       "<!-- 73 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>73</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1178,-1293.5C1178,-1293.5 1028,-1293.5 1028,-1293.5 1022,-1293.5 1016,-1287.5 1016,-1281.5 1016,-1281.5 1016,-1252.5 1016,-1252.5 1016,-1246.5 1022,-1240.5 1028,-1240.5 1028,-1240.5 1178,-1240.5 1178,-1240.5 1184,-1240.5 1190,-1246.5 1190,-1252.5 1190,-1252.5 1190,-1281.5 1190,-1281.5 1190,-1287.5 1184,-1293.5 1178,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1024\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.128</text>\n",
       "<text text-anchor=\"start\" x=\"1050\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 387</text>\n",
       "<text text-anchor=\"start\" x=\"1053\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.529</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;73 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>23&#45;&gt;73</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M973.34,-1336.88C996.73,-1324.46 1023.18,-1310.41 1046.06,-1298.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1047.77,-1301.3 1054.96,-1293.52 1044.49,-1295.12 1047.77,-1301.3\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M986,-1197C986,-1197 836,-1197 836,-1197 830,-1197 824,-1191 824,-1185 824,-1185 824,-1141 824,-1141 824,-1135 830,-1129 836,-1129 836,-1129 986,-1129 986,-1129 992,-1129 998,-1135 998,-1141 998,-1141 998,-1185 998,-1185 998,-1191 992,-1197 986,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"839\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 1915.45</text>\n",
       "<text text-anchor=\"start\" x=\"832\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.664</text>\n",
       "<text text-anchor=\"start\" x=\"858\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 487</text>\n",
       "<text text-anchor=\"start\" x=\"861\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.403</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M911,-1232.88C911,-1224.78 911,-1215.98 911,-1207.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"914.5,-1207.3 911,-1197.3 907.5,-1207.3 914.5,-1207.3\"/>\n",
       "</g>\n",
       "<!-- 60 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>60</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1441,-1197C1441,-1197 1291,-1197 1291,-1197 1285,-1197 1279,-1191 1279,-1185 1279,-1185 1279,-1141 1279,-1141 1279,-1135 1285,-1129 1291,-1129 1291,-1129 1441,-1129 1441,-1129 1447,-1129 1453,-1135 1453,-1141 1453,-1141 1453,-1185 1453,-1185 1453,-1191 1447,-1197 1441,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"1289.5\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_1 â‰¤ 104930.0</text>\n",
       "<text text-anchor=\"start\" x=\"1287\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.272</text>\n",
       "<text text-anchor=\"start\" x=\"1313\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 664</text>\n",
       "<text text-anchor=\"start\" x=\"1316\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.956</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;60 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>24&#45;&gt;60</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M998.22,-1235.43C1001.17,-1234.59 1004.11,-1233.77 1007,-1233 1094.56,-1209.66 1196.13,-1190.89 1268.84,-1178.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1269.6,-1182.28 1278.9,-1177.2 1268.47,-1175.37 1269.6,-1182.28\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M899.5,-1093C899.5,-1093 710.5,-1093 710.5,-1093 704.5,-1093 698.5,-1087 698.5,-1081 698.5,-1081 698.5,-1037 698.5,-1037 698.5,-1031 704.5,-1025 710.5,-1025 710.5,-1025 899.5,-1025 899.5,-1025 905.5,-1025 911.5,-1031 911.5,-1037 911.5,-1037 911.5,-1081 911.5,-1081 911.5,-1087 905.5,-1093 899.5,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"706.5\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesRetrans_4 â‰¤ 146054.0</text>\n",
       "<text text-anchor=\"start\" x=\"726\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.386</text>\n",
       "<text text-anchor=\"start\" x=\"752\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 217</text>\n",
       "<text text-anchor=\"start\" x=\"755\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.089</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>25&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M876.58,-1128.88C867.14,-1119.8 856.8,-1109.85 846.98,-1100.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"849.24,-1097.71 839.61,-1093.3 844.38,-1102.76 849.24,-1097.71\"/>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>39</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1092,-1093C1092,-1093 942,-1093 942,-1093 936,-1093 930,-1087 930,-1081 930,-1081 930,-1037 930,-1037 930,-1031 936,-1025 942,-1025 942,-1025 1092,-1025 1092,-1025 1098,-1025 1104,-1031 1104,-1037 1104,-1037 1104,-1081 1104,-1081 1104,-1087 1098,-1093 1092,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"958.5\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_4 â‰¤ 41.8</text>\n",
       "<text text-anchor=\"start\" x=\"938\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.207</text>\n",
       "<text text-anchor=\"start\" x=\"964\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 270</text>\n",
       "<text text-anchor=\"start\" x=\"967\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.853</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;39 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>25&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M945.42,-1128.88C954.86,-1119.8 965.2,-1109.85 975.02,-1100.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"977.62,-1102.76 982.39,-1093.3 972.76,-1097.71 977.62,-1102.76\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M690.5,-989C690.5,-989 511.5,-989 511.5,-989 505.5,-989 499.5,-983 499.5,-977 499.5,-977 499.5,-933 499.5,-933 499.5,-927 505.5,-921 511.5,-921 511.5,-921 690.5,-921 690.5,-921 696.5,-921 702.5,-927 702.5,-933 702.5,-933 702.5,-977 702.5,-977 702.5,-983 696.5,-989 690.5,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"507.5\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime â‰¤ 2182068.0</text>\n",
       "<text text-anchor=\"start\" x=\"522\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.131</text>\n",
       "<text text-anchor=\"start\" x=\"548\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 197</text>\n",
       "<text text-anchor=\"start\" x=\"555.5\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.27</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M738.77,-1024.88C718.8,-1014.9 696.74,-1003.87 676.24,-993.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"677.76,-990.47 667.25,-989.12 674.63,-996.73 677.76,-990.47\"/>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>38</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M883,-981.5C883,-981.5 733,-981.5 733,-981.5 727,-981.5 721,-975.5 721,-969.5 721,-969.5 721,-940.5 721,-940.5 721,-934.5 727,-928.5 733,-928.5 733,-928.5 883,-928.5 883,-928.5 889,-928.5 895,-934.5 895,-940.5 895,-940.5 895,-969.5 895,-969.5 895,-975.5 889,-981.5 883,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"729\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.376</text>\n",
       "<text text-anchor=\"start\" x=\"759.5\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 20</text>\n",
       "<text text-anchor=\"start\" x=\"758\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.301</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;38 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>26&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M805.97,-1024.88C806.29,-1014.22 806.64,-1002.35 806.96,-991.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"810.45,-991.62 807.25,-981.52 803.46,-991.41 810.45,-991.62\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M676,-877.5C676,-877.5 526,-877.5 526,-877.5 520,-877.5 514,-871.5 514,-865.5 514,-865.5 514,-836.5 514,-836.5 514,-830.5 520,-824.5 526,-824.5 526,-824.5 676,-824.5 676,-824.5 682,-824.5 688,-830.5 688,-836.5 688,-836.5 688,-865.5 688,-865.5 688,-871.5 682,-877.5 676,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"522\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.148</text>\n",
       "<text text-anchor=\"start\" x=\"552.5\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n",
       "<text text-anchor=\"start\" x=\"551\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.191</text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>27&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-920.88C601,-910.33 601,-898.6 601,-887.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-887.52 601,-877.52 597.5,-887.52 604.5,-887.52\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M868,-885C868,-885 718,-885 718,-885 712,-885 706,-879 706,-873 706,-873 706,-829 706,-829 706,-823 712,-817 718,-817 718,-817 868,-817 868,-817 874,-817 880,-823 880,-829 880,-829 880,-873 880,-873 880,-879 874,-885 868,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"716.5\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_4 â‰¤ 4617.475</text>\n",
       "<text text-anchor=\"start\" x=\"714\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.823</text>\n",
       "<text text-anchor=\"start\" x=\"740\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 154</text>\n",
       "<text text-anchor=\"start\" x=\"743\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.013</text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>27&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M663.34,-920.88C681.96,-910.99 702.52,-900.07 721.67,-889.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"723.46,-892.91 730.65,-885.12 720.17,-886.73 723.46,-892.91\"/>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M752,-773.5C752,-773.5 602,-773.5 602,-773.5 596,-773.5 590,-767.5 590,-761.5 590,-761.5 590,-732.5 590,-732.5 590,-726.5 596,-720.5 602,-720.5 602,-720.5 752,-720.5 752,-720.5 758,-720.5 764,-726.5 764,-732.5 764,-732.5 764,-761.5 764,-761.5 764,-767.5 758,-773.5 752,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"598\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.525</text>\n",
       "<text text-anchor=\"start\" x=\"624\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 133</text>\n",
       "<text text-anchor=\"start\" x=\"627\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.222</text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>29&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M755.34,-816.88C741.96,-805.12 726.92,-791.89 713.62,-780.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"715.84,-777.49 706.02,-773.52 711.22,-782.75 715.84,-777.49\"/>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>31</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M944,-781C944,-781 794,-781 794,-781 788,-781 782,-775 782,-769 782,-769 782,-725 782,-725 782,-719 788,-713 794,-713 794,-713 944,-713 944,-713 950,-713 956,-719 956,-725 956,-725 956,-769 956,-769 956,-775 950,-781 944,-781\"/>\n",
       "<text text-anchor=\"start\" x=\"792.5\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_4 â‰¤ 15981.05</text>\n",
       "<text text-anchor=\"start\" x=\"790\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.679</text>\n",
       "<text text-anchor=\"start\" x=\"820.5\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 21</text>\n",
       "<text text-anchor=\"start\" x=\"819\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.689</text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;31 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>29&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M817.68,-816.88C824.18,-808.15 831.28,-798.62 838.07,-789.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"841.02,-791.41 844.19,-781.3 835.41,-787.23 841.02,-791.41\"/>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>32</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M790,-677C790,-677 640,-677 640,-677 634,-677 628,-671 628,-665 628,-665 628,-621 628,-621 628,-615 634,-609 640,-609 640,-609 790,-609 790,-609 796,-609 802,-615 802,-621 802,-621 802,-665 802,-665 802,-671 796,-677 790,-677\"/>\n",
       "<text text-anchor=\"start\" x=\"641\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_1 â‰¤ 152977.199</text>\n",
       "<text text-anchor=\"start\" x=\"636\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.225</text>\n",
       "<text text-anchor=\"start\" x=\"666.5\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 16</text>\n",
       "<text text-anchor=\"start\" x=\"665\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.054</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;32 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>31&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M819,-712.88C804.54,-703.3 788.62,-692.76 773.68,-682.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"775.28,-679.73 765.01,-677.12 771.42,-685.56 775.28,-679.73\"/>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>37</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M982,-669.5C982,-669.5 832,-669.5 832,-669.5 826,-669.5 820,-663.5 820,-657.5 820,-657.5 820,-628.5 820,-628.5 820,-622.5 826,-616.5 832,-616.5 832,-616.5 982,-616.5 982,-616.5 988,-616.5 994,-622.5 994,-628.5 994,-628.5 994,-657.5 994,-657.5 994,-663.5 988,-669.5 982,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"828\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.335</text>\n",
       "<text text-anchor=\"start\" x=\"863\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"857\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.518</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;37 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>31&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881.34,-712.88C885.35,-702.11 889.82,-690.11 893.89,-679.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"897.28,-680.11 897.49,-669.52 890.72,-677.67 897.28,-680.11\"/>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>33</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M694,-565.5C694,-565.5 544,-565.5 544,-565.5 538,-565.5 532,-559.5 532,-553.5 532,-553.5 532,-524.5 532,-524.5 532,-518.5 538,-512.5 544,-512.5 544,-512.5 694,-512.5 694,-512.5 700,-512.5 706,-518.5 706,-524.5 706,-524.5 706,-553.5 706,-553.5 706,-559.5 700,-565.5 694,-565.5\"/>\n",
       "<text text-anchor=\"start\" x=\"540\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.009</text>\n",
       "<text text-anchor=\"start\" x=\"575\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"569\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.354</text>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;33 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>32&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M683.83,-608.88C672.96,-597.34 660.78,-584.39 649.93,-572.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"652.42,-570.4 643.02,-565.52 647.32,-575.2 652.42,-570.4\"/>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>34</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M886,-573C886,-573 736,-573 736,-573 730,-573 724,-567 724,-561 724,-561 724,-517 724,-517 724,-511 730,-505 736,-505 736,-505 886,-505 886,-505 892,-505 898,-511 898,-517 898,-517 898,-561 898,-561 898,-567 892,-573 886,-573\"/>\n",
       "<text text-anchor=\"start\" x=\"739\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 978.125</text>\n",
       "<text text-anchor=\"start\" x=\"732\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.135</text>\n",
       "<text text-anchor=\"start\" x=\"762.5\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\n",
       "<text text-anchor=\"start\" x=\"761\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.216</text>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;34 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>32&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M746.17,-608.88C754.64,-599.89 763.9,-590.04 772.71,-580.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"775.35,-582.98 779.66,-573.3 770.26,-578.18 775.35,-582.98\"/>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>35</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M790,-461.5C790,-461.5 640,-461.5 640,-461.5 634,-461.5 628,-455.5 628,-449.5 628,-449.5 628,-420.5 628,-420.5 628,-414.5 634,-408.5 640,-408.5 640,-408.5 790,-408.5 790,-408.5 796,-408.5 802,-414.5 802,-420.5 802,-420.5 802,-449.5 802,-449.5 802,-455.5 796,-461.5 790,-461.5\"/>\n",
       "<text text-anchor=\"start\" x=\"636\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.019</text>\n",
       "<text text-anchor=\"start\" x=\"671\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 7</text>\n",
       "<text text-anchor=\"start\" x=\"665\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.499</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;35 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>34&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M779.83,-504.88C768.96,-493.34 756.78,-480.39 745.93,-468.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"748.42,-466.4 739.02,-461.52 743.32,-471.2 748.42,-466.4\"/>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>36</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M982,-461.5C982,-461.5 832,-461.5 832,-461.5 826,-461.5 820,-455.5 820,-449.5 820,-449.5 820,-420.5 820,-420.5 820,-414.5 826,-408.5 832,-408.5 832,-408.5 982,-408.5 982,-408.5 988,-408.5 994,-414.5 994,-420.5 994,-420.5 994,-449.5 994,-449.5 994,-455.5 988,-461.5 982,-461.5\"/>\n",
       "<text text-anchor=\"start\" x=\"828\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.068</text>\n",
       "<text text-anchor=\"start\" x=\"863\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"857\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.885</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M842.17,-504.88C853.04,-493.34 865.22,-480.39 876.07,-468.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"878.68,-471.2 882.98,-461.52 873.58,-466.4 878.68,-471.2\"/>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>40</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M1057,-981.5C1057,-981.5 925,-981.5 925,-981.5 919,-981.5 913,-975.5 913,-969.5 913,-969.5 913,-940.5 913,-940.5 913,-934.5 919,-928.5 925,-928.5 925,-928.5 1057,-928.5 1057,-928.5 1063,-928.5 1069,-934.5 1069,-940.5 1069,-940.5 1069,-969.5 1069,-969.5 1069,-975.5 1063,-981.5 1057,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"921\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"947\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"936.5\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 12.266</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;40 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>39&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1008.56,-1024.88C1005.84,-1014.22 1002.82,-1002.35 1000.05,-991.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1003.37,-990.35 997.51,-981.52 996.58,-992.07 1003.37,-990.35\"/>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>41</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1249,-989C1249,-989 1099,-989 1099,-989 1093,-989 1087,-983 1087,-977 1087,-977 1087,-933 1087,-933 1087,-927 1093,-921 1099,-921 1099,-921 1249,-921 1249,-921 1255,-921 1261,-927 1261,-933 1261,-933 1261,-977 1261,-977 1261,-983 1255,-989 1249,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"1100\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_3 â‰¤ 251816.305</text>\n",
       "<text text-anchor=\"start\" x=\"1095\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.058</text>\n",
       "<text text-anchor=\"start\" x=\"1121\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 269</text>\n",
       "<text text-anchor=\"start\" x=\"1124\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.829</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;41 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>39&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1067.97,-1024.88C1082.86,-1015.21 1099.25,-1004.56 1114.61,-994.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1116.54,-997.51 1123.01,-989.12 1112.72,-991.64 1116.54,-997.51\"/>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>42</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1231,-885C1231,-885 1081,-885 1081,-885 1075,-885 1069,-879 1069,-873 1069,-873 1069,-829 1069,-829 1069,-823 1075,-817 1081,-817 1081,-817 1231,-817 1231,-817 1237,-817 1243,-823 1243,-829 1243,-829 1243,-873 1243,-873 1243,-879 1237,-885 1231,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"1082\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_2 â‰¤ 113190.625</text>\n",
       "<text text-anchor=\"start\" x=\"1077\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.178</text>\n",
       "<text text-anchor=\"start\" x=\"1103\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 189</text>\n",
       "<text text-anchor=\"start\" x=\"1106\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.052</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;42 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>41&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1168.16,-920.88C1166.71,-912.69 1165.14,-903.79 1163.62,-895.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1167.06,-894.54 1161.88,-885.3 1160.17,-895.76 1167.06,-894.54\"/>\n",
       "</g>\n",
       "<!-- 59 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>59</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1423,-877.5C1423,-877.5 1273,-877.5 1273,-877.5 1267,-877.5 1261,-871.5 1261,-865.5 1261,-865.5 1261,-836.5 1261,-836.5 1261,-830.5 1267,-824.5 1273,-824.5 1273,-824.5 1423,-824.5 1423,-824.5 1429,-824.5 1435,-830.5 1435,-836.5 1435,-836.5 1435,-865.5 1435,-865.5 1435,-871.5 1429,-877.5 1423,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1269\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.379</text>\n",
       "<text text-anchor=\"start\" x=\"1299.5\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n",
       "<text text-anchor=\"start\" x=\"1298\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.301</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;59 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>41&#45;&gt;59</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1230.49,-920.88C1251.41,-908.62 1275.02,-894.78 1295.57,-882.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1297.61,-885.6 1304.47,-877.52 1294.07,-879.56 1297.61,-885.6\"/>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>43</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1203,-773.5C1203,-773.5 1053,-773.5 1053,-773.5 1047,-773.5 1041,-767.5 1041,-761.5 1041,-761.5 1041,-732.5 1041,-732.5 1041,-726.5 1047,-720.5 1053,-720.5 1053,-720.5 1203,-720.5 1203,-720.5 1209,-720.5 1215,-726.5 1215,-732.5 1215,-732.5 1215,-761.5 1215,-761.5 1215,-767.5 1209,-773.5 1203,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1049\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.713</text>\n",
       "<text text-anchor=\"start\" x=\"1079.5\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n",
       "<text text-anchor=\"start\" x=\"1078\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.273</text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;43 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>42&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1146.91,-816.88C1143.95,-806.11 1140.66,-794.11 1137.66,-783.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1141.03,-782.24 1135.01,-773.52 1134.28,-784.09 1141.03,-782.24\"/>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>44</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1395,-781C1395,-781 1245,-781 1245,-781 1239,-781 1233,-775 1233,-769 1233,-769 1233,-725 1233,-725 1233,-719 1239,-713 1245,-713 1245,-713 1395,-713 1395,-713 1401,-713 1407,-719 1407,-725 1407,-725 1407,-769 1407,-769 1407,-775 1401,-781 1395,-781\"/>\n",
       "<text text-anchor=\"start\" x=\"1256\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar â‰¤ 5000.15</text>\n",
       "<text text-anchor=\"start\" x=\"1241\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.906</text>\n",
       "<text text-anchor=\"start\" x=\"1267\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 179</text>\n",
       "<text text-anchor=\"start\" x=\"1270\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.152</text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;44 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>42&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1209.25,-816.88C1224.79,-807.21 1241.92,-796.56 1257.96,-786.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1260.1,-789.38 1266.74,-781.12 1256.4,-783.43 1260.1,-789.38\"/>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>45</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1231,-669.5C1231,-669.5 1081,-669.5 1081,-669.5 1075,-669.5 1069,-663.5 1069,-657.5 1069,-657.5 1069,-628.5 1069,-628.5 1069,-622.5 1075,-616.5 1081,-616.5 1081,-616.5 1231,-616.5 1231,-616.5 1237,-616.5 1243,-622.5 1243,-628.5 1243,-628.5 1243,-657.5 1243,-657.5 1243,-663.5 1237,-669.5 1231,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1077\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.935</text>\n",
       "<text text-anchor=\"start\" x=\"1103\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"1106\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.559</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;45 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>44&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1266.75,-712.88C1247.13,-700.68 1224.98,-686.9 1205.68,-674.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1207.37,-671.83 1197.03,-669.52 1203.68,-677.77 1207.37,-671.83\"/>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>46</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1423,-677C1423,-677 1273,-677 1273,-677 1267,-677 1261,-671 1261,-665 1261,-665 1261,-621 1261,-621 1261,-615 1267,-609 1273,-609 1273,-609 1423,-609 1423,-609 1429,-609 1435,-615 1435,-621 1435,-621 1435,-665 1435,-665 1435,-671 1429,-677 1423,-677\"/>\n",
       "<text text-anchor=\"start\" x=\"1281.5\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ 143980.703</text>\n",
       "<text text-anchor=\"start\" x=\"1269\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.394</text>\n",
       "<text text-anchor=\"start\" x=\"1299.5\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 79</text>\n",
       "<text text-anchor=\"start\" x=\"1298\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.636</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;46 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>44&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1329.09,-712.88C1331.34,-704.69 1333.78,-695.79 1336.14,-687.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1339.59,-687.87 1338.86,-677.3 1332.84,-686.02 1339.59,-687.87\"/>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>47</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1337,-565.5C1337,-565.5 1187,-565.5 1187,-565.5 1181,-565.5 1175,-559.5 1175,-553.5 1175,-553.5 1175,-524.5 1175,-524.5 1175,-518.5 1181,-512.5 1187,-512.5 1187,-512.5 1337,-512.5 1337,-512.5 1343,-512.5 1349,-518.5 1349,-524.5 1349,-524.5 1349,-553.5 1349,-553.5 1349,-559.5 1343,-565.5 1337,-565.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1183\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.162</text>\n",
       "<text text-anchor=\"start\" x=\"1218\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"1212\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.536</text>\n",
       "</g>\n",
       "<!-- 46&#45;&gt;47 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>46&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1320.08,-608.88C1310.43,-597.45 1299.63,-584.63 1289.98,-573.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1292.64,-570.91 1283.52,-565.52 1287.29,-575.42 1292.64,-570.91\"/>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>48</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1529,-573C1529,-573 1379,-573 1379,-573 1373,-573 1367,-567 1367,-561 1367,-561 1367,-517 1367,-517 1367,-511 1373,-505 1379,-505 1379,-505 1529,-505 1529,-505 1535,-505 1541,-511 1541,-517 1541,-517 1541,-561 1541,-561 1541,-567 1535,-573 1529,-573\"/>\n",
       "<text text-anchor=\"start\" x=\"1389\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_1 â‰¤ 146176.0</text>\n",
       "<text text-anchor=\"start\" x=\"1375\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.338</text>\n",
       "<text text-anchor=\"start\" x=\"1405.5\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 75</text>\n",
       "<text text-anchor=\"start\" x=\"1404\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.695</text>\n",
       "</g>\n",
       "<!-- 46&#45;&gt;48 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>46&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1382.42,-608.88C1391.86,-599.8 1402.2,-589.85 1412.02,-580.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1414.62,-582.76 1419.39,-573.3 1409.76,-577.71 1414.62,-582.76\"/>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>49</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1426,-461.5C1426,-461.5 1294,-461.5 1294,-461.5 1288,-461.5 1282,-455.5 1282,-449.5 1282,-449.5 1282,-420.5 1282,-420.5 1282,-414.5 1288,-408.5 1294,-408.5 1294,-408.5 1426,-408.5 1426,-408.5 1432,-408.5 1438,-414.5 1438,-420.5 1438,-420.5 1438,-449.5 1438,-449.5 1438,-455.5 1432,-461.5 1426,-461.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1290\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"1316\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"1310\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.477</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;49 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>48&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1423.48,-504.88C1412.94,-493.45 1401.13,-480.63 1390.59,-469.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1392.87,-466.5 1383.52,-461.52 1387.72,-471.25 1392.87,-466.5\"/>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>50</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1626,-469C1626,-469 1468,-469 1468,-469 1462,-469 1456,-463 1456,-457 1456,-457 1456,-413 1456,-413 1456,-407 1462,-401 1468,-401 1468,-401 1626,-401 1626,-401 1632,-401 1638,-407 1638,-413 1638,-413 1638,-457 1638,-457 1638,-463 1632,-469 1626,-469\"/>\n",
       "<text text-anchor=\"start\" x=\"1464\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime â‰¤ 1088500.0</text>\n",
       "<text text-anchor=\"start\" x=\"1468\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.237</text>\n",
       "<text text-anchor=\"start\" x=\"1498.5\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n",
       "<text text-anchor=\"start\" x=\"1497\" y=\"-408.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.657</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;50 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>48&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1484.19,-504.88C1492.32,-495.98 1501.2,-486.24 1509.66,-476.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1512.49,-479.05 1516.64,-469.3 1507.31,-474.33 1512.49,-479.05\"/>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>51</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1519,-357.5C1519,-357.5 1387,-357.5 1387,-357.5 1381,-357.5 1375,-351.5 1375,-345.5 1375,-345.5 1375,-316.5 1375,-316.5 1375,-310.5 1381,-304.5 1387,-304.5 1387,-304.5 1519,-304.5 1519,-304.5 1525,-304.5 1531,-310.5 1531,-316.5 1531,-316.5 1531,-345.5 1531,-345.5 1531,-351.5 1525,-357.5 1519,-357.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1383\" y=\"-342.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"1409\" y=\"-327.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"1403\" y=\"-312.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.647</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;51 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>50&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1516.48,-400.88C1505.94,-389.45 1494.13,-376.63 1483.59,-365.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1485.87,-362.5 1476.52,-357.52 1480.72,-367.25 1485.87,-362.5\"/>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>52</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1719,-365C1719,-365 1561,-365 1561,-365 1555,-365 1549,-359 1549,-353 1549,-353 1549,-309 1549,-309 1549,-303 1555,-297 1561,-297 1561,-297 1719,-297 1719,-297 1725,-297 1731,-303 1731,-309 1731,-309 1731,-353 1731,-353 1731,-359 1725,-365 1719,-365\"/>\n",
       "<text text-anchor=\"start\" x=\"1557\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime â‰¤ 7256500.0</text>\n",
       "<text text-anchor=\"start\" x=\"1561\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.185</text>\n",
       "<text text-anchor=\"start\" x=\"1591.5\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 73</text>\n",
       "<text text-anchor=\"start\" x=\"1594.5\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.63</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1577.19,-400.88C1585.32,-391.98 1594.2,-382.24 1602.66,-372.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1605.49,-375.05 1609.64,-365.3 1600.31,-370.33 1605.49,-375.05\"/>\n",
       "</g>\n",
       "<!-- 53 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>53</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1630,-261C1630,-261 1436,-261 1436,-261 1430,-261 1424,-255 1424,-249 1424,-249 1424,-205 1424,-205 1424,-199 1430,-193 1436,-193 1436,-193 1630,-193 1630,-193 1636,-193 1642,-199 1642,-205 1642,-205 1642,-249 1642,-249 1642,-255 1636,-261 1630,-261\"/>\n",
       "<text text-anchor=\"start\" x=\"1432\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_1 â‰¤ 1162696.0</text>\n",
       "<text text-anchor=\"start\" x=\"1458.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.18</text>\n",
       "<text text-anchor=\"start\" x=\"1484.5\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 55</text>\n",
       "<text text-anchor=\"start\" x=\"1483\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.735</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;53 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>52&#45;&gt;53</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1605.26,-296.88C1595.73,-287.8 1585.29,-277.85 1575.38,-268.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1577.58,-265.67 1567.93,-261.3 1572.75,-270.73 1577.58,-265.67\"/>\n",
       "</g>\n",
       "<!-- 58 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>58</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1822,-253.5C1822,-253.5 1672,-253.5 1672,-253.5 1666,-253.5 1660,-247.5 1660,-241.5 1660,-241.5 1660,-212.5 1660,-212.5 1660,-206.5 1666,-200.5 1672,-200.5 1672,-200.5 1822,-200.5 1822,-200.5 1828,-200.5 1834,-206.5 1834,-212.5 1834,-212.5 1834,-241.5 1834,-241.5 1834,-247.5 1828,-253.5 1822,-253.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1668\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.063</text>\n",
       "<text text-anchor=\"start\" x=\"1698.5\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 18</text>\n",
       "<text text-anchor=\"start\" x=\"1697\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.309</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;58 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>52&#45;&gt;58</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1674.74,-296.88C1686.97,-285.23 1700.7,-272.14 1712.87,-260.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1715.41,-262.95 1720.23,-253.52 1710.58,-257.89 1715.41,-262.95\"/>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>54</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1503.5,-149.5C1503.5,-149.5 1362.5,-149.5 1362.5,-149.5 1356.5,-149.5 1350.5,-143.5 1350.5,-137.5 1350.5,-137.5 1350.5,-108.5 1350.5,-108.5 1350.5,-102.5 1356.5,-96.5 1362.5,-96.5 1362.5,-96.5 1503.5,-96.5 1503.5,-96.5 1509.5,-96.5 1515.5,-102.5 1515.5,-108.5 1515.5,-108.5 1515.5,-137.5 1515.5,-137.5 1515.5,-143.5 1509.5,-149.5 1503.5,-149.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1358.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.11</text>\n",
       "<text text-anchor=\"start\" x=\"1389\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"1383\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.179</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;54 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>53&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1500.53,-192.88C1489.21,-181.34 1476.52,-168.39 1465.22,-156.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1467.52,-154.21 1458.02,-149.52 1462.52,-159.11 1467.52,-154.21\"/>\n",
       "</g>\n",
       "<!-- 55 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>55</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1722.5,-157C1722.5,-157 1545.5,-157 1545.5,-157 1539.5,-157 1533.5,-151 1533.5,-145 1533.5,-145 1533.5,-101 1533.5,-101 1533.5,-95 1539.5,-89 1545.5,-89 1545.5,-89 1722.5,-89 1722.5,-89 1728.5,-89 1734.5,-95 1734.5,-101 1734.5,-101 1734.5,-145 1734.5,-145 1734.5,-151 1728.5,-157 1722.5,-157\"/>\n",
       "<text text-anchor=\"start\" x=\"1541.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesAcked_3 â‰¤ 539634.0</text>\n",
       "<text text-anchor=\"start\" x=\"1555\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.121</text>\n",
       "<text text-anchor=\"start\" x=\"1585.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"start\" x=\"1584\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.844</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;55 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>53&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1565.79,-192.88C1574.7,-183.89 1584.45,-174.04 1593.72,-164.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1596.48,-166.87 1601.03,-157.3 1591.5,-161.94 1596.48,-166.87\"/>\n",
       "</g>\n",
       "<!-- 56 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>56</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1610.5,-53C1610.5,-53 1469.5,-53 1469.5,-53 1463.5,-53 1457.5,-47 1457.5,-41 1457.5,-41 1457.5,-12 1457.5,-12 1457.5,-6 1463.5,0 1469.5,0 1469.5,0 1610.5,0 1610.5,0 1616.5,0 1622.5,-6 1622.5,-12 1622.5,-12 1622.5,-41 1622.5,-41 1622.5,-47 1616.5,-53 1610.5,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"1465.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.09</text>\n",
       "<text text-anchor=\"start\" x=\"1496\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"1499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.7</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;56 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>55&#45;&gt;56</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1601.12,-88.95C1591.93,-79.71 1581.95,-69.67 1572.76,-60.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1575.13,-57.86 1565.6,-53.24 1570.17,-62.79 1575.13,-57.86\"/>\n",
       "</g>\n",
       "<!-- 57 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>57</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1803,-53C1803,-53 1653,-53 1653,-53 1647,-53 1641,-47 1641,-41 1641,-41 1641,-12 1641,-12 1641,-6 1647,0 1653,0 1653,0 1803,0 1803,0 1809,0 1815,-6 1815,-12 1815,-12 1815,-41 1815,-41 1815,-47 1809,-53 1803,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"1649\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.069</text>\n",
       "<text text-anchor=\"start\" x=\"1679.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n",
       "<text text-anchor=\"start\" x=\"1678\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.784</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;57 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>55&#45;&gt;57</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1666.88,-88.95C1676.07,-79.71 1686.05,-69.67 1695.24,-60.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1697.83,-62.79 1702.4,-53.24 1692.87,-57.86 1697.83,-62.79\"/>\n",
       "</g>\n",
       "<!-- 61 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>61</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1441,-1093C1441,-1093 1291,-1093 1291,-1093 1285,-1093 1279,-1087 1279,-1081 1279,-1081 1279,-1037 1279,-1037 1279,-1031 1285,-1025 1291,-1025 1291,-1025 1441,-1025 1441,-1025 1447,-1025 1453,-1031 1453,-1037 1453,-1037 1453,-1081 1453,-1081 1453,-1087 1447,-1093 1441,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"1298.5\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 1184.2</text>\n",
       "<text text-anchor=\"start\" x=\"1287\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.227</text>\n",
       "<text text-anchor=\"start\" x=\"1313\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 659</text>\n",
       "<text text-anchor=\"start\" x=\"1316\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.974</text>\n",
       "</g>\n",
       "<!-- 60&#45;&gt;61 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>60&#45;&gt;61</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1366,-1128.88C1366,-1120.78 1366,-1111.98 1366,-1103.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1369.5,-1103.3 1366,-1093.3 1362.5,-1103.3 1369.5,-1103.3\"/>\n",
       "</g>\n",
       "<!-- 72 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>72</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1633,-1085.5C1633,-1085.5 1483,-1085.5 1483,-1085.5 1477,-1085.5 1471,-1079.5 1471,-1073.5 1471,-1073.5 1471,-1044.5 1471,-1044.5 1471,-1038.5 1477,-1032.5 1483,-1032.5 1483,-1032.5 1633,-1032.5 1633,-1032.5 1639,-1032.5 1645,-1038.5 1645,-1044.5 1645,-1044.5 1645,-1073.5 1645,-1073.5 1645,-1079.5 1639,-1085.5 1633,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1479\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.534</text>\n",
       "<text text-anchor=\"start\" x=\"1514\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"1508\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.585</text>\n",
       "</g>\n",
       "<!-- 60&#45;&gt;72 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>60&#45;&gt;72</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1428.34,-1128.88C1451.73,-1116.46 1478.18,-1102.41 1501.06,-1090.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1502.77,-1093.3 1509.96,-1085.52 1499.49,-1087.12 1502.77,-1093.3\"/>\n",
       "</g>\n",
       "<!-- 62 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>62</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1441,-981.5C1441,-981.5 1291,-981.5 1291,-981.5 1285,-981.5 1279,-975.5 1279,-969.5 1279,-969.5 1279,-940.5 1279,-940.5 1279,-934.5 1285,-928.5 1291,-928.5 1291,-928.5 1441,-928.5 1441,-928.5 1447,-928.5 1453,-934.5 1453,-940.5 1453,-940.5 1453,-969.5 1453,-969.5 1453,-975.5 1447,-981.5 1441,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1287\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.369</text>\n",
       "<text text-anchor=\"start\" x=\"1317.5\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\n",
       "<text text-anchor=\"start\" x=\"1316\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.045</text>\n",
       "</g>\n",
       "<!-- 61&#45;&gt;62 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>61&#45;&gt;62</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1366,-1024.88C1366,-1014.33 1366,-1002.6 1366,-991.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1369.5,-991.52 1366,-981.52 1362.5,-991.52 1369.5,-991.52\"/>\n",
       "</g>\n",
       "<!-- 63 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>63</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1633,-989C1633,-989 1483,-989 1483,-989 1477,-989 1471,-983 1471,-977 1471,-977 1471,-933 1471,-933 1471,-927 1477,-921 1483,-921 1483,-921 1633,-921 1633,-921 1639,-921 1645,-927 1645,-933 1645,-933 1645,-977 1645,-977 1645,-983 1639,-989 1633,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"1486\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 10252.9</text>\n",
       "<text text-anchor=\"start\" x=\"1479\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.173</text>\n",
       "<text text-anchor=\"start\" x=\"1505\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 644</text>\n",
       "<text text-anchor=\"start\" x=\"1508\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.949</text>\n",
       "</g>\n",
       "<!-- 61&#45;&gt;63 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>61&#45;&gt;63</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1428.34,-1024.88C1446.96,-1014.99 1467.52,-1004.07 1486.67,-993.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1488.46,-996.91 1495.65,-989.12 1485.17,-990.73 1488.46,-996.91\"/>\n",
       "</g>\n",
       "<!-- 64 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>64</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1624,-877.5C1624,-877.5 1474,-877.5 1474,-877.5 1468,-877.5 1462,-871.5 1462,-865.5 1462,-865.5 1462,-836.5 1462,-836.5 1462,-830.5 1468,-824.5 1474,-824.5 1474,-824.5 1624,-824.5 1624,-824.5 1630,-824.5 1636,-830.5 1636,-836.5 1636,-836.5 1636,-865.5 1636,-865.5 1636,-871.5 1630,-877.5 1624,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1470\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.158</text>\n",
       "<text text-anchor=\"start\" x=\"1496\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 317</text>\n",
       "<text text-anchor=\"start\" x=\"1499\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.793</text>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;64 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>63&#45;&gt;64</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1555.08,-920.88C1554.14,-910.22 1553.09,-898.35 1552.13,-887.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1555.62,-887.17 1551.25,-877.52 1548.64,-887.79 1555.62,-887.17\"/>\n",
       "</g>\n",
       "<!-- 65 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>65</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1807.5,-885C1807.5,-885 1666.5,-885 1666.5,-885 1660.5,-885 1654.5,-879 1654.5,-873 1654.5,-873 1654.5,-829 1654.5,-829 1654.5,-823 1660.5,-817 1666.5,-817 1666.5,-817 1807.5,-817 1807.5,-817 1813.5,-817 1819.5,-823 1819.5,-829 1819.5,-829 1819.5,-873 1819.5,-873 1819.5,-879 1813.5,-885 1807.5,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"1665\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_3 â‰¤ 60789.0</text>\n",
       "<text text-anchor=\"start\" x=\"1662.5\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.14</text>\n",
       "<text text-anchor=\"start\" x=\"1684\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 327</text>\n",
       "<text text-anchor=\"start\" x=\"1696\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.1</text>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;65 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>63&#45;&gt;65</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1616.12,-920.88C1633.24,-911.12 1652.12,-900.37 1669.77,-890.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1671.91,-893.12 1678.87,-885.12 1668.45,-887.03 1671.91,-893.12\"/>\n",
       "</g>\n",
       "<!-- 66 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>66</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1655.5,-781C1655.5,-781 1488.5,-781 1488.5,-781 1482.5,-781 1476.5,-775 1476.5,-769 1476.5,-769 1476.5,-725 1476.5,-725 1476.5,-719 1482.5,-713 1488.5,-713 1488.5,-713 1655.5,-713 1655.5,-713 1661.5,-713 1667.5,-719 1667.5,-725 1667.5,-725 1667.5,-769 1667.5,-769 1667.5,-775 1661.5,-781 1655.5,-781\"/>\n",
       "<text text-anchor=\"start\" x=\"1484.5\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent_3 â‰¤ 985636.0</text>\n",
       "<text text-anchor=\"start\" x=\"1493\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.117</text>\n",
       "<text text-anchor=\"start\" x=\"1519\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 319</text>\n",
       "<text text-anchor=\"start\" x=\"1522\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.125</text>\n",
       "</g>\n",
       "<!-- 65&#45;&gt;66 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>65&#45;&gt;66</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1683.43,-816.88C1667.79,-807.21 1650.56,-796.56 1634.42,-786.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1635.93,-783.41 1625.58,-781.12 1632.25,-789.36 1635.93,-783.41\"/>\n",
       "</g>\n",
       "<!-- 71 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>71</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1848,-773.5C1848,-773.5 1698,-773.5 1698,-773.5 1692,-773.5 1686,-767.5 1686,-761.5 1686,-761.5 1686,-732.5 1686,-732.5 1686,-726.5 1692,-720.5 1698,-720.5 1698,-720.5 1848,-720.5 1848,-720.5 1854,-720.5 1860,-726.5 1860,-732.5 1860,-732.5 1860,-761.5 1860,-761.5 1860,-767.5 1854,-773.5 1848,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1694\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.063</text>\n",
       "<text text-anchor=\"start\" x=\"1729\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n",
       "<text text-anchor=\"start\" x=\"1723\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.107</text>\n",
       "</g>\n",
       "<!-- 65&#45;&gt;71 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>65&#45;&gt;71</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1748.69,-816.88C1752.49,-806.11 1756.73,-794.11 1760.58,-783.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1763.97,-784.11 1763.99,-773.52 1757.36,-781.78 1763.97,-784.11\"/>\n",
       "</g>\n",
       "<!-- 67 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>67</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1619,-669.5C1619,-669.5 1469,-669.5 1469,-669.5 1463,-669.5 1457,-663.5 1457,-657.5 1457,-657.5 1457,-628.5 1457,-628.5 1457,-622.5 1463,-616.5 1469,-616.5 1469,-616.5 1619,-616.5 1619,-616.5 1625,-616.5 1631,-622.5 1631,-628.5 1631,-628.5 1631,-657.5 1631,-657.5 1631,-663.5 1625,-669.5 1619,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1465\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.035</text>\n",
       "<text text-anchor=\"start\" x=\"1495.5\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"1494\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.692</text>\n",
       "</g>\n",
       "<!-- 66&#45;&gt;67 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>66&#45;&gt;67</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1562.91,-712.88C1559.95,-702.11 1556.66,-690.11 1553.66,-679.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1557.03,-678.24 1551.01,-669.52 1550.28,-680.09 1557.03,-678.24\"/>\n",
       "</g>\n",
       "<!-- 68 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>68</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1850.5,-677C1850.5,-677 1661.5,-677 1661.5,-677 1655.5,-677 1649.5,-671 1649.5,-665 1649.5,-665 1649.5,-621 1649.5,-621 1649.5,-615 1655.5,-609 1661.5,-609 1661.5,-609 1850.5,-609 1850.5,-609 1856.5,-609 1862.5,-615 1862.5,-621 1862.5,-621 1862.5,-665 1862.5,-665 1862.5,-671 1856.5,-677 1850.5,-677\"/>\n",
       "<text text-anchor=\"start\" x=\"1657.5\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesRetrans_1 â‰¤ 172996.0</text>\n",
       "<text text-anchor=\"start\" x=\"1677\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.106</text>\n",
       "<text text-anchor=\"start\" x=\"1703\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 294</text>\n",
       "<text text-anchor=\"start\" x=\"1706\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.162</text>\n",
       "</g>\n",
       "<!-- 66&#45;&gt;68 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>66&#45;&gt;68</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1631.74,-712.88C1649.5,-703.04 1669.11,-692.17 1687.39,-682.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1689.2,-685.03 1696.25,-677.12 1685.8,-678.91 1689.2,-685.03\"/>\n",
       "</g>\n",
       "<!-- 69 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>69</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1725,-565.5C1725,-565.5 1575,-565.5 1575,-565.5 1569,-565.5 1563,-559.5 1563,-553.5 1563,-553.5 1563,-524.5 1563,-524.5 1563,-518.5 1569,-512.5 1575,-512.5 1575,-512.5 1725,-512.5 1725,-512.5 1731,-512.5 1737,-518.5 1737,-524.5 1737,-524.5 1737,-553.5 1737,-553.5 1737,-559.5 1731,-565.5 1725,-565.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1571\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.099</text>\n",
       "<text text-anchor=\"start\" x=\"1597\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 262</text>\n",
       "<text text-anchor=\"start\" x=\"1600\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.201</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;69 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>68&#45;&gt;69</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1721.58,-608.88C1709.47,-597.23 1695.87,-584.14 1683.81,-572.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1686.15,-569.93 1676.52,-565.52 1681.3,-574.98 1686.15,-569.93\"/>\n",
       "</g>\n",
       "<!-- 70 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>70</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1917,-565.5C1917,-565.5 1767,-565.5 1767,-565.5 1761,-565.5 1755,-559.5 1755,-553.5 1755,-553.5 1755,-524.5 1755,-524.5 1755,-518.5 1761,-512.5 1767,-512.5 1767,-512.5 1917,-512.5 1917,-512.5 1923,-512.5 1929,-518.5 1929,-524.5 1929,-524.5 1929,-553.5 1929,-553.5 1929,-559.5 1923,-565.5 1917,-565.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1763\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.053</text>\n",
       "<text text-anchor=\"start\" x=\"1793.5\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n",
       "<text text-anchor=\"start\" x=\"1792\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.841</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;70 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>68&#45;&gt;70</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1783.92,-608.88C1793.57,-597.45 1804.37,-584.63 1814.02,-573.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1816.71,-575.42 1820.48,-565.52 1811.36,-570.91 1816.71,-575.42\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7fe92d916a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trustee_report(regression, regression=True, X=X, y=y, scaler=None, feature_names=features.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
