{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288be4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from predictor import ThroughputPredictor\n",
    "from trustee import RegressionTrustee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3442611",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b66590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE_MS = 100 # Number of milliseconds for each time window; TCP_INFO statistics are aggregated over this period\n",
    "TIME_SERIES_LENGTH = 5 # Number of time series steps to consider for prediction\n",
    "\n",
    "def load_dataset(file_path):\n",
    "  df = pd.read_csv(file_path).dropna()\n",
    "  df.sort_values(by=['TestID', 'ElapsedTime'], inplace=True)\n",
    "  \n",
    "  df['k'] = df['ElapsedTime'] // (WINDOW_SIZE_MS * 1000)\n",
    "  agg_df = df.groupby(['TestID', 'k'], as_index=False).agg({\n",
    "    'ElapsedTime': 'max',\n",
    "    'BusyTime': 'max',\n",
    "    'BytesSent': 'max',\n",
    "    'BytesAcked': 'max',\n",
    "    'BytesRetrans': 'max',\n",
    "    'RTT': 'mean',\n",
    "    'RTTVar': 'mean',\n",
    "    'MinRTT': 'min',\n",
    "    'RWndLimited': 'max',\n",
    "    'SndBufLimited': 'max',\n",
    "    'FinalSpeed': 'max',\n",
    "  }, inplace=True)\n",
    "  \n",
    "  for i in range(1, TIME_SERIES_LENGTH):\n",
    "    agg_df[f'ElapsedTime_{i}'] = agg_df.groupby('TestID')['ElapsedTime'].shift(i).fillna(0)\n",
    "    agg_df[f'BusyTime_{i}'] = agg_df.groupby('TestID')['BusyTime'].shift(i).fillna(0)\n",
    "    agg_df[f'BytesSent_{i}'] = agg_df.groupby('TestID')['BytesSent'].shift(i).fillna(0)\n",
    "    agg_df[f'BytesAcked_{i}'] = agg_df.groupby('TestID')['BytesAcked'].shift(i).fillna(0)\n",
    "    agg_df[f'BytesRetrans_{i}'] = agg_df.groupby('TestID')['BytesRetrans'].shift(i).fillna(0)\n",
    "    agg_df[f'RTT_{i}'] = agg_df.groupby('TestID')['RTT'].shift(i).fillna(0)\n",
    "    agg_df[f'RTTVar_{i}'] = agg_df.groupby('TestID')['RTTVar'].shift(i).fillna(0)\n",
    "    agg_df[f'RWndLimited_{i}'] = agg_df.groupby('TestID')['RWndLimited'].shift(i).fillna(0)\n",
    "    agg_df[f'SndBufLimited_{i}'] = agg_df.groupby('TestID')['SndBufLimited'].shift(i).fillna(0)\n",
    "    \n",
    "  agg_df = agg_df.groupby('TestID').apply(lambda x: x.iloc[TIME_SERIES_LENGTH:]).reset_index(drop=True)\n",
    "  agg_df.drop(columns=['TestID'], inplace=True)\n",
    "  labels = agg_df.pop('FinalSpeed')\n",
    "  \n",
    "  return agg_df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522fd1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>ElapsedTime</th>\n",
       "      <th>BusyTime</th>\n",
       "      <th>BytesSent</th>\n",
       "      <th>BytesAcked</th>\n",
       "      <th>BytesRetrans</th>\n",
       "      <th>RTT</th>\n",
       "      <th>RTTVar</th>\n",
       "      <th>MinRTT</th>\n",
       "      <th>RWndLimited</th>\n",
       "      <th>...</th>\n",
       "      <th>SndBufLimited_3</th>\n",
       "      <th>ElapsedTime_4</th>\n",
       "      <th>BusyTime_4</th>\n",
       "      <th>BytesSent_4</th>\n",
       "      <th>BytesAcked_4</th>\n",
       "      <th>BytesRetrans_4</th>\n",
       "      <th>RTT_4</th>\n",
       "      <th>RTTVar_4</th>\n",
       "      <th>RWndLimited_4</th>\n",
       "      <th>SndBufLimited_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>579234</td>\n",
       "      <td>575000</td>\n",
       "      <td>212785</td>\n",
       "      <td>93673</td>\n",
       "      <td>1418</td>\n",
       "      <td>121797.25</td>\n",
       "      <td>9535.5</td>\n",
       "      <td>100816</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124037.0</td>\n",
       "      <td>119000.0</td>\n",
       "      <td>1.284700e+04</td>\n",
       "      <td>1.547000e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101050.00</td>\n",
       "      <td>28799.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>682398</td>\n",
       "      <td>678000</td>\n",
       "      <td>248235</td>\n",
       "      <td>93673</td>\n",
       "      <td>2836</td>\n",
       "      <td>163404.60</td>\n",
       "      <td>16453.2</td>\n",
       "      <td>100816</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288933.0</td>\n",
       "      <td>284000.0</td>\n",
       "      <td>4.120700e+04</td>\n",
       "      <td>1.284700e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106504.25</td>\n",
       "      <td>16665.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>786233</td>\n",
       "      <td>782000</td>\n",
       "      <td>279431</td>\n",
       "      <td>93673</td>\n",
       "      <td>5672</td>\n",
       "      <td>221776.00</td>\n",
       "      <td>23703.8</td>\n",
       "      <td>100816</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>392226.0</td>\n",
       "      <td>388000.0</td>\n",
       "      <td>8.091100e+04</td>\n",
       "      <td>4.120700e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109021.75</td>\n",
       "      <td>6359.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>891832</td>\n",
       "      <td>887000</td>\n",
       "      <td>336151</td>\n",
       "      <td>133377</td>\n",
       "      <td>8508</td>\n",
       "      <td>274418.60</td>\n",
       "      <td>9798.4</td>\n",
       "      <td>100816</td>\n",
       "      <td>15000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496415.0</td>\n",
       "      <td>492000.0</td>\n",
       "      <td>1.603190e+05</td>\n",
       "      <td>7.240300e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109827.20</td>\n",
       "      <td>4563.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>998020</td>\n",
       "      <td>993000</td>\n",
       "      <td>402797</td>\n",
       "      <td>205695</td>\n",
       "      <td>12762</td>\n",
       "      <td>216210.20</td>\n",
       "      <td>11117.8</td>\n",
       "      <td>100816</td>\n",
       "      <td>74000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579234.0</td>\n",
       "      <td>575000.0</td>\n",
       "      <td>2.127850e+05</td>\n",
       "      <td>9.367300e+04</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>121797.25</td>\n",
       "      <td>9535.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>92</td>\n",
       "      <td>9293469</td>\n",
       "      <td>9293000</td>\n",
       "      <td>1412073970</td>\n",
       "      <td>1304071780</td>\n",
       "      <td>106774202</td>\n",
       "      <td>1891.60</td>\n",
       "      <td>296.6</td>\n",
       "      <td>161</td>\n",
       "      <td>4146000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8787459.0</td>\n",
       "      <td>8788000.0</td>\n",
       "      <td>1.312769e+09</td>\n",
       "      <td>1.214030e+09</td>\n",
       "      <td>98098878.0</td>\n",
       "      <td>1579.80</td>\n",
       "      <td>218.40</td>\n",
       "      <td>3952000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>93</td>\n",
       "      <td>9314891</td>\n",
       "      <td>9315000</td>\n",
       "      <td>1418576562</td>\n",
       "      <td>1307920232</td>\n",
       "      <td>107103178</td>\n",
       "      <td>2422.00</td>\n",
       "      <td>843.0</td>\n",
       "      <td>161</td>\n",
       "      <td>4149000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8808776.0</td>\n",
       "      <td>8809000.0</td>\n",
       "      <td>1.316681e+09</td>\n",
       "      <td>1.214769e+09</td>\n",
       "      <td>98189630.0</td>\n",
       "      <td>2084.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>3959000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>95</td>\n",
       "      <td>9583439</td>\n",
       "      <td>9584000</td>\n",
       "      <td>1436655000</td>\n",
       "      <td>1324500906</td>\n",
       "      <td>108148244</td>\n",
       "      <td>1974.50</td>\n",
       "      <td>707.0</td>\n",
       "      <td>161</td>\n",
       "      <td>4364000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9099539.0</td>\n",
       "      <td>9100000.0</td>\n",
       "      <td>1.349074e+09</td>\n",
       "      <td>1.248059e+09</td>\n",
       "      <td>100265582.0</td>\n",
       "      <td>2280.40</td>\n",
       "      <td>344.00</td>\n",
       "      <td>4146000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>97</td>\n",
       "      <td>9789169</td>\n",
       "      <td>9789000</td>\n",
       "      <td>1447534832</td>\n",
       "      <td>1337624014</td>\n",
       "      <td>108847318</td>\n",
       "      <td>2000.50</td>\n",
       "      <td>429.0</td>\n",
       "      <td>161</td>\n",
       "      <td>4534000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9185733.0</td>\n",
       "      <td>9186000.0</td>\n",
       "      <td>1.375302e+09</td>\n",
       "      <td>1.271397e+09</td>\n",
       "      <td>103331298.0</td>\n",
       "      <td>1498.75</td>\n",
       "      <td>497.00</td>\n",
       "      <td>4146000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>98</td>\n",
       "      <td>9896221</td>\n",
       "      <td>9897000</td>\n",
       "      <td>1476319542</td>\n",
       "      <td>1361344318</td>\n",
       "      <td>111338744</td>\n",
       "      <td>1598.80</td>\n",
       "      <td>307.2</td>\n",
       "      <td>161</td>\n",
       "      <td>4556000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9293469.0</td>\n",
       "      <td>9293000.0</td>\n",
       "      <td>1.412074e+09</td>\n",
       "      <td>1.304072e+09</td>\n",
       "      <td>106774202.0</td>\n",
       "      <td>1891.60</td>\n",
       "      <td>296.60</td>\n",
       "      <td>4146000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22534 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  ElapsedTime  BusyTime   BytesSent  BytesAcked  BytesRetrans  \\\n",
       "0       5       579234    575000      212785       93673          1418   \n",
       "1       6       682398    678000      248235       93673          2836   \n",
       "2       7       786233    782000      279431       93673          5672   \n",
       "3       8       891832    887000      336151      133377          8508   \n",
       "4       9       998020    993000      402797      205695         12762   \n",
       "...    ..          ...       ...         ...         ...           ...   \n",
       "22529  92      9293469   9293000  1412073970  1304071780     106774202   \n",
       "22530  93      9314891   9315000  1418576562  1307920232     107103178   \n",
       "22531  95      9583439   9584000  1436655000  1324500906     108148244   \n",
       "22532  97      9789169   9789000  1447534832  1337624014     108847318   \n",
       "22533  98      9896221   9897000  1476319542  1361344318     111338744   \n",
       "\n",
       "             RTT   RTTVar  MinRTT  RWndLimited  ...  SndBufLimited_3  \\\n",
       "0      121797.25   9535.5  100816            0  ...              0.0   \n",
       "1      163404.60  16453.2  100816            0  ...              0.0   \n",
       "2      221776.00  23703.8  100816            0  ...              0.0   \n",
       "3      274418.60   9798.4  100816        15000  ...              0.0   \n",
       "4      216210.20  11117.8  100816        74000  ...              0.0   \n",
       "...          ...      ...     ...          ...  ...              ...   \n",
       "22529    1891.60    296.6     161      4146000  ...              0.0   \n",
       "22530    2422.00    843.0     161      4149000  ...              0.0   \n",
       "22531    1974.50    707.0     161      4364000  ...              0.0   \n",
       "22532    2000.50    429.0     161      4534000  ...              0.0   \n",
       "22533    1598.80    307.2     161      4556000  ...              0.0   \n",
       "\n",
       "       ElapsedTime_4  BusyTime_4   BytesSent_4  BytesAcked_4  BytesRetrans_4  \\\n",
       "0           124037.0    119000.0  1.284700e+04  1.547000e+03             0.0   \n",
       "1           288933.0    284000.0  4.120700e+04  1.284700e+04             0.0   \n",
       "2           392226.0    388000.0  8.091100e+04  4.120700e+04             0.0   \n",
       "3           496415.0    492000.0  1.603190e+05  7.240300e+04             0.0   \n",
       "4           579234.0    575000.0  2.127850e+05  9.367300e+04          1418.0   \n",
       "...              ...         ...           ...           ...             ...   \n",
       "22529      8787459.0   8788000.0  1.312769e+09  1.214030e+09      98098878.0   \n",
       "22530      8808776.0   8809000.0  1.316681e+09  1.214769e+09      98189630.0   \n",
       "22531      9099539.0   9100000.0  1.349074e+09  1.248059e+09     100265582.0   \n",
       "22532      9185733.0   9186000.0  1.375302e+09  1.271397e+09     103331298.0   \n",
       "22533      9293469.0   9293000.0  1.412074e+09  1.304072e+09     106774202.0   \n",
       "\n",
       "           RTT_4  RTTVar_4  RWndLimited_4  SndBufLimited_4  \n",
       "0      101050.00  28799.00            0.0              0.0  \n",
       "1      106504.25  16665.00            0.0              0.0  \n",
       "2      109021.75   6359.75            0.0              0.0  \n",
       "3      109827.20   4563.20            0.0              0.0  \n",
       "4      121797.25   9535.50            0.0              0.0  \n",
       "...          ...       ...            ...              ...  \n",
       "22529    1579.80    218.40      3952000.0              0.0  \n",
       "22530    2084.00    577.00      3959000.0              0.0  \n",
       "22531    2280.40    344.00      4146000.0              0.0  \n",
       "22532    1498.75    497.00      4146000.0              0.0  \n",
       "22533    1891.60    296.60      4146000.0              0.0  \n",
       "\n",
       "[22534 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = load_dataset('./dataset.csv')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edc9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = features.values, labels.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d788be",
   "metadata": {},
   "source": [
    "# Throughput Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962f3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 26880.89823135933\n",
      "Epoch 2/100, Loss: 3551.24853464556\n",
      "Epoch 3/100, Loss: 2392.512026022704\n",
      "Epoch 4/100, Loss: 1854.1536599695078\n",
      "Epoch 5/100, Loss: 1424.724317136691\n",
      "Epoch 6/100, Loss: 1157.5020049445286\n",
      "Epoch 7/100, Loss: 1122.3021873281646\n",
      "Epoch 8/100, Loss: 1004.7151862037109\n",
      "Epoch 9/100, Loss: 1023.2108202938134\n",
      "Epoch 10/100, Loss: 895.4241622456673\n",
      "Epoch 11/100, Loss: 868.3515651163175\n",
      "Epoch 12/100, Loss: 832.7009747168597\n",
      "Epoch 13/100, Loss: 875.9095864450713\n",
      "Epoch 14/100, Loss: 799.7051101253193\n",
      "Epoch 15/100, Loss: 779.7569359233848\n",
      "Epoch 16/100, Loss: 727.4977092385051\n",
      "Epoch 17/100, Loss: 771.2904466622741\n",
      "Epoch 18/100, Loss: 687.8364045503898\n",
      "Epoch 19/100, Loss: 690.9796876501118\n",
      "Epoch 20/100, Loss: 713.8774106879747\n",
      "Epoch 21/100, Loss: 711.6334895550842\n",
      "Epoch 22/100, Loss: 639.0602778595311\n",
      "Epoch 23/100, Loss: 670.6177671337708\n",
      "Epoch 24/100, Loss: 681.2190470347298\n",
      "Epoch 25/100, Loss: 623.4010976666611\n",
      "Epoch 26/100, Loss: 605.6962895579561\n",
      "Epoch 27/100, Loss: 611.3254481706368\n",
      "Epoch 28/100, Loss: 632.160885298228\n",
      "Epoch 29/100, Loss: 601.7118031449057\n",
      "Epoch 30/100, Loss: 591.3033661827837\n",
      "Epoch 31/100, Loss: 575.8098673880946\n",
      "Epoch 32/100, Loss: 578.8414906169048\n",
      "Epoch 33/100, Loss: 568.5953655823249\n",
      "Epoch 34/100, Loss: 580.2240369716474\n",
      "Epoch 35/100, Loss: 600.093307092272\n",
      "Epoch 36/100, Loss: 571.1998386143671\n",
      "Epoch 37/100, Loss: 559.580712267158\n",
      "Epoch 38/100, Loss: 562.171486738971\n",
      "Epoch 39/100, Loss: 545.6159351418517\n",
      "Epoch 40/100, Loss: 540.0492887204366\n",
      "Epoch 41/100, Loss: 565.5274637229061\n",
      "Epoch 42/100, Loss: 563.3439758779069\n",
      "Epoch 43/100, Loss: 561.1970038204842\n",
      "Epoch 44/100, Loss: 522.4122796498738\n",
      "Epoch 45/100, Loss: 517.5481108057088\n",
      "Epoch 46/100, Loss: 523.8631354867807\n",
      "Epoch 47/100, Loss: 516.5053741675604\n",
      "Epoch 48/100, Loss: 530.0165592366735\n",
      "Epoch 49/100, Loss: 510.8186167188397\n",
      "Epoch 50/100, Loss: 501.78910412701345\n",
      "Epoch 51/100, Loss: 499.33231252105435\n",
      "Epoch 52/100, Loss: 495.82074738479776\n",
      "Epoch 53/100, Loss: 499.4843861292632\n",
      "Epoch 54/100, Loss: 520.9281138418654\n",
      "Epoch 55/100, Loss: 511.68926087378486\n",
      "Epoch 56/100, Loss: 488.7264244485095\n",
      "Epoch 57/100, Loss: 501.0536756278535\n",
      "Epoch 58/100, Loss: 486.2007433451697\n",
      "Epoch 59/100, Loss: 501.65333237947846\n",
      "Epoch 60/100, Loss: 488.6010601003078\n",
      "Epoch 61/100, Loss: 461.63565459913946\n",
      "Epoch 62/100, Loss: 478.732909039479\n",
      "Epoch 63/100, Loss: 468.35859749742747\n",
      "Epoch 64/100, Loss: 503.6395583878182\n",
      "Epoch 65/100, Loss: 460.3145965754865\n",
      "Epoch 66/100, Loss: 480.18461721615665\n",
      "Epoch 67/100, Loss: 458.059008627465\n",
      "Epoch 68/100, Loss: 460.5247215252619\n",
      "Epoch 69/100, Loss: 459.20090395804596\n",
      "Epoch 70/100, Loss: 459.8705096563993\n",
      "Epoch 71/100, Loss: 460.0425915793038\n",
      "Epoch 72/100, Loss: 476.4744340507786\n",
      "Epoch 73/100, Loss: 471.8187535718295\n",
      "Epoch 74/100, Loss: 450.8814333508029\n",
      "Epoch 75/100, Loss: 474.9196847072721\n",
      "Epoch 76/100, Loss: 454.07692644127\n",
      "Epoch 77/100, Loss: 450.295272978034\n",
      "Epoch 78/100, Loss: 443.3926896093825\n",
      "Epoch 79/100, Loss: 444.5199726336384\n",
      "Epoch 80/100, Loss: 441.7214918097908\n",
      "Epoch 81/100, Loss: 430.67757551375075\n",
      "Epoch 82/100, Loss: 436.1067855965051\n",
      "Epoch 83/100, Loss: 447.3770570310086\n",
      "Epoch 84/100, Loss: 424.1588067571967\n",
      "Epoch 85/100, Loss: 449.20349260892635\n",
      "Epoch 86/100, Loss: 428.12663402421964\n",
      "Epoch 87/100, Loss: 423.0389384177587\n",
      "Epoch 88/100, Loss: 439.63753555089414\n",
      "Epoch 89/100, Loss: 432.434298886126\n",
      "Epoch 90/100, Loss: 427.54623562182184\n",
      "Epoch 91/100, Loss: 432.16541514266095\n",
      "Epoch 92/100, Loss: 429.31379416663555\n",
      "Epoch 93/100, Loss: 435.7176150622764\n",
      "Epoch 94/100, Loss: 437.839161559486\n",
      "Epoch 95/100, Loss: 418.4316649577197\n",
      "Epoch 96/100, Loss: 443.3437081176417\n",
      "Epoch 97/100, Loss: 408.6906872853546\n",
      "Epoch 98/100, Loss: 411.3203154599207\n",
      "Epoch 99/100, Loss: 410.3278029056882\n",
      "Epoch 100/100, Loss: 408.0412556445623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThroughputPredictor(\n",
       "  (stack): Sequential(\n",
       "    (0): Linear(in_features=47, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regression = ThroughputPredictor(num_features=len(features.columns))\n",
    "regression.to(device)\n",
    "regression.fit(X_train, y_train, epochs=100, batch_size=16, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ebde4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a51cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regression.predict(X_test)\n",
    "percent_error = np.abs((y_pred - y_test) / y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b139af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAG2CAYAAACeUpnVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAT/RJREFUeJzt3XlYVGX/BvB7GGDYQfZFENxTEVEU0UpLUtPXtMVMTcy1xR0rtXLPSE2z0rRV85emZm6ZuYRbKqmguIMLKIiyC8M+MHN+f5CTI6gMznCAc3+ui+t65znnzHzhvK9zv895FpkgCAKIiIiIJMxE7AKIiIiIxMZARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREkidqIDp8+DD69esHT09PyGQybNu27ZHXHDx4EO3bt4dCoUDTpk2xZs0ao9dJRERE9ZuogaigoAABAQFYsWJFlc5PTExE37598cwzzyA2NhaTJ0/G6NGjsWfPHiNXSkRERPWZrLZs7iqTybB161YMGDDggedMmzYNf/zxB86fP69te+2115CTk4Pdu3fXQJVERERUH5mKXYA+oqKiEBoaqtPWq1cvTJ48+YHXlJSUoKSkRPtao9EgOzsbTk5OkMlkxiqViIiIDEgQBOTl5cHT0xMmJoZ/wFWnAlFqairc3Nx02tzc3KBUKlFUVARLS8sK10RERGDu3Lk1VSIREREZUXJyMho2bGjw961Tgag6ZsyYgfDwcO3r3Nxc+Pj4IDk5GXZ2diJWRkRERFWlVCrh7e0NW1tbo7x/nQpE7u7uSEtL02lLS0uDnZ1dpb1DAKBQKKBQKCq029nZMRARERHVMcYa7lKn1iEKCQlBZGSkTtu+ffsQEhIiUkVERERUH4gaiPLz8xEbG4vY2FgA5dPqY2NjkZSUBKD8cVdYWJj2/LfeegsJCQl4//33ERcXh6+//hqbNm3ClClTxCifiIiI6glRA1F0dDQCAwMRGBgIAAgPD0dgYCBmzZoFALh9+7Y2HAGAn58f/vjjD+zbtw8BAQFYsmQJvv/+e/Tq1UuU+omIiKh+qDXrENUUpVIJe3t75ObmcgwRERFRHWHs7+86NYaIiIiIyBgYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyTMUugIiIiOheqjINkrILkFtUipzCUtzKLcYn22KM+pkMRERERCQatUbAsWuZOJ+ixJnkHOy9mAqNUPE8TYnaqHUwEBEREZHRaDQCCkvVuHmnEH+cvY2UnCKUlGlw+HIG8orLHnl9S3dbqNQaKDSmSDZinQxEREREZBC3coqw69xtJGQW4OadIpxOugO1RkCh6uG9OwpTE1iYyRHo4wBXWwWeaeGKXq3dYWIi056jVCph/77xamcgIiIioioTBAEJmQU4GJ+BradvwtJMjrM3c1FSpnnodSYywEQmQ48nXBHS2AkWZnIozEwQ6N0Ang6WMDcVd54XAxERERHpSFcWIz2vBEWlasQm5eB6VgG2nk6BvaUZMvJKUFbZIJ972ChM0S/AA42dbdDa0w7tGzWAwtQEMpnsodeJiYGIiIiIEJ+ah70XUnHkaiaOJ2ZXes69j76szOVwsjHHk01dEPqEK+wszdDc1Rb2VmY1VbJBMRARERFJUPT1bOyPS0dscg7iUvOQXaCqcI6vkxUszOTIKSzF082d0c67Abo0cYK7vQUszOQiVG08DERERET1XEFJGc6n5OK7vxMQm5yDzPyK4eeuoEYNMDCoIV4N8q7Vj7gMjYGIiIionihVa3D2Zi5OJGbjTHIOLqflISGz4IHnW5vL0b5RAzzfxgPdWrjAy8GyBqutXRiIiIiI6pCCkjIkZhYgM78EUQlZSM4uxMnrd2AuN0FKTtFDr23lYYcAbwc808IFnZs4wc6ibo73MQYGIiIiolrsfEoufjySiL8upcFaYYrbucWPvKaBlRmautqgqasNOjRyxFPNnOFqq5DUIzB9MRARERHVEmdv5iAxswBHr2biYHwG0vNKdI4r71vZuYWbLco0GvR4wg1NXWzQzscBvk7Woq/pUxcxEBEREdWw4lI1UnKKcDIxG39dSkeashjnUnIfes0bXXzRubEj2vs0gKudRQ1VKh0MREREREZUptYgI78EkZfSkXynEN8cSnjkNU83d4G9pRn+19YDTzZ1hrWCX9fGxr8wERGRAQmCgAu3lDh0OQN7L6bh7M0cCA9Y2NnWwhSe9pZo29AewY2d0LmxIxo2sKrZggkAAxEREdFjiU/Nw+7zqcjML8G20ynIK6l8B3e5iQxWZnJM6NEUXZo4o7WnHQc51yIMRERERFUQc+MOrqXn41TSHViay3HkSiaupOc/9Jr2Pg54urkLRj3pB1tOca/VGIiIiIgqcSunCEnZhTgYn4FVh6498vyuTZ3QpYkzOjd2RGtP+3q3tUV9x0BERESSl5FXgmsZ+biWkY+9F9KQfKcQCRmVr/D8v7YeUBaXIaSxE5q72SDI1xH2luz9qesYiIiISHLK1BqcuZmD5fuv4sItZYX1fu6yMDNBM1dbAMCEZ5uiZ2v3miyTahADERER1Xt3ClRYd/wGYpNz8Nel9ErPkckAG4UpnG0U6NLECUOCfdDa076GKyWxMBAREVG9o9YI2B6bgog/45DxgN4fAHCyNkfP1u6Y1rsFHKzMa7BCqm0YiIiIqM7LLlDh4z8uYte52yhTCyjTVL7wj7ejJQYFeeOZlq5o4WYLUzm3uKByDERERFSnpCuLcTwxG8euZWF/XBrSlJX3AFmby9Hayx592rijT1sPuNpyuwt6MAYiIiKqte4UqHAjuxCxSXdwI7sQe86n4tZDdnt/KdALg4N90NjZGnaWZjBjDxBVEQMRERHVCqVqDTbH3MSRq5n451oWsgpUDz2/sYs1mrnaoKOvI/r4e8DD3oIrP1O1MRAREZEo1BoBN7IKsD32FvbHpT90t/dGTlawMJUjtJUrBgX5wMeJ+32RYTEQERFRjRAEAbHJOVhx4OoDp74D5WN/BnX0QUsPW3Rr7gI3O479IeNjICIiIoMTBAGJmQVYc+w6fou5CWuF6QMXP5TJgCebOqNLE2cMDGoIZxtFDVdLxEBERESPoVStwd9XMrDnfBriUpUoVKkr3fC0QKXW/mc/Z2t09G2AsBBftHC35cBnqhVED0QrVqzA4sWLkZqaioCAAHz11Vfo1KnTA89ftmwZVq5ciaSkJDg7O+OVV15BREQELCzYpUpEVBOKS9X47dRNfPHXFeQVl6GoVP3Ac20Vpni5Q0M809IVjRyt4GqngJW56F89RBWI+t/KjRs3Ijw8HKtWrUJwcDCWLVuGXr16IT4+Hq6urhXOX79+PaZPn44ff/wRXbp0weXLl/HGG29AJpNh6dKlIvwGRET1W06hCttOpyAxswBbTqUgr6Ss0vMCvB3gaqtAUKMGaO1pD3d7C/g4WsHclL0/VDfIBEGofDnPGhAcHIyOHTti+fLlAACNRgNvb29MmDAB06dPr3D++PHjcenSJURGRmrbpk6diuPHj+PIkSNV+kylUgl7e3vk5ubCzs7OML8IEVE9IAgCLtxS4vCVDBy7moXcotIHzvySm8gQ0tgJL7TzxIuBXnzsRUZn7O9v0XqIVCoVYmJiMGPGDG2biYkJQkNDERUVVek1Xbp0wc8//4wTJ06gU6dOSEhIwK5duzBs2LAHfk5JSQlKSv4byKdUKg33SxAR1XGFqjLsj0vHpuibOHw544HnNXa2xv8CPNHAygx9/D3gYqOAiQnX/KH6Q7RAlJmZCbVaDTc3N512Nzc3xMXFVXrNkCFDkJmZiSeffBKCIKCsrAxvvfUWPvjggwd+TkREBObOnWvQ2omI6qpCVRnWRt3A/0XdgEqteeDGp4E+Dgj2c8JzrVzR3qcBFzykeq9OjWw7ePAgPvnkE3z99dcIDg7G1atXMWnSJMyfPx8zZ86s9JoZM2YgPDxc+1qpVMLb27umSiYiElWpWoOzN3OxPy4NKw9ewwP2PIWTtTmebemKCc8246KHJEmiBSJnZ2fI5XKkpaXptKelpcHd3b3Sa2bOnIlhw4Zh9OjRAAB/f38UFBRg7Nix+PDDD2FiUvEZtkKhgELBNS2ISBoKVWXYdvoW/r6SgdNJOUhVVr7v13Ot3DCscyP4OlnD08GCu76T5IkWiMzNzdGhQwdERkZiwIABAMoHVUdGRmL8+PGVXlNYWFgh9MjlcgDlgwGJiKQip1CF2OQc7L2YhuTsQiiLy3AtPR/5D5gFBgAjuvri6eYueLqZC+Qc/0OkQ9RHZuHh4Rg+fDiCgoLQqVMnLFu2DAUFBRgxYgQAICwsDF5eXoiIiAAA9OvXD0uXLkVgYKD2kdnMmTPRr18/bTAiIqpvytQa7L2YhuMJWYhNzsGFW0qUPejZ179CGjvB1U6BEV390NbLngOgiR5B1EA0aNAgZGRkYNasWUhNTUW7du2we/du7UDrpKQknR6hjz76CDKZDB999BFSUlLg4uKCfv36YcGCBWL9CkRERlGoKsM3hxLwa3QybuVW/tgLAJxtFHjCwxbPtHBFgLc9vBtYwcVWwUHQRHoSdR0iMXAdIiKqrVRlGny07Rw2Rd+s9LiNwhRPN3dGO28HDAj0gqstV+gn6ai36xARERGg0Qj4/ewtLN9/tdI9wDr5OWJosA+ea+XGLS+IjIj/6yIiqmHnU3Kx4sBVnL+Vi+TsogrHRz3ph0EdvdHExYaDn4lqCAMREVEN2R+Xhq/2X8XppJwKxyzMTDC8iy/Cn2sOhSkniRDVNAYiIiIjO3IlE2P/LxqFKt1d4Qd38sb/2nqio68jN0ElEhkDERGREQiCgFWHErBwt+5WRC3dbbHw5bYI8HYQpzAiqhQDERGRgWTklWDjySQcvZqFqIQsnWP+XvZ4p3sTPO/vIVJ1RPQwDERERI/pWkY+Pt93GTvP3q5wzNvREj8O74hmbrYiVEZEVcVARERUDUUqNfZdSsMvx5Mq9AYF+zni5fYN0crTDm287EWqkIj0wUBERKSHWzlFCPvxBK7et2ZQwwaWCH+uOV5q31CkyojocTAQERFVwZW0PMzYcg7RN+7otHf0bYDJoc3RpYkTt8sgqsMYiIiIHmLvhVS89XMM7t1L1cpcjhnPt8TrnRsxBBHVEwxERET3ScjIx5RNZ3AmOUenXSYD5r7QGoM7+cBMznWDiOoTBiIiIgC5RaX47nAC1kZdh7K4TOeYmVyGzwYGoH87L5GqIyJjYyAiIkkrVWuw69xtTNoQq9Pe0t0WrT3tMe35FtxVnkgCGIiISJJyClUYueYkTt23r1gff3eM6OqHjr6O4hRGRKJgICIiSTmTnIP3N59FfFqeTruPoxW2j+uKBtbmIlVGRGJiICKiek+jEbD3Yip+OJKIk9d1p83PfaE1wkI4W4xI6hiIiKje0mgEfPt3Ar49nIDsApXOsTn9WuH1zo1gytliRAQGIiKqh4pUavx4NBGL98TrtLd0t8UbXXzxWicfkSojotqKgYiI6g1lcSnm7LiALadSdNpbedhhzYiOcLXjbDEiqhwDERHVeUlZhRi99iQup+nuLxbUqAEWDwyAn7O1SJURUV3BQEREddpXkVewZN9lnbbJoc0woqsf7C3NRKqKiOoaBiIiqpNyClV48etjSMws0La93L4hFrzYBhZmchErI6K6iIGIiOqUnEIVpm46g8i4dG2br5MVdk9+mkGIiKqNgYiI6ozVRxMx9/eLOm2jn/TDh32f4DpCRPRYGIiIqNbbevompmw8o9MW+oQbIl7yh4utQqSqiKg+YSAiolorLlWJj7aeR/SN/1aXDmnshO+GB8FGwX++iMhw+C8KEdUagiDgwi0ldpy5hW8PJ+gck5vIsH50MIIbO4lUHRHVZwxERCS6OwUqrDl2HV9EXqlwzNlGgcWvtEX3Fi4cJ0RERqN3IDp16hTMzMzg7+8PANi+fTtWr16NVq1aYc6cOTA3507RRPRogiDgdHIOfjmehF9jblY4/lwrN0x4tinaNnSo+eKISHL0DkRvvvkmpk+fDn9/fyQkJOC1117Diy++iF9//RWFhYVYtmyZEcokovoiu0CFYT8cx4VbygrHJvVohuFdfOFozf9jRUQ1S+9AdPnyZbRr1w4A8Ouvv+Lpp5/G+vXrcfToUbz22msMRERUqUJVGb49nIBlf+k+FnOwMsO8/m3Q198DchM+EiMicegdiARBgEajAQD89ddf+N///gcA8Pb2RmZmpmGrI6I6r0ytwW+nbmLGlnPQCP+1v9+7BYYGN+L2GkRUK+gdiIKCgvDxxx8jNDQUhw4dwsqVKwEAiYmJcHNzM3iBRFQ35ZeUYd7vF7ApWnd80Kgn/fB29yZwtuH6QURUe+gdiJYtW4ahQ4di27Zt+PDDD9G0aVMAwObNm9GlSxeDF0hEdUt2gQpzdlzAjjO3dNqfauaM5UPas0eIiGolmSAIwqNPe7Ti4mLI5XKYmdXuf+yUSiXs7e2Rm5sLOzs7scshqjfO3czFh9vO4ezNXJ32vm09MPeF1uwRIqLHYuzv72qtQ5STk4PNmzfj2rVreO+99+Do6IiLFy/Czc0NXl5ehq6RiGqp/JIyLNkbj9VHr1c49mpQQ8x9oQ0szbnhKhHVfnoHorNnz6JHjx5wcHDA9evXMWbMGDg6OmLLli1ISkrC2rVrjVEnEdUiR65k4sv9V3AiMbvCsYiX/DEoyBsmnDFGRHWI3oEoPDwcI0aMwKJFi2Bra6tt79OnD4YMGWLQ4oiodsnKL8GHW89j94VUnfa+bT0w+kk/BPo0EKkyIqLHo3cgOnnyJL755psK7V5eXkhNTa3kCiKq6+JSlQjfeAYXb/+3mKKDlRlm/a8VXgz04pYaRFTn6R2IFAoFlMqKK8xevnwZLi4uBimKiMQlCALO3szFN4evIe52HhIyC3SOzx/QBsM6NxKpOiIiw9M7EL3wwguYN28eNm3aBACQyWRISkrCtGnT8PLLLxu8QCKqWT8eScS8nRcrtFubyzGoow8+7PsEV5QmonpH72n3ubm5eOWVVxAdHY28vDx4enoiNTUVISEh2LVrF6ytrY1Vq0Fw2j1R5f66mIbpW84iM1+lbbMwM0FYiC9eD24Eb0dLPhojItHUumn39vb22LdvH44cOYKzZ88iPz8f7du3R2hoqMGLIyLjiktV4tvDCdhyKkWn3dHaHGtHdkIbL3uRKiMiqlkGW5ixrmAPEVH5atJTN8XiQHyGTruHvQXe6tYEw7v4ilMYEdED1Ioeoi+//LLKbzhx4sRqF0NExvf5vsv4IlJ3x/khwT4Y90xTeDlYilQVEZG4qtRD5OfnV7U3k8mQkJDw2EUZE3uISIqy8kvwy4kkfLb3sk77sM6NMK9/a44NIqJar1b0ECUmJhr8g4nIeARBwIH4dKw+eh1/X8mscNzFVoEtb3eBt6OVCNUREdU+1drL7K67nUv8f5dEtUfUtSxM2nAa6XklFY691tEbA4MaokMjRxEqIyKqvaoViH744Qd8/vnnuHKlfBxCs2bNMHnyZIwePdqgxRFR1SRnF2LfxbQK6wd5OViiX4AnXgz0QnM3G/6fFyKiB9A7EM2aNQtLly7FhAkTEBISAgCIiorClClTkJSUhHnz5hm8SCKqSKMRMOf3C1h/PAllmopDAX99KwQdfdkTRERUFXpPu3dxccGXX36JwYMH67T/8ssvmDBhAjIzK45XqE04qJrqujK1BmuOXcfHf1yqcCyksRPe6OqLXq3dRaiMiMh4asWg6nuVlpYiKCioQnuHDh1QVlZmkKKIqKJ/ErLw/uazSMou1Glv4WaLzwe1QytPBnwiourSOxANGzYMK1euxNKlS3Xav/32WwwdOtRghRFRuYPx6Zi0IRa5RaU67UODfRAW4osW7rYiVUZEVH9Ue1D13r170blzZwDA8ePHkZSUhLCwMISHh2vPuz80EVHVqDUCfjp2HTvO3EJsco623c/ZGr1au+Otbo3hYGUuXoFERPWM3oHo/PnzaN++PQDg2rVrAABnZ2c4Ozvj/Pnz2vM4m4Woeg5fzsAbq0/g3nHSXg6WWD4kEIE+DcQrjIioHtM7EB04cMAYdRBJXnaBChN+OYWjV7O0be19HPBWtyboyUHSRERG9VgLMxKRYew6dxvv/noGhSq1tu3kh6FwsVWIWBURkXToHYiKi4vx1Vdf4cCBA0hPT4dGo9E5furUKYMVR1TfCYKA7/9OxIJd/02hnz+gDYZ28oGJCR87ExHVFL0D0ahRo7B371688sor6NSpE8cKEVWDIAhYG3UDPx5NxI2s8mn0ClMT7JvSDT5O3F+MiKim6R2Idu7ciV27dqFr167GqIeo3rt0W4mx/xeN5OwibVsff3cseiUANgo+xSYiEoPe//p6eXnB1pbrnhBVx6Ldcfj64DXta2cbBT4fFICnmrmIWBUREekdiJYsWYJp06Zh1apVaNSokTFqIqpXsvJL8Nupm/hkV5xO+5Z3uqA9p9ETEdUKJvpeEBQUhOLiYjRu3Bi2trZwdHTU+dHXihUr4OvrCwsLCwQHB+PEiRMPPT8nJwfjxo2Dh4cHFAoFmjdvjl27dun9uUQ1IWLXJXT4+C+dMNS5sSOufdKHYYiIqBbRu4do8ODBSElJwSeffAI3N7fHGlS9ceNGhIeHY9WqVQgODsayZcvQq1cvxMfHw9XVtcL5KpUKzz33HFxdXbF582Z4eXnhxo0bcHBwqHYNRIam0Qj4JyEL76w/hZzC/7bbmBLaHH3buqOpKx85ExHVNnrvdm9lZYWoqCgEBAQ89ocHBwejY8eOWL58OQBAo9HA29sbEyZMwPTp0yucv2rVKixevBhxcXEwMzOr1mdyt3syph+OJGLh7jioyv5bjuK1jt6IeMmfMzKJiB6Dsb+/9X5k1rJlSxQVFT36xEdQqVSIiYlBaGjof8WYmCA0NBRRUVGVXrNjxw6EhIRg3LhxcHNzQ5s2bfDJJ59ArVZXej4AlJSUQKlU6vwQGVq6shg9lhzE/J0XtWGoW3MXrBnREZ++3JZhiIioltP7kdmnn36KqVOnYsGCBfD396/QU1PV1JaZmQm1Wg03Nzeddjc3N8TFxVV6TUJCAvbv34+hQ4di165duHr1Kt555x2UlpZi9uzZlV4TERGBuXPnVqkmIn1pNALCN8ViW+wtbVtjF2v8NKITvB25nhARUV2hdyDq3bs3AKBHjx467YIgQCaTPbS35nFpNBq4urri22+/hVwuR4cOHZCSkoLFixc/MBDNmDED4eHh2tdKpRLe3t5Gq5GkY9/FNIxZG6197eNohU9f9keXJs4iVkVERNUh2uauzs7OkMvlSEtL02lPS0uDu3vlG1l6eHjAzMwMcrlc2/bEE08gNTUVKpUK5ubmFa5RKBRQKLgfFBnO7vOp+GjbeWTml2jbBnfywccD2kDO7TaIiOokvQNRt27dDPLB5ubm6NChAyIjIzFgwAAA5T1AkZGRGD9+fKXXdO3aFevXr4dGo4GJSfnwp8uXL8PDw6PSMERkSIIg4LO98Vhx4L+FFf297LF4YFu0dOcAfSKiuqza+wQUFhYiKSkJKpVKp71t27ZVfo/w8HAMHz4cQUFB6NSpE5YtW4aCggKMGDECABAWFgYvLy9EREQAAN5++20sX74ckyZNwoQJE3DlyhV88sknmDhxYnV/DaIq+fPcbby97r+Ni51tFPiwb0sMaOfFAdNERPWA3oEoIyMDI0aMwJ9//lnpcX3GEA0aNAgZGRmYNWsWUlNT0a5dO+zevVs70DopKUnbEwQA3t7e2LNnD6ZMmYK2bdvCy8sLkyZNwrRp0/T9NYiqRFWmwXOfH9JuwAoA459pind7tRCxKiIiMjS91yEaOnQobty4gWXLlqF79+7YunUr0tLS8PHHH2PJkiXo27evsWo1CK5DRI+SV1yKA/EZ2HMhFWdv5uhswnpk2jNo2ICzx4iIapqxv7/17iHav38/tm/fjqCgIJiYmKBRo0Z47rnnYGdnh4iIiFofiIgepEilxowtZ3Wm0N/15tONMf35lnw8RkRUT+kdiAoKCrTbajRo0AAZGRlo3rw5/P39cerUqUdcTVQ7HU/IwqBv/9Fp87S3QI8n3DDmqcbwcWKvEBFRfaZ3IGrRogXi4+Ph6+uLgIAAfPPNN/D19cWqVavg4eFhjBqJjOZOgQpDvj+OS7f/W8Hc38seP48Khr1V9baHISKiukfvQDRp0iTcvn0bADB79mz07t0b69atg7m5OdasWWPo+ogMTq0RsObYdaw7fgMJGQXadmtzOfaFd4Ong6WI1RERkRj0HlR9v8LCQsTFxcHHxwfOzrV/hV4Oqpa2DSeSMH3LuQrtE55tivDnmnOMEBFRLVXrBlXfz9zcHM2bN4eNjY0h6iEyilK1BpM2nMauc6natoCG9hj3TFOEPuEGE64wTUQkaVXe7f7333+v8EhswYIFsLGxgYODA3r27Ik7d+4Yuj6ix5JXXIole+PR7MM/tWHIy8ESJz8MxfbxT6Jna3eGISIiqnogWrp0KQoK/htvcezYMcyaNQszZ87Epk2bkJycjPnz5xulSCJ93SlQ4YOt5+A/Zy++2n9V297X3wOH338GLrbc346IiP5T5UdmFy5cwNKlS7WvN2/ejOeeew4ffvghAMDCwgKTJk3SOYdIDPfvQm9tLkf3lq6Y+0JrONswCBERUUVVDkR5eXlwcnLSvj5y5AgGDhyofd26dWvculVxQTuimhJzIxtro25g+z0LK07s0QzjnmkChalcxMqIiKi2q3Ig8vLywqVLl+Dj44P8/HycOXMGn3/+ufZ4VlYWrKy4eB3VvNTcYgz85pjOFhsBDe2xdiTXEiIioqqpciAaOHAgJk+ejA8++AC7du2Cu7s7OnfurD0eHR2NFi244SXVHEEQ8MmuS/ju70Sd9mm9W+Ktbo05hZ6IiKqsyoFo1qxZSElJwcSJE+Hu7o6ff/4Zcvl/jyF++eUX9OvXzyhFEt1PWVyKwd/+gwu3/lth+sVALyx9NYBBiIiI9PbYCzPWNVyYse47di0TQ747rn09uJM35vVvAzN5lSdNEhFRHVPrF2YkqikFJWUYsfokTlzP1rb9NLITujV3EbEqIiKqDxiIqNa7nJaH5fuvYscZ3VmMf7//DLwdOZCfiIgeHwMR1WqvrDyG6Bu6K6CHP9ccE55tyrFCRERkMAxEVCsJgoDP/7qiE4YmPtsUI5/0g4OVuYiVERFRfVSlUaiOjo7IzMwEAIwcORJ5eXlGLYqkTVlcinfWncKXkVcAAAPaeeL6p30R3rMFwxARERlFlQKRSqWCUlk+vfmnn35CcXGxUYsi6bqeWYC2c/biz/PlG7H2C/DE4oEBIldFRET1XZUemYWEhGDAgAHo0KEDBEHAxIkTYWlpWem5P/74o0ELJOmIupalswfZpy/547VOPiJWREREUlGlQPTzzz/j888/x7Vr1yCTyZCbm8teIjKYjLwSvLH6hHaRRVsLU2x+qwtauNuKXBkREUmF3gsz+vn5ITo6Wmej17qECzPWHoIg4Nfom3j/t7PaNrmJDMemPws3OwsRKyMiotqm1i3MmJiY+OiTiB5h2+kUzN5xAblFpdq2ZYPaYUCgl4hVERGRVFVr2v2hQ4fw2Wef4dKlSwCAVq1a4b333sNTTz1l0OKo/tkfl4aRa6J12l4M9MLM/7WCozVnkBERkTj03vzp559/RmhoKKysrDBx4kTtAOsePXpg/fr1xqiR6ok5Oy7ohCFbC1PsnfI0Ph/UjmGIiIhEpfcYoieeeAJjx47FlClTdNqXLl2K7777TttrVFtxDFHNyy0sxfNfHMat3PKB+DYKU+x/txtcbTlOiIiIqsbY39969xAlJCSgX79+FdpfeOEFji8iHWqNgGmbz6LDx/u0YWhQkDfOzenJMERERLWK3mOIvL29ERkZiaZNm+q0//XXX/D29jZYYVS3Hb2aiaHfH9dp+3poe/Tx9xCpIiIiogfTOxBNnToVEydORGxsLLp06QIAOHr0KNasWYMvvvjC4AVS3XI66Q5WHLiGvy6laduautpg+7iusFZw6zwiIqqd9P6Gevvtt+Hu7o4lS5Zg06ZNAMrHFW3cuBH9+/c3eIFUd6w7fgMfbj2vfe1kbY6t73SFj5OViFURERE9mt6Dqus6Dqo2jiV74/HV/qva1x/1fQJvdPGFqVzvYWpEREQV1LqFGYnudfJ6NlYdvIbIuHQAgJeDJba+0wWuXGmaiIjqEAYiqrYdZ25h4i+nta897S1w8L3uMGOvEBER1TEMRKS3NGUxhv1wHJfT8gEA5nITfDm4HXq34QwyIiKqmxiISC+X0/LwwvIjKC7VACifQbZjfFdYmfO/SkREVHfxW4yqpEilxpwdF7AxOhkAYCIDPh7gjyHBPiJXRkRE9Pj0DkRqtRpr1qxBZGQk0tPTodFodI7v37/fYMVR7VCoKsPwH0/g5PU7AIDmbjb4ZlgQ/JytRa6MiIjIMPQORJMmTcKaNWvQt29ftGnTBjKZzBh1US3x57nbeHvdKe3rvv4eWD4kkPediIjqFb0D0YYNG7Bp0yb06dPHGPVQLSEIAsb/chp/nL2tbVv0Slu8GsTtWYiIqP7ROxCZm5tX2MeM6pebdwox/McTuJZRAAAI8HbAD8OD4GyjELkyIiIi49B7wZipU6fiiy++gMQWuJaMfRfT8OTCA9ow1C/AE9ve6cIwRERE9ZrePURHjhzBgQMH8Oeff6J169YwMzPTOb5lyxaDFUc1R6MRMGZttHbFaVuFKb5+vT2eauYicmVERETGp3cgcnBwwIsvvmiMWkgk1zMLMHptNK6mly+06GKrwN/vPwMLM7nIlREREdUMvQPR6tWrjVEHiWTVoWv49M847esP+rTEmKcacxYZERFJSrUXZszIyEB8fDwAoEWLFnBx4aOVuiZ8Uyy2nErRvn6/dwuMfbqJiBURERGJQ+9AVFBQgAkTJmDt2rXaRRnlcjnCwsLw1VdfwcrKyuBFkmEVqsrw4opjiE/LAwC42ipwdPqz3JSViIgkS+9vwPDwcBw6dAi///47cnJykJOTg+3bt+PQoUOYOnWqMWokAzqfkovgTyK1Yailuy3+mdGDYYiIiCRNJug5f97Z2RmbN29G9+7dddoPHDiAV199FRkZGYasz+CUSiXs7e2Rm5sLOzs7scupUd//nYCP/7ikfT2yqx9m9GnJMERERLWesb+/9X5kVlhYCDc3twrtrq6uKCwsNEhRZHjrjyfphKFD73VHIyfuRUZERARU45FZSEgIZs+ejeLiYm1bUVER5s6di5CQEIMWR4Zx7Gom5u+8CADwdrTElQXPMwwRERHdQ+8eoi+++AK9evVCw4YNERAQAAA4c+YMLCwssGfPHoMXSI/n2NVMDPn+OACgnbcDNr7ZmY/IiIiI7qN3IGrTpg2uXLmCdevWIS6ufP2awYMHY+jQobC0tDR4gVR9WfklGPt/MdrXa0Z0hMKUiy0SERHdr1rrEFlZWWHMmDGGroUM6FZOEbp8uh8AYGUux84JT8LBylzkqoiIiGqnKgWiHTt24Pnnn4eZmRl27Njx0HNfeOEFgxRG1ZeVX4IXvz6qfb34lQA0drERsSIiIqLarUrT7k1MTJCamgpXV1eYmDx4/IlMJoNarTZogYZW36fdZ+aXoPvig8gvKYPcRIaNYzsjyNdR7LKIiIgeS62Ydn93Rer7/zPVLhl5JXju80PILykDAHzyYhuGISIioirQe7rR2rVrUVJSUqFdpVJh7dq1BimK9JdbWIqwH08gp7AUchMZvhnWAYM6+ohdFhERUZ2g90rVcrkct2/fhqurq057VlYWXF1d+chMBPklZXh60QFkF6ggkwFb3+mKdt4OYpdFRERkMLXikdm9BEGATCar0H7z5k3Y29sbpCiquqz8EnT4+C/t6+WD2zMMERER6anKgSgwMBAymQwymQw9evSAqel/l6rVaiQmJqJ3795GKZIql5JThOE/ntC+nv58S/Rt6yFiRURERHVTlQPRgAEDAACxsbHo1asXbGz+m8Ztbm4OX19fvPzyywYvkCp35EomXv/huPb1utHB6NrUWcSKiIiI6q4qB6LZs2dDrVbD19cXPXv2hIcHeyLEcuFWLsb+X7T29S9jOiOkiZOIFREREdVtes0yk8vlePPNN3U2djWEFStWwNfXFxYWFggODsaJEycefRGADRs2QCaTaXuvpCDmRjaGfHcchSo13OwUOD3zOYYhIiKix6T3tPs2bdogISHBYAVs3LgR4eHhmD17Nk6dOoWAgAD06tUL6enpD73u+vXrePfdd/HUU08ZrJbabv3xJLy8Mgq5RaVwsDLDtnFd0cCa23EQERE9Lr0D0ccff4x3330XO3fuxO3bt6FUKnV+9LV06VKMGTMGI0aMQKtWrbBq1SpYWVnhxx9/fOA1arUaQ4cOxdy5c9G4cWO9P7Muir6ejQ+2ngMANHKywp+TnoKHPTfTJSIiMgS9p9336dMHQPmeZfdOv787HV+fdYhUKhViYmIwY8YMbZuJiQlCQ0MRFRX1wOvmzZsHV1dXjBo1Cn///fdDP6OkpERnIcnqhDaxpSmL8cqq8r9HwwaW2D+1O+QmFZc+ICIiourROxAdOHDAYB+emZkJtVoNNzc3nXY3NzfExcVVes2RI0fwww8/IDY2tkqfERERgblz5z5uqaIpLlXjxRXlG7Vamsmx6c0QhiEiIiID0zsQdevWzRh1VEleXh6GDRuG7777Ds7OVZtiPmPGDISHh2tfK5VKeHt7G6tEg7p5pxAvrzyGNGV5D9fXr7eHpwMfkxERERma3oEIAHJycvDDDz/g0qVLAIDWrVtj5MiReq9U7ezsDLlcjrS0NJ32tLQ0uLu7Vzj/2rVruH79Ovr166dtu7vZrKmpKeLj49GkSROdaxQKBRQKhV511QbJ2YV4atF/vXGrXm+PZ1q4PuQKIiIiqi69B1VHR0ejSZMm+Pzzz5GdnY3s7GwsXboUTZo0walTp/R6L3Nzc3To0AGRkZHaNo1Gg8jISISEhFQ4v2XLljh37hxiY2O1Py+88AKeeeYZxMbG1pmen0dJzyvWhiG5iQyrXm+P3m247hMREZGx6N1DNGXKFLzwwgv47rvvtNt3lJWVYfTo0Zg8eTIOHz6s1/uFh4dj+PDhCAoKQqdOnbBs2TIUFBRgxIgRAICwsDB4eXkhIiICFhYWaNOmjc71Dg4OAFChva5KVxbjte/+0b5ePzoYwY25zhAREZEx6R2IoqOjdcIQUP646v3330dQUJDeBQwaNAgZGRmYNWsWUlNT0a5dO+zevVs70DopKQkmJnp3ZNVJxaVqPLnwAFRqDczlJtjyThe08eKGuURERMamdyCys7NDUlISWrZsqdOenJwMW1vbahUxfvx4jB8/vtJjBw8efOi1a9asqdZn1kaTN8RCpS4fE/X10PYMQ0RERDVE766XQYMGYdSoUdi4cSOSk5ORnJyMDRs2YPTo0Rg8eLAxapSE7bEp2H0hFQDwzbAOCG3l9ogriIiIyFD07iH67LPPIJPJEBYWhrKyMgCAmZkZ3n77bXz66acGL1AK/rqYhikbYwEAgzt5o1frijPsiIiIyHhkgiAI1bmwsLAQ165dAwA0adIEVlZWBi3MWJRKJezt7ZGbmws7Ozuxy8H640naLTmau9lg05shcLDi/mRERET3Mvb3d7XWIQIAKysr7QyvuhKGapsD8enaMOTlYIkt73SFjaLat4SIiIiqSe8xRGVlZZg5cybs7e3h6+sLX19f2Nvb46OPPkJpaakxaqyX8opLMXfHBQBAUKMGOPhed4YhIiIikej9DTxhwgRs2bIFixYt0i6eGBUVhTlz5iArKwsrV640eJH10YI/LuF6ViEcrMzw9evtYSaXxtICREREtZHegWj9+vXYsGEDnn/+eW1b27Zt4e3tjcGDBzMQVcGBuHRsOJkMAPh4QBu42lqIXBEREZG06d0toVAo4OvrW6Hdz88P5uYcDPwoV9PzMPKnkwCAHi1d8b+2niJXRERERHoHovHjx2P+/PkoKSnRtpWUlGDBggUPXFyR/jP9t3MQBMDWwhRfDQkUuxwiIiJCNR6ZnT59GpGRkWjYsCECAgIAAGfOnIFKpUKPHj3w0ksvac/dsmWL4SqtB44nZCH6xh0AwDevd4CVOQdRExER1QZ6fyM7ODjg5Zdf1mmrL7vMG9O1jHyM/ikaADCgnSe6NHUWuSIiIiK6S+9AtHr1amPUUa9l5ZfglZXHkFdShgZWZpj5v1Zil0RERET3qPYzm4yMDMTHxwMAWrRoARcXF4MVVd98tjcedwpLYWoiw+a3u8DJRiF2SURERHQPvQdVFxQUYOTIkfDw8MDTTz+Np59+Gp6enhg1ahQKCwuNUWOdlp5XjN9OpQAA5vVvgyYuNiJXRERERPfTOxCFh4fj0KFD+P3335GTk4OcnBxs374dhw4dwtSpU41RY5322Z54qMo0aORkhUEdOdaKiIioNtL7kdlvv/2GzZs3o3v37tq2Pn36wNLSEq+++ioXZrxHfGoeNkXfBADM7NsKchOZyBURERFRZfTuISosLISbm1uFdldXVz4yu883h68BABo5WaHHE64iV0NEREQPoncgCgkJwezZs1FcXKxtKyoqwty5c7V7mxGQmluM38/cAgAsfLktZDL2DhEREdVWej8yW7ZsGXr37l1hYUYLCwvs2bPH4AXWVdN+O4tStYCW7rYI9nMUuxwiIiJ6CL0Dkb+/P65cuYJ169YhLi4OADB48GAMHToUlpaWBi+wLlobdR2HLmcAAGb3a83eISIiolpOr0BUWlqKli1bYufOnRgzZoyxaqrTikvVWLL3MgAgLKQRQpo4iVwRERERPYpeY4jMzMx0xg5RRT8du47colLYWpjivV4txC6HiIiIqkDvQdXjxo3DwoULUVZWZox66rSSMjW+P5IIAJgc2hy2FmYiV0RERERVofcYopMnTyIyMhJ79+6Fv78/rK2tdY5LeYf7HbG3kJFXAmtzOYYG+4hdDhEREVWRQXa7p3LrTyQBAEY/1RgWZnKRqyEiIqKq4m73BnI9swCnk3IAgL1DREREdUyVxxBpNBosXLgQXbt2RceOHTF9+nQUFRUZs7Y6Zcup8i06Arwd4GpnIXI1REREpI8qB6IFCxbggw8+gI2NDby8vPDFF19g3LhxxqytzihSqbHiYPk2HX3auItcDREREemryoFo7dq1+Prrr7Fnzx5s27YNv//+O9atWweNRmPM+uqEdcdvQK0RYCaX4fXOjcQuh4iIiPRU5UCUlJSEPn36aF+HhoZCJpPh1q1bRimsrhAEAZtjyh+XTevdEtYKvYdlERERkciqHIjKyspgYaE7NsbMzAylpaUGL6ouiU3OQVxqHkxNZOgX4Cl2OURERFQNVe7OEAQBb7zxBhQKhbatuLgYb731ls5aRFJbh+j//rkBAOjxhCvcOJiaiIioTqpyIBo+fHiFttdff92gxdQ1Wfkl2Ho6BQAwuBOn2hMREdVVVQ5EXH+oop//SYIgAN6OlujW3EXscoiIiKia9N7LjMppNAK2xZb3Do19qjFkMpnIFREREVF1MRBV0/64dCRmFsBcboIX2nmJXQ4RERE9Bgaiavop6joAoF+AJ+wtuas9ERFRXcZAVA1X0/Px95VMAEBYCBdiJCIiqusYiKrh15hkAEAnX0cEeDuIWwwRERE9NgaiajiekA0AeK2Tt8iVEBERkSEwEOnpdm4RztzMAQC092kgbjFERERkEAxEevr2cAIEAQhoaA9fZ+tHX0BERES1HgORHjQaATvP3gYADAvxFbcYIiIiMhgGIj3EpeYhI68EFmYm6OvvIXY5REREZCAMRHo4dq18qn1HX0dYmstFroaIiIgMhYFID4cuZwAAnm7GfcuIiIjqEwaiKipTa3AisXy6fQdfzi4jIiKqTxiIquj8LSVKyjSwUZiijae92OUQERGRATEQVdH+uHQAQCc/R5ib8s9GRERUn/CbvYqOXS0fUP1MC44fIiIiqm8YiKqgpEyN2OQcAMCTHFBNRERU7zAQVcGZ5FyUaQQ4WpvD18lK7HKIiIjIwBiIqiA2+Q6A8t3tZTKZyNUQERGRoTEQVcGpGzkAgABvB1HrICIiIuNgIHoEQRAQfaO8h6i9j4O4xRAREZFRMBA9woVbSmTml0BhasIeIiIionqKgegRTieV9w51aNQAFmbcv4yIiKg+YiB6hLuPy9qxd4iIiKjeYiB6hJh/A1FHP0eRKyEiIiJjYSB6iHRlMW7eKQIABLKHiIiIqN5iIHqIu6tTN3ezgYOVubjFEBERkdEwED3EtYwCAEBLdzuRKyEiIiJjYiB6iLsrVLdwtxW5EiIiIjKmWhGIVqxYAV9fX1hYWCA4OBgnTpx44LnfffcdnnrqKTRo0AANGjRAaGjoQ89/HDH/rlDd2pM9RERERPWZ6IFo48aNCA8Px+zZs3Hq1CkEBASgV69eSE9Pr/T8gwcPYvDgwThw4ACioqLg7e2Nnj17IiUlxaB1ZeaXIDO/BAAQ5MsZZkRERPWZTBAEQcwCgoOD0bFjRyxfvhwAoNFo4O3tjQkTJmD69OmPvF6tVqNBgwZYvnw5wsLCHnm+UqmEvb09cnNzYWf34J6fP8/dxtvrTsHP2RoH3u1e5d+HiIiIDK+q39/VJWoPkUqlQkxMDEJDQ7VtJiYmCA0NRVRUVJXeo7CwEKWlpXB0rLwXp6SkBEqlUuenKq5l5AMA2vs0qNL5REREVHeJGogyMzOhVqvh5uam0+7m5obU1NQqvce0adPg6empE6ruFRERAXt7e+2Pt7d3ld737gwzXyerKp1PREREdZfoY4gex6effooNGzZg69atsLCwqPScGTNmIDc3V/uTnJxcpfe+kp4HAGjmZmOweomIiKh2MhXzw52dnSGXy5GWlqbTnpaWBnd394de+9lnn+HTTz/FX3/9hbZt2z7wPIVCAYVCoVddGo2Aa+nlPUTN3TjlnoiIqL4TtYfI3NwcHTp0QGRkpLZNo9EgMjISISEhD7xu0aJFmD9/Pnbv3o2goCCD15WYVYCiUjUszEzg48hHZkRERPWdqD1EABAeHo7hw4cjKCgInTp1wrJly1BQUIARI0YAAMLCwuDl5YWIiAgAwMKFCzFr1iysX78evr6+2rFGNjY2sLExzOOtK2nlj8uau9nCVF6nnyoSERFRFYgeiAYNGoSMjAzMmjULqampaNeuHXbv3q0daJ2UlAQTk/9CycqVK6FSqfDKK6/ovM/s2bMxZ84cg9R08XZ5IGrqwvFDREREUiB6IAKA8ePHY/z48ZUeO3jwoM7r69evG72ey6nlgai1l73RP4uIiIjEx+dBlbieVT6gurGztciVEBERUU1gILqPIAhIySkCAHg6WIpcDREREdUEBqL7XMsoQF5xGczlJvB15gwzIiIiKWAgus+Z5BwAQBsvOyhM5eIWQ0RERDWCgeg+52/lAgDaeXMPMyIiIqlgILrPjaxCAEBjFw6oJiIikgoGovvc3cPMjzPMiIiIJIOB6B6CICAttwQAuGUHERGRhDAQ3SMzXwWVWgOZDHC20W9DWCIiIqq7GIjuEXPjDgCgiYsNLM05w4yIiEgqGIjucXeF6taediJXQkRERDWJgeget/5dodqLK1QTERFJCgPRPZKzy6fcN2zAAdVERERSwkB0jxv/BiJfJwYiIiIiKWEgukdmXvmUe1c7zjAjIiKSEgaif2UXqKAsLgMAeNhzDBEREZGUMBD96+ad8sdlbnYKWCtMRa6GiIiIahID0b+Ss8tnmLF3iIiISHoYiP51d8p9wwYMRERERFLDQPSvxH8XZWzEGWZERESSw0D0rxvaQMRd7omIiKSGgehfCRnlgcjPmYGIiIhIahiIABSXqnE7txgA4MseIiIiIslhIMJ/vUN2FqZwtjEXuRoiIiKqaQxEAJL+3bLDz9kaMplM5GqIiIiopjEQAbiSlgcAaOJqI3IlREREJAYGIgA375SvQdTIkeOHiIiIpIiBCMDNnPJHZu723NSViIhIihiIACT+O6i6iQsfmREREUmR5ANRmVqDtLwSAIC3I1epJiIikiLJB6LsAhXUGgEmMsDZho/MiIiIpEjygejulHt3OwvITTjlnoiISIokH4iupucD4JR7IiIiKZN8IIpL/XcNIg6oJiIikizJB6JrGeU9RK087ESuhIiIiMQi+UCU8e8MMxc7DqgmIiKSKkkHIkEQkPzvoGrvBpxyT0REJFWSDkQZ+SUoUKlhIgO8HS3FLoeIiIhEIulAdCOrvHfI08ESClO5yNUQERGRWCQdiBL+HVDdyImPy4iIiKRM0oHobg9RU065JyIikjRJB6LkO0UAyh+ZERERkXRJOhDdvFPeQ8RHZkRERNIm6UB0O6cYAOBiayFyJURERCQmyQaiIpUaqcryQNTExVrkaoiIiEhMkg1EyXcKAAA2ClPYW5qJXA0RERGJSbKB6HrmvzPMXG0gk8lEroaIiIjEJNlAlF2oAgC42HIPMyIiIqmTbCBKzy3f1NXdjgOqiYiIpE6ygeh2bvkaRB4ODERERERSJ9lAlPjvKtVeXJSRiIhI8iQbiG5ml88ya8JtO4iIiCRPsoEop6gMAOBhz0dmREREUifZQAQAlmZyNLAyF7sMIiIiEpmkA1ELd1uYmHANIiIiIqmTdCDy5aauREREBIkHIkdrLspIREREEg9E7vYMRERERCTxQOTrxF3uiYiISOKByM+ZgYiIiIgkHog8uUo1ERERQcKByFohh7XCVOwyiIiIqBaQbCDigoxERER0V60IRCtWrICvry8sLCwQHByMEydOPPT8X3/9FS1btoSFhQX8/f2xa9cuvT/T25FrEBEREVE50QPRxo0bER4ejtmzZ+PUqVMICAhAr169kJ6eXun5x44dw+DBgzFq1CicPn0aAwYMwIABA3D+/Hm9PtePizISERHRv2SCIAhiFhAcHIyOHTti+fLlAACNRgNvb29MmDAB06dPr3D+oEGDUFBQgJ07d2rbOnfujHbt2mHVqlWP/DylUgl7e3ss2Xka4X3bGez3ICIiIuO5+/2dm5sLOzs7g7+/qKOKVSoVYmJiMGPGDG2biYkJQkNDERUVVek1UVFRCA8P12nr1asXtm3bVun5JSUlKCkp0b7Ozc0FANiblkGpVD7mb0BEREQ14e53trH6cUQNRJmZmVCr1XBzc9Npd3NzQ1xcXKXXpKamVnp+ampqpedHRERg7ty5FdpH9+6I0dWsm4iIiMSRlZUFe3t7g79vvZ93PmPGDJ0epZycHDRq1AhJSUlG+YNS1SmVSnh7eyM5Odko3Z+kH96P2oP3ovbgvag9cnNz4ePjA0dHR6O8v6iByNnZGXK5HGlpaTrtaWlpcHd3r/Qad3d3vc5XKBRQKCruWWZvb8//ctcSdnZ2vBe1CO9H7cF7UXvwXtQeJibGmQ8m6iwzc3NzdOjQAZGRkdo2jUaDyMhIhISEVHpNSEiIzvkAsG/fvgeeT0RERPQooj8yCw8Px/DhwxEUFIROnTph2bJlKCgowIgRIwAAYWFh8PLyQkREBABg0qRJ6NatG5YsWYK+fftiw4YNiI6Oxrfffivmr0FERER1mOiBaNCgQcjIyMCsWbOQmpqKdu3aYffu3dqB00lJSTrdY126dMH69evx0Ucf4YMPPkCzZs2wbds2tGnTpkqfp1AoMHv27Eofo1HN4r2oXXg/ag/ei9qD96L2MPa9EH0dIiIiIiKxib5SNREREZHYGIiIiIhI8hiIiIiISPIYiIiIiEjyJBeIVqxYAV9fX1hYWCA4OBgnTpwQu6R6LyIiAh07doStrS1cXV0xYMAAxMfH65xTXFyMcePGwcnJCTY2Nnj55ZcrLMBJhvfpp59CJpNh8uTJ2jbei5qTkpKC119/HU5OTrC0tIS/vz+io6O1xwVBwKxZs+Dh4QFLS0uEhobiypUrIlZcP6nVasycORN+fn6wtLREkyZNMH/+fJ09s3gvjOfw4cPo168fPD09IZPJKuxNWpW/fXZ2NoYOHQo7Ozs4ODhg1KhRyM/P16sOSQWijRs3Ijw8HLNnz8apU6cQEBCAXr16IT09XezS6rVDhw5h3Lhx+Oeff7Bv3z6UlpaiZ8+eKCgo0J4zZcoU/P777/j1119x6NAh3Lp1Cy+99JKIVdd/J0+exDfffIO2bdvqtPNe1Iw7d+6ga9euMDMzw59//omLFy9iyZIlaNCggfacRYsW4csvv8SqVatw/PhxWFtbo1evXiguLhax8vpn4cKFWLlyJZYvX45Lly5h4cKFWLRoEb766ivtObwXxlNQUICAgACsWLGi0uNV+dsPHToUFy5cwL59+7Bz504cPnwYY8eO1a8QQUI6deokjBs3TvtarVYLnp6eQkREhIhVSU96eroAQDh06JAgCIKQk5MjmJmZCb/++qv2nEuXLgkAhKioKLHKrNfy8vKEZs2aCfv27RO6desmTJo0SRAE3ouaNG3aNOHJJ5984HGNRiO4u7sLixcv1rbl5OQICoVC+OWXX2qiRMno27evMHLkSJ22l156SRg6dKggCLwXNQmAsHXrVu3rqvztL168KAAQTp48qT3nzz//FGQymZCSklLlz5ZMD5FKpUJMTAxCQ0O1bSYmJggNDUVUVJSIlUlPbm4uAGg36IuJiUFpaanOvWnZsiV8fHx4b4xk3Lhx6Nu3r87fHOC9qEk7duxAUFAQBg4cCFdXVwQGBuK7777THk9MTERqaqrOvbC3t0dwcDDvhYF16dIFkZGRuHz5MgDgzJkzOHLkCJ5//nkAvBdiqsrfPioqCg4ODggKCtKeExoaChMTExw/frzKnyX6StU1JTMzE2q1WrsC9l1ubm6Ii4sTqSrp0Wg0mDx5Mrp27apdXTw1NRXm5uZwcHDQOdfNzQ2pqakiVFm/bdiwAadOncLJkycrHOO9qDkJCQlYuXIlwsPD8cEHH+DkyZOYOHEizM3NMXz4cO3fu7J/s3gvDGv69OlQKpVo2bIl5HI51Go1FixYgKFDhwIA74WIqvK3T01Nhaurq85xU1NTODo66nV/JBOIqHYYN24czp8/jyNHjohdiiQlJydj0qRJ2LdvHywsLMQuR9I0Gg2CgoLwySefAAACAwNx/vx5rFq1CsOHDxe5OmnZtGkT1q1bh/Xr16N169aIjY3F5MmT4enpyXshIZJ5ZObs7Ay5XF5htkxaWhrc3d1Fqkpaxo8fj507d+LAgQNo2LChtt3d3R0qlQo5OTk65/PeGF5MTAzS09PRvn17mJqawtTUFIcOHcKXX34JU1NTuLm58V7UEA8PD7Rq1Uqn7YknnkBSUhIAaP/e/DfL+N577z1Mnz4dr732Gvz9/TFs2DBMmTJFu6k474V4qvK3d3d3rzA5qqysDNnZ2XrdH8kEInNzc3To0AGRkZHaNo1Gg8jISISEhIhYWf0nCALGjx+PrVu3Yv/+/fDz89M53qFDB5iZmencm/j4eCQlJfHeGFiPHj1w7tw5xMbGan+CgoIwdOhQ7X/mvagZXbt2rbD8xOXLl9GoUSMAgJ+fH9zd3XXuhVKpxPHjx3kvDKywsFBnE3EAkMvl0Gg0AHgvxFSVv31ISAhycnIQExOjPWf//v3QaDQIDg6u+oc99pDwOmTDhg2CQqEQ1qxZI1y8eFEYO3as4ODgIKSmpopdWr329ttvC/b29sLBgweF27dva38KCwu157z11luCj4+PsH//fiE6OloICQkRQkJCRKxaOu6dZSYIvBc15cSJE4KpqamwYMEC4cqVK8K6desEKysr4eeff9ae8+mnnwoODg7C9u3bhbNnzwr9+/cX/Pz8hKKiIhErr3+GDx8ueHl5CTt37hQSExOFLVu2CM7OzsL777+vPYf3wnjy8vKE06dPC6dPnxYACEuXLhVOnz4t3LhxQxCEqv3te/fuLQQGBgrHjx8Xjhw5IjRr1kwYPHiwXnVIKhAJgiB89dVXgo+Pj2Bubi506tRJ+Oeff8Quqd4DUOnP6tWrtecUFRUJ77zzjtCgQQPByspKePHFF4Xbt2+LV7SE3B+IeC9qzu+//y60adNGUCgUQsuWLYVvv/1W57hGoxFmzpwpuLm5CQqFQujRo4cQHx8vUrX1l1KpFCZNmiT4+PgIFhYWQuPGjYUPP/xQKCkp0Z7De2E8Bw4cqPQ7Yvjw4YIgVO1vn5WVJQwePFiwsbER7OzshBEjRgh5eXl61SEThHuW4iQiIiKSIMmMISIiIiJ6EAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiKiGpCVlQVXV1dcv3692u+RmZkJV1dX3Lx503CFEREABiIiyXjjjTcgk8kgk8lgbm6Opk2bYt68eSgrKxO7tEeSyWTYtm1blc6r7GfDhg3GL/IRFixYgP79+8PX1xcAkJ2djX79+sHGxgaBgYE4ffq0zvnjxo3DkiVLdNqcnZ0RFhaG2bNn11TZRJLBQEQkIb1798bt27dx5coVTJ06FXPmzMHixYur9V5qtVq7G3htsnr1aty+fVvnZ8CAAZWe+6DfQaVSVeuzH3RdYWEhfvjhB4waNUrbtmDBAuTl5eHUqVPo3r07xowZoz32zz//4Pjx45g8eXKF9xoxYgTWrVuH7OzsatVIRJVjICKSEIVCAXd3dzRq1Ahvv/02QkNDsWPHDgBASUkJ3n33XXh5ecHa2hrBwcE4ePCg9to1a9bAwcEBO3bsQKtWraBQKJCUlISSkhJMmzYN3t7eUCgUaNq0KX744QftdefPn8fzzz8PGxsbuLm5YdiwYcjMzNQe7969OyZOnIj3338fjo6OcHd3x5w5c7TH7/aovPjii5DJZNrXD+Lg4AB3d3edHwsLi4f+Dr6+vpg/fz7CwsJgZ2eHsWPHAgB+++03tG7dGgqFAr6+vhV6bB503f127doFhUKBzp07a9suXbqE1157Dc2bN8fYsWNx6dIlAEBpaSneeustrFq1CnK5vMJ7tW7dGp6enti6detD/w5EpB8GIiIJs7S01PZqjB8/HlFRUdiwYQPOnj2LgQMHonfv3rhy5Yr2/MLCQixcuBDff/89Lly4AFdXV4SFheGXX37Bl19+iUuXLuGbb76BjY0NACAnJwfPPvssAgMDER0djd27dyMtLQ2vvvqqTh0//fQTrK2tcfz4cSxatAjz5s3Dvn37AAAnT54E8F/Pz93X1VXZ7wAAn332GQICAnD69GnMnDkTMTExePXVV/Haa6/h3LlzmDNnDmbOnIk1a9bovN/911Xm77//RocOHXTaAgICsH//fpSVlWHPnj1o27YtAGDRokXo3r07goKCHvg7dOrUCX///fdj/BWIqAKBiCRh+PDhQv/+/QVBEASNRiPs27dPUCgUwrvvvivcuHFDkMvlQkpKis41PXr0EGbMmCEIgiCsXr1aACDExsZqj8fHxwsAhH379lX6mfPnzxd69uyp05acnCwAEOLj4wVBEIRu3boJTz75pM45HTt2FKZNm6Z9DUDYunXrI39HAIKFhYVgbW2t83Pjxo0H/g6CIAiNGjUSBgwYoNM2ZMgQ4bnnntNpe++994RWrVo99LrK9O/fXxg5cqROW05OjjB48GDBx8dHePrpp4ULFy4Ily9fFpo1ayZkZmYKb775puDn5ycMHDhQyMnJ0bl2ypQpQvfu3R/5uURUdaZihjEiqlk7d+6EjY0NSktLodFoMGTIEMyZMwcHDx6EWq1G8+bNdc4vKSmBk5OT9rW5ubm2JwMAYmNjIZfL0a1bt0o/78yZMzhw4IC2x+he165d037eve8JAB4eHkhPT6/W7/j5558jNDRUp83T0/OBv8Nd9/fIXLp0Cf3799dp69q1K5YtWwa1Wq19nPWwnpy7ioqKtI/t7rK3t8f69et12p599lksXrwY69atQ0JCAuLj4zFmzBjMmzdP53GdpaUlCgsLH/m5RFR1DEREEvLMM89g5cqVMDc3h6enJ0xNy/8JyM/Ph1wuR0xMTIVxK/eGGUtLS8hkMp3XD5Ofn49+/fph4cKFFY55eHho/7OZmZnOMZlMVu0B2+7u7mjatOkDj9//O9xlbW1drc+rynXOzs64c+fOQ89ZvXo1HBwc0L9/f7z00ksYMGAAzMzMMHDgQMyaNUvn3OzsbLi4uFSrXiKqHAMRkYRYW1tXGhYCAwOhVquRnp6Op556qsrv5+/vD41Gg0OHDlXolQGA9u3b47fffoOvr682fFWHmZkZ1Gp1ta+vjieeeAJHjx7VaTt69CiaN29e6WDnhwkMDMTPP//8wOMZGRmYN28ejhw5AqB89ltpaSmA8kHW9//u58+fR/fu3fWqgYgejoOqiQjNmzfH0KFDERYWhi1btiAxMREnTpxAREQE/vjjjwde5+vri+HDh2PkyJHYtm0bEhMTcfDgQWzatAlA+Vo62dnZGDx4ME6ePIlr165hz549GDFihF4Bx9fXF5GRkUhNTX1kT0tOTg5SU1N1fgoKCqr8WXdNnToVkZGRmD9/Pi5fvoyffvoJy5cvx7vvvqv3e/Xq1QsXLlx4YO2TJ0/G1KlT4eXlBaD80dz//d//4dKlS/j222/RtWtX7bmFhYWIiYlBz5499a6DiB6MgYiIAJQ/sgkLC8PUqVPRokULDBgwACdPnoSPj89Dr1u5ciVeeeUVvPPOO2jZsiXGjBmjDSCenp44evQo1Go1evbsCX9/f0yePBkODg4wMan6Pz9LlizBvn374O3tjcDAwIeeO2LECHh4eOj8fPXVV1X+rLvat2+PTZs2YcOGDWjTpg1mzZqFefPm4Y033tD7vfz9/bXvd789e/bg6tWreOedd7Rt48ePR+PGjREcHAyVSqWzEOP27dvh4+OjV08eET2aTBAEQewiiIjquz/++APvvfcezp8/r1cYvF/nzp0xceJEDBkyxIDVERHHEBER1YC+ffviypUrSElJgbe3d7XeIzMzEy+99BIGDx5s4OqIiD1EREREJHkcQ0RERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJL3/yU8Qx7brt4EAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "ax.ecdf(percent_error, label='Percent Error')\n",
    "ax.set_xlabel('Percent Error (%)')\n",
    "ax.set_ylabel('Proportion of Samples')\n",
    "plt.xlim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8760c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_vals, inv = np.unique(scaler.inverse_transform(X_test)[::, 0], return_inverse=True)\n",
    "ax = plt.axes()\n",
    "ax.bar(k_vals * WINDOW_SIZE_MS/1000.0, np.bincount(inv, weights=percent_error) / np.bincount(inv), width=WINDOW_SIZE_MS/1000.0, align='edge')\n",
    "ax.set_xlabel('Elapsed Time (s)')\n",
    "ax.set_ylabel('Mean Percent Error (%)')\n",
    "ax.set_ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b816634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training dataset using ThroughputPredictor(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=47, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ") as expert model\n",
      "Expert model score: 0.9967545879860044\n",
      "Initializing Trustee outer-loop with 10 iterations\n",
      "########## Outer-loop Iteration 0/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (15773, 15773) entries\n",
      "Student model 0-0 trained with depth 29 and 3310 leaves:\n",
      "Student model score: 0.9850587442057595\n",
      "Student model 0-0 fidelity: 0.985058744388269\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (17193, 17193) entries\n",
      "Student model 0-1 trained with depth 33 and 3256 leaves:\n",
      "Student model score: 0.9941795956708802\n",
      "Student model 0-1 fidelity: 0.9941795949092133\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (18613, 18613) entries\n",
      "Student model 0-2 trained with depth 31 and 3205 leaves:\n",
      "Student model score: 0.9978424230791846\n",
      "Student model 0-2 fidelity: 0.9978424226312723\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (20033, 20033) entries\n",
      "Student model 0-3 trained with depth 34 and 3183 leaves:\n",
      "Student model score: 0.9887546374297457\n",
      "Student model 0-3 fidelity: 0.9887546375743014\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (21453, 21453) entries\n",
      "Student model 0-4 trained with depth 37 and 3152 leaves:\n",
      "Student model score: 0.9977804495053209\n",
      "Student model 0-4 fidelity: 0.9977804492131063\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (22873, 22873) entries\n",
      "Student model 0-5 trained with depth 32 and 3099 leaves:\n",
      "Student model score: 0.9934051459886387\n",
      "Student model 0-5 fidelity: 0.9934051461779408\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (24293, 24293) entries\n",
      "Student model 0-6 trained with depth 36 and 3091 leaves:\n",
      "Student model score: 0.997581528522117\n",
      "Student model 0-6 fidelity: 0.9975815290581609\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (25713, 25713) entries\n",
      "Student model 0-7 trained with depth 33 and 3082 leaves:\n",
      "Student model score: 0.9948528202281651\n",
      "Student model 0-7 fidelity: 0.9948528201542467\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (27133, 27133) entries\n",
      "Student model 0-8 trained with depth 35 and 3056 leaves:\n",
      "Student model score: 0.9740368016667419\n",
      "Student model 0-8 fidelity: 0.9740368011173779\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (28553, 28553) entries\n",
      "Student model 0-9 trained with depth 30 and 3026 leaves:\n",
      "Student model score: 0.9973658389309126\n",
      "Student model 0-9 fidelity: 0.9973658389824168\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (29973, 29973) entries\n",
      "Student model 0-10 trained with depth 31 and 3017 leaves:\n",
      "Student model score: 0.9981422957117317\n",
      "Student model 0-10 fidelity: 0.9981422954670146\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (31393, 31393) entries\n",
      "Student model 0-11 trained with depth 30 and 3014 leaves:\n",
      "Student model score: 0.995264697464563\n",
      "Student model 0-11 fidelity: 0.9952646973715208\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (32813, 32813) entries\n",
      "Student model 0-12 trained with depth 30 and 2976 leaves:\n",
      "Student model score: 0.9934160121618829\n",
      "Student model 0-12 fidelity: 0.9934160120845071\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (34233, 34233) entries\n",
      "Student model 0-13 trained with depth 37 and 2966 leaves:\n",
      "Student model score: 0.9932405463043599\n",
      "Student model 0-13 fidelity: 0.9932405462703029\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (35653, 35653) entries\n",
      "Student model 0-14 trained with depth 34 and 3008 leaves:\n",
      "Student model score: 0.9950685059637011\n",
      "Student model 0-14 fidelity: 0.9950685058405966\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (37073, 37073) entries\n",
      "Student model 0-15 trained with depth 31 and 2979 leaves:\n",
      "Student model score: 0.9948454614320048\n",
      "Student model 0-15 fidelity: 0.9948454613715152\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (38493, 38493) entries\n",
      "Student model 0-16 trained with depth 32 and 2958 leaves:\n",
      "Student model score: 0.9928217435324005\n",
      "Student model 0-16 fidelity: 0.9928217442104339\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (39913, 39913) entries\n",
      "Student model 0-17 trained with depth 32 and 2932 leaves:\n",
      "Student model score: 0.9975513328153018\n",
      "Student model 0-17 fidelity: 0.997551333350122\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (41333, 41333) entries\n",
      "Student model 0-18 trained with depth 30 and 2961 leaves:\n",
      "Student model score: 0.996074903301711\n",
      "Student model 0-18 fidelity: 0.9960749035490625\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (42753, 42753) entries\n",
      "Student model 0-19 trained with depth 30 and 2950 leaves:\n",
      "Student model score: 0.9982517213944565\n",
      "Student model 0-19 fidelity: 0.9982517212133576\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (44173, 44173) entries\n",
      "Student model 0-20 trained with depth 32 and 2909 leaves:\n",
      "Student model score: 0.9949229654018635\n",
      "Student model 0-20 fidelity: 0.9949229656216653\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (45593, 45593) entries\n",
      "Student model 0-21 trained with depth 31 and 2900 leaves:\n",
      "Student model score: 0.9980887101718028\n",
      "Student model 0-21 fidelity: 0.9980887102507475\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (47013, 47013) entries\n",
      "Student model 0-22 trained with depth 31 and 2937 leaves:\n",
      "Student model score: 0.983361476626649\n",
      "Student model 0-22 fidelity: 0.983361476594955\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (48433, 48433) entries\n",
      "Student model 0-23 trained with depth 32 and 2926 leaves:\n",
      "Student model score: 0.9953967156751915\n",
      "Student model 0-23 fidelity: 0.9953967158045742\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (49853, 49853) entries\n",
      "Student model 0-24 trained with depth 31 and 2943 leaves:\n",
      "Student model score: 0.9982124236429963\n",
      "Student model 0-24 fidelity: 0.9982124232100864\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (51273, 51273) entries\n",
      "Student model 0-25 trained with depth 35 and 2908 leaves:\n",
      "Student model score: 0.9988921680532301\n",
      "Student model 0-25 fidelity: 0.998892168078639\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (52693, 52693) entries\n",
      "Student model 0-26 trained with depth 33 and 2869 leaves:\n",
      "Student model score: 0.9972484421580695\n",
      "Student model 0-26 fidelity: 0.9972484423653063\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (54113, 54113) entries\n",
      "Student model 0-27 trained with depth 32 and 2926 leaves:\n",
      "Student model score: 0.9950161148263311\n",
      "Student model 0-27 fidelity: 0.9950161142660808\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (55533, 55533) entries\n",
      "Student model 0-28 trained with depth 35 and 2872 leaves:\n",
      "Student model score: 0.9979566280373527\n",
      "Student model 0-28 fidelity: 0.9979566279108427\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (56953, 56953) entries\n",
      "Student model 0-29 trained with depth 31 and 2868 leaves:\n",
      "Student model score: 0.9984761049152704\n",
      "Student model 0-29 fidelity: 0.9984761048197769\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (58373, 58373) entries\n",
      "Student model 0-30 trained with depth 33 and 2886 leaves:\n",
      "Student model score: 0.9944197761762866\n",
      "Student model 0-30 fidelity: 0.9944197760677832\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (59793, 59793) entries\n",
      "Student model 0-31 trained with depth 29 and 2896 leaves:\n",
      "Student model score: 0.9980172999956628\n",
      "Student model 0-31 fidelity: 0.9980172999332624\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (61213, 61213) entries\n",
      "Student model 0-32 trained with depth 30 and 2864 leaves:\n",
      "Student model score: 0.9952847327843295\n",
      "Student model 0-32 fidelity: 0.9952847325608054\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (62633, 62633) entries\n",
      "Student model 0-33 trained with depth 28 and 2895 leaves:\n",
      "Student model score: 0.9984997118515501\n",
      "Student model 0-33 fidelity: 0.9984997118179154\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (64053, 64053) entries\n",
      "Student model 0-34 trained with depth 32 and 2882 leaves:\n",
      "Student model score: 0.995744986199594\n",
      "Student model 0-34 fidelity: 0.9957449864788731\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (65473, 65473) entries\n",
      "Student model 0-35 trained with depth 30 and 2880 leaves:\n",
      "Student model score: 0.9950261596454946\n",
      "Student model 0-35 fidelity: 0.9950261595398522\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (66893, 66893) entries\n",
      "Student model 0-36 trained with depth 29 and 2876 leaves:\n",
      "Student model score: 0.9984145678902837\n",
      "Student model 0-36 fidelity: 0.9984145677079448\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (68313, 68313) entries\n",
      "Student model 0-37 trained with depth 33 and 2850 leaves:\n",
      "Student model score: 0.9953926041640617\n",
      "Student model 0-37 fidelity: 0.9953926042076291\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (69733, 69733) entries\n",
      "Student model 0-38 trained with depth 34 and 2870 leaves:\n",
      "Student model score: 0.9927693175513415\n",
      "Student model 0-38 fidelity: 0.9927693177297616\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (71153, 71153) entries\n",
      "Student model 0-39 trained with depth 30 and 2848 leaves:\n",
      "Student model score: 0.9881737594705571\n",
      "Student model 0-39 fidelity: 0.9881737596196095\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (72573, 72573) entries\n",
      "Student model 0-40 trained with depth 32 and 2880 leaves:\n",
      "Student model score: 0.9948764213140798\n",
      "Student model 0-40 fidelity: 0.9948764211431894\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (73993, 73993) entries\n",
      "Student model 0-41 trained with depth 29 and 2867 leaves:\n",
      "Student model score: 0.9919195091232457\n",
      "Student model 0-41 fidelity: 0.9919195091840073\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (75413, 75413) entries\n",
      "Student model 0-42 trained with depth 29 and 2893 leaves:\n",
      "Student model score: 0.9918887967991046\n",
      "Student model 0-42 fidelity: 0.9918887959088449\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (76833, 76833) entries\n",
      "Student model 0-43 trained with depth 30 and 2829 leaves:\n",
      "Student model score: 0.9986434075482372\n",
      "Student model 0-43 fidelity: 0.9986434075622288\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (78253, 78253) entries\n",
      "Student model 0-44 trained with depth 30 and 2852 leaves:\n",
      "Student model score: 0.9973348628969819\n",
      "Student model 0-44 fidelity: 0.997334862533907\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (79673, 79673) entries\n",
      "Student model 0-45 trained with depth 29 and 2880 leaves:\n",
      "Student model score: 0.9985864205293343\n",
      "Student model 0-45 fidelity: 0.9985864203300121\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (81093, 81093) entries\n",
      "Student model 0-46 trained with depth 27 and 2851 leaves:\n",
      "Student model score: 0.9913472901500334\n",
      "Student model 0-46 fidelity: 0.9913472902360216\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (82513, 82513) entries\n",
      "Student model 0-47 trained with depth 35 and 2825 leaves:\n",
      "Student model score: 0.99853035060714\n",
      "Student model 0-47 fidelity: 0.9985303504335533\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (83933, 83933) entries\n",
      "Student model 0-48 trained with depth 32 and 2852 leaves:\n",
      "Student model score: 0.9970345098938967\n",
      "Student model 0-48 fidelity: 0.9970345096074513\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (85353, 85353) entries\n",
      "Student model 0-49 trained with depth 35 and 2846 leaves:\n",
      "Student model score: 0.9907059135347215\n",
      "Student model 0-49 fidelity: 0.9907059134954344\n",
      "########## Outer-loop Iteration 1/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (86773, 86773) entries\n",
      "Student model 1-0 trained with depth 30 and 2813 leaves:\n",
      "Student model score: 0.9990041181559481\n",
      "Student model 1-0 fidelity: 0.9990041179795034\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (88193, 88193) entries\n",
      "Student model 1-1 trained with depth 32 and 2837 leaves:\n",
      "Student model score: 0.9989260276435797\n",
      "Student model 1-1 fidelity: 0.9989260274982713\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (89613, 89613) entries\n",
      "Student model 1-2 trained with depth 31 and 2820 leaves:\n",
      "Student model score: 0.9986424618392067\n",
      "Student model 1-2 fidelity: 0.998642461896351\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (91033, 91033) entries\n",
      "Student model 1-3 trained with depth 30 and 2846 leaves:\n",
      "Student model score: 0.9888357603779268\n",
      "Student model 1-3 fidelity: 0.9888357604422051\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (92453, 92453) entries\n",
      "Student model 1-4 trained with depth 31 and 2854 leaves:\n",
      "Student model score: 0.9905681939940446\n",
      "Student model 1-4 fidelity: 0.99056819384948\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (93873, 93873) entries\n",
      "Student model 1-5 trained with depth 30 and 2792 leaves:\n",
      "Student model score: 0.9970813849840242\n",
      "Student model 1-5 fidelity: 0.9970813850353789\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (95293, 95293) entries\n",
      "Student model 1-6 trained with depth 31 and 2859 leaves:\n",
      "Student model score: 0.9983412916400672\n",
      "Student model 1-6 fidelity: 0.9983412915155776\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (96713, 96713) entries\n",
      "Student model 1-7 trained with depth 33 and 2807 leaves:\n",
      "Student model score: 0.998387793858979\n",
      "Student model 1-7 fidelity: 0.9983877937345343\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (98133, 98133) entries\n",
      "Student model 1-8 trained with depth 32 and 2852 leaves:\n",
      "Student model score: 0.9960897387580708\n",
      "Student model 1-8 fidelity: 0.9960897387978829\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (99553, 99553) entries\n",
      "Student model 1-9 trained with depth 28 and 2838 leaves:\n",
      "Student model score: 0.9934242383822312\n",
      "Student model 1-9 fidelity: 0.9934242382371619\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (100973, 100973) entries\n",
      "Student model 1-10 trained with depth 34 and 2844 leaves:\n",
      "Student model score: 0.9841483580526471\n",
      "Student model 1-10 fidelity: 0.9841483581726026\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (102393, 102393) entries\n",
      "Student model 1-11 trained with depth 35 and 2825 leaves:\n",
      "Student model score: 0.9953269033433056\n",
      "Student model 1-11 fidelity: 0.9953269034528345\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (103813, 103813) entries\n",
      "Student model 1-12 trained with depth 38 and 2819 leaves:\n",
      "Student model score: 0.9923105673634737\n",
      "Student model 1-12 fidelity: 0.9923105673527397\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (105233, 105233) entries\n",
      "Student model 1-13 trained with depth 33 and 2831 leaves:\n",
      "Student model score: 0.9973095509825968\n",
      "Student model 1-13 fidelity: 0.9973095510271686\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (106653, 106653) entries\n",
      "Student model 1-14 trained with depth 34 and 2824 leaves:\n",
      "Student model score: 0.9979887005199561\n",
      "Student model 1-14 fidelity: 0.9979887006116409\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (108073, 108073) entries\n",
      "Student model 1-15 trained with depth 29 and 2832 leaves:\n",
      "Student model score: 0.9982326871485854\n",
      "Student model 1-15 fidelity: 0.9982326871058724\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (109493, 109493) entries\n",
      "Student model 1-16 trained with depth 40 and 2822 leaves:\n",
      "Student model score: 0.9959646991528794\n",
      "Student model 1-16 fidelity: 0.995964699104932\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (110913, 110913) entries\n",
      "Student model 1-17 trained with depth 31 and 2802 leaves:\n",
      "Student model score: 0.9915137323998914\n",
      "Student model 1-17 fidelity: 0.99151373244063\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (112333, 112333) entries\n",
      "Student model 1-18 trained with depth 31 and 2797 leaves:\n",
      "Student model score: 0.9977179368324028\n",
      "Student model 1-18 fidelity: 0.9977179365812884\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (113753, 113753) entries\n",
      "Student model 1-19 trained with depth 34 and 2798 leaves:\n",
      "Student model score: 0.9984943587433186\n",
      "Student model 1-19 fidelity: 0.9984943587404193\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (115173, 115173) entries\n",
      "Student model 1-20 trained with depth 33 and 2850 leaves:\n",
      "Student model score: 0.9951686518279199\n",
      "Student model 1-20 fidelity: 0.9951686519049201\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (116593, 116593) entries\n",
      "Student model 1-21 trained with depth 30 and 2837 leaves:\n",
      "Student model score: 0.9980693147582376\n",
      "Student model 1-21 fidelity: 0.9980693148766075\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (118013, 118013) entries\n",
      "Student model 1-22 trained with depth 32 and 2815 leaves:\n",
      "Student model score: 0.99809747112428\n",
      "Student model 1-22 fidelity: 0.998097471149482\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (119433, 119433) entries\n",
      "Student model 1-23 trained with depth 32 and 2833 leaves:\n",
      "Student model score: 0.9921996708296558\n",
      "Student model 1-23 fidelity: 0.9921996705823716\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (120853, 120853) entries\n",
      "Student model 1-24 trained with depth 31 and 2797 leaves:\n",
      "Student model score: 0.9979901322795406\n",
      "Student model 1-24 fidelity: 0.9979901322811312\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (122273, 122273) entries\n",
      "Student model 1-25 trained with depth 34 and 2829 leaves:\n",
      "Student model score: 0.9958036119840804\n",
      "Student model 1-25 fidelity: 0.9958036118327177\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (123693, 123693) entries\n",
      "Student model 1-26 trained with depth 29 and 2795 leaves:\n",
      "Student model score: 0.994944365399973\n",
      "Student model 1-26 fidelity: 0.9949443651099917\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (125113, 125113) entries\n",
      "Student model 1-27 trained with depth 31 and 2815 leaves:\n",
      "Student model score: 0.9942782471909893\n",
      "Student model 1-27 fidelity: 0.9942782471809256\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (126533, 126533) entries\n",
      "Student model 1-28 trained with depth 30 and 2802 leaves:\n",
      "Student model score: 0.9902184898719072\n",
      "Student model 1-28 fidelity: 0.9902184896895092\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (127953, 127953) entries\n",
      "Student model 1-29 trained with depth 35 and 2816 leaves:\n",
      "Student model score: 0.9946815387462304\n",
      "Student model 1-29 fidelity: 0.9946815387590565\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (129373, 129373) entries\n",
      "Student model 1-30 trained with depth 33 and 2829 leaves:\n",
      "Student model score: 0.9930808148990002\n",
      "Student model 1-30 fidelity: 0.9930808148005356\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (130793, 130793) entries\n",
      "Student model 1-31 trained with depth 29 and 2762 leaves:\n",
      "Student model score: 0.9975164974774889\n",
      "Student model 1-31 fidelity: 0.9975164974537976\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (132213, 132213) entries\n",
      "Student model 1-32 trained with depth 31 and 2825 leaves:\n",
      "Student model score: 0.9942150696135329\n",
      "Student model 1-32 fidelity: 0.9942150697491399\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (133633, 133633) entries\n",
      "Student model 1-33 trained with depth 30 and 2792 leaves:\n",
      "Student model score: 0.9940948501410737\n",
      "Student model 1-33 fidelity: 0.9940948497577539\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (135053, 135053) entries\n",
      "Student model 1-34 trained with depth 30 and 2804 leaves:\n",
      "Student model score: 0.9980162232049067\n",
      "Student model 1-34 fidelity: 0.9980162233067111\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (136473, 136473) entries\n",
      "Student model 1-35 trained with depth 33 and 2823 leaves:\n",
      "Student model score: 0.9950248482771183\n",
      "Student model 1-35 fidelity: 0.9950248484377184\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (137893, 137893) entries\n",
      "Student model 1-36 trained with depth 29 and 2816 leaves:\n",
      "Student model score: 0.9982509862624287\n",
      "Student model 1-36 fidelity: 0.9982509861553572\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (139313, 139313) entries\n",
      "Student model 1-37 trained with depth 29 and 2802 leaves:\n",
      "Student model score: 0.9978247800565536\n",
      "Student model 1-37 fidelity: 0.9978247800933672\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (140733, 140733) entries\n",
      "Student model 1-38 trained with depth 32 and 2796 leaves:\n",
      "Student model score: 0.9978748830400235\n",
      "Student model 1-38 fidelity: 0.99787488292941\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (142153, 142153) entries\n",
      "Student model 1-39 trained with depth 28 and 2823 leaves:\n",
      "Student model score: 0.9970147356844373\n",
      "Student model 1-39 fidelity: 0.9970147357310175\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (143573, 143573) entries\n",
      "Student model 1-40 trained with depth 32 and 2811 leaves:\n",
      "Student model score: 0.9985064650451644\n",
      "Student model 1-40 fidelity: 0.9985064650200876\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (144993, 144993) entries\n",
      "Student model 1-41 trained with depth 33 and 2813 leaves:\n",
      "Student model score: 0.9982911440178656\n",
      "Student model 1-41 fidelity: 0.9982911443156153\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (146413, 146413) entries\n",
      "Student model 1-42 trained with depth 31 and 2763 leaves:\n",
      "Student model score: 0.9944888581961281\n",
      "Student model 1-42 fidelity: 0.9944888581836021\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (147833, 147833) entries\n",
      "Student model 1-43 trained with depth 27 and 2842 leaves:\n",
      "Student model score: 0.9982631937778247\n",
      "Student model 1-43 fidelity: 0.9982631937864148\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (149253, 149253) entries\n",
      "Student model 1-44 trained with depth 32 and 2804 leaves:\n",
      "Student model score: 0.9976415150982842\n",
      "Student model 1-44 fidelity: 0.9976415152376712\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (150673, 150673) entries\n",
      "Student model 1-45 trained with depth 32 and 2846 leaves:\n",
      "Student model score: 0.9987660790633757\n",
      "Student model 1-45 fidelity: 0.9987660790325906\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (152093, 152093) entries\n",
      "Student model 1-46 trained with depth 33 and 2791 leaves:\n",
      "Student model score: 0.9951123309396177\n",
      "Student model 1-46 fidelity: 0.9951123308426296\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (153513, 153513) entries\n",
      "Student model 1-47 trained with depth 31 and 2806 leaves:\n",
      "Student model score: 0.998751939922528\n",
      "Student model 1-47 fidelity: 0.9987519399726548\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (154933, 154933) entries\n",
      "Student model 1-48 trained with depth 37 and 2806 leaves:\n",
      "Student model score: 0.9976365016185351\n",
      "Student model 1-48 fidelity: 0.9976365015803139\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (156353, 156353) entries\n",
      "Student model 1-49 trained with depth 32 and 2816 leaves:\n",
      "Student model score: 0.9953874329490767\n",
      "Student model 1-49 fidelity: 0.9953874328980095\n",
      "########## Outer-loop Iteration 2/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (157773, 157773) entries\n",
      "Student model 2-0 trained with depth 33 and 2820 leaves:\n",
      "Student model score: 0.9946385020385858\n",
      "Student model 2-0 fidelity: 0.9946385019153672\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (159193, 159193) entries\n",
      "Student model 2-1 trained with depth 32 and 2820 leaves:\n",
      "Student model score: 0.9913125231595079\n",
      "Student model 2-1 fidelity: 0.9913125232654815\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (160613, 160613) entries\n",
      "Student model 2-2 trained with depth 30 and 2806 leaves:\n",
      "Student model score: 0.995111437260402\n",
      "Student model 2-2 fidelity: 0.9951114370876587\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (162033, 162033) entries\n",
      "Student model 2-3 trained with depth 37 and 2801 leaves:\n",
      "Student model score: 0.9985706281060885\n",
      "Student model 2-3 fidelity: 0.9985706281051661\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (163453, 163453) entries\n",
      "Student model 2-4 trained with depth 34 and 2805 leaves:\n",
      "Student model score: 0.9981685242662602\n",
      "Student model 2-4 fidelity: 0.9981685242155558\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (164873, 164873) entries\n",
      "Student model 2-5 trained with depth 32 and 2773 leaves:\n",
      "Student model score: 0.997948752124474\n",
      "Student model 2-5 fidelity: 0.9979487520877585\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (166293, 166293) entries\n",
      "Student model 2-6 trained with depth 32 and 2814 leaves:\n",
      "Student model score: 0.9908621356474473\n",
      "Student model 2-6 fidelity: 0.990862135836128\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (167713, 167713) entries\n",
      "Student model 2-7 trained with depth 28 and 2814 leaves:\n",
      "Student model score: 0.9914923428466873\n",
      "Student model 2-7 fidelity: 0.9914923427914504\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (169133, 169133) entries\n",
      "Student model 2-8 trained with depth 33 and 2773 leaves:\n",
      "Student model score: 0.9974626634713047\n",
      "Student model 2-8 fidelity: 0.9974626634822583\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (170553, 170553) entries\n",
      "Student model 2-9 trained with depth 32 and 2824 leaves:\n",
      "Student model score: 0.9988654691467285\n",
      "Student model 2-9 fidelity: 0.9988654689900892\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (171973, 171973) entries\n",
      "Student model 2-10 trained with depth 29 and 2800 leaves:\n",
      "Student model score: 0.9981845874177413\n",
      "Student model 2-10 fidelity: 0.9981845873518916\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (173393, 173393) entries\n",
      "Student model 2-11 trained with depth 34 and 2798 leaves:\n",
      "Student model score: 0.9983535988433502\n",
      "Student model 2-11 fidelity: 0.9983535988357898\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (174813, 174813) entries\n",
      "Student model 2-12 trained with depth 34 and 2826 leaves:\n",
      "Student model score: 0.9983894066047937\n",
      "Student model 2-12 fidelity: 0.9983894065532823\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (176233, 176233) entries\n",
      "Student model 2-13 trained with depth 30 and 2807 leaves:\n",
      "Student model score: 0.9978419679474931\n",
      "Student model 2-13 fidelity: 0.9978419677420829\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (177653, 177653) entries\n",
      "Student model 2-14 trained with depth 32 and 2787 leaves:\n",
      "Student model score: 0.998177070548124\n",
      "Student model 2-14 fidelity: 0.9981770703429369\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (179073, 179073) entries\n",
      "Student model 2-15 trained with depth 32 and 2808 leaves:\n",
      "Student model score: 0.9911447094619431\n",
      "Student model 2-15 fidelity: 0.9911447094743319\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (180493, 180493) entries\n",
      "Student model 2-16 trained with depth 33 and 2799 leaves:\n",
      "Student model score: 0.9951728610303011\n",
      "Student model 2-16 fidelity: 0.9951728611004244\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (181913, 181913) entries\n",
      "Student model 2-17 trained with depth 35 and 2774 leaves:\n",
      "Student model score: 0.9980714938622598\n",
      "Student model 2-17 fidelity: 0.9980714938647274\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (183333, 183333) entries\n",
      "Student model 2-18 trained with depth 32 and 2812 leaves:\n",
      "Student model score: 0.997718039048963\n",
      "Student model 2-18 fidelity: 0.9977180390091464\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (184753, 184753) entries\n",
      "Student model 2-19 trained with depth 31 and 2806 leaves:\n",
      "Student model score: 0.9985261465307608\n",
      "Student model 2-19 fidelity: 0.9985261463688629\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (186173, 186173) entries\n",
      "Student model 2-20 trained with depth 35 and 2789 leaves:\n",
      "Student model score: 0.9878810447194976\n",
      "Student model 2-20 fidelity: 0.9878810448233277\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (187593, 187593) entries\n",
      "Student model 2-21 trained with depth 30 and 2801 leaves:\n",
      "Student model score: 0.993515510961539\n",
      "Student model 2-21 fidelity: 0.9935155109717408\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (189013, 189013) entries\n",
      "Student model 2-22 trained with depth 31 and 2806 leaves:\n",
      "Student model score: 0.9924949783671819\n",
      "Student model 2-22 fidelity: 0.9924949783711372\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (190433, 190433) entries\n",
      "Student model 2-23 trained with depth 31 and 2812 leaves:\n",
      "Student model score: 0.9939186017003491\n",
      "Student model 2-23 fidelity: 0.9939186015745561\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (191853, 191853) entries\n",
      "Student model 2-24 trained with depth 30 and 2783 leaves:\n",
      "Student model score: 0.998260931061198\n",
      "Student model 2-24 fidelity: 0.9982609312021737\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (193273, 193273) entries\n",
      "Student model 2-25 trained with depth 31 and 2782 leaves:\n",
      "Student model score: 0.9923576497017101\n",
      "Student model 2-25 fidelity: 0.992357649645914\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (194693, 194693) entries\n",
      "Student model 2-26 trained with depth 30 and 2790 leaves:\n",
      "Student model score: 0.9955183108011505\n",
      "Student model 2-26 fidelity: 0.9955183107491161\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (196113, 196113) entries\n",
      "Student model 2-27 trained with depth 31 and 2803 leaves:\n",
      "Student model score: 0.9917534101454303\n",
      "Student model 2-27 fidelity: 0.991753410114465\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (197533, 197533) entries\n",
      "Student model 2-28 trained with depth 27 and 2794 leaves:\n",
      "Student model score: 0.9985509454805965\n",
      "Student model 2-28 fidelity: 0.9985509454849046\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (198953, 198953) entries\n",
      "Student model 2-29 trained with depth 32 and 2818 leaves:\n",
      "Student model score: 0.9976943875980536\n",
      "Student model 2-29 fidelity: 0.9976943875382747\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (200373, 200373) entries\n",
      "Student model 2-30 trained with depth 33 and 2769 leaves:\n",
      "Student model score: 0.9904994892122592\n",
      "Student model 2-30 fidelity: 0.9904994892500465\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (201793, 201793) entries\n",
      "Student model 2-31 trained with depth 33 and 2795 leaves:\n",
      "Student model score: 0.9986960917436828\n",
      "Student model 2-31 fidelity: 0.9986960917349962\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (203213, 203213) entries\n",
      "Student model 2-32 trained with depth 30 and 2791 leaves:\n",
      "Student model score: 0.9873469674421997\n",
      "Student model 2-32 fidelity: 0.9873469674583698\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (204633, 204633) entries\n",
      "Student model 2-33 trained with depth 32 and 2787 leaves:\n",
      "Student model score: 0.9981074139283396\n",
      "Student model 2-33 fidelity: 0.9981074138680278\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (206053, 206053) entries\n",
      "Student model 2-34 trained with depth 31 and 2786 leaves:\n",
      "Student model score: 0.9949897934688583\n",
      "Student model 2-34 fidelity: 0.9949897935659002\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (207473, 207473) entries\n",
      "Student model 2-35 trained with depth 31 and 2790 leaves:\n",
      "Student model score: 0.9946224997492863\n",
      "Student model 2-35 fidelity: 0.9946224996790924\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (208893, 208893) entries\n",
      "Student model 2-36 trained with depth 34 and 2799 leaves:\n",
      "Student model score: 0.9942551973447337\n",
      "Student model 2-36 fidelity: 0.9942551972096701\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (210313, 210313) entries\n",
      "Student model 2-37 trained with depth 32 and 2789 leaves:\n",
      "Student model score: 0.997742983769028\n",
      "Student model 2-37 fidelity: 0.9977429838507674\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (211733, 211733) entries\n",
      "Student model 2-38 trained with depth 37 and 2779 leaves:\n",
      "Student model score: 0.9974472040431628\n",
      "Student model 2-38 fidelity: 0.9974472040267949\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (213153, 213153) entries\n",
      "Student model 2-39 trained with depth 29 and 2775 leaves:\n",
      "Student model score: 0.9923606246728622\n",
      "Student model 2-39 fidelity: 0.9923606246745765\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (214573, 214573) entries\n",
      "Student model 2-40 trained with depth 31 and 2786 leaves:\n",
      "Student model score: 0.9942631417291816\n",
      "Student model 2-40 fidelity: 0.9942631417203166\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (215993, 215993) entries\n",
      "Student model 2-41 trained with depth 34 and 2772 leaves:\n",
      "Student model score: 0.9980077897943955\n",
      "Student model 2-41 fidelity: 0.9980077897470022\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (217413, 217413) entries\n",
      "Student model 2-42 trained with depth 28 and 2789 leaves:\n",
      "Student model score: 0.9969719718120165\n",
      "Student model 2-42 fidelity: 0.9969719717555379\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (218833, 218833) entries\n",
      "Student model 2-43 trained with depth 34 and 2778 leaves:\n",
      "Student model score: 0.9976734783078202\n",
      "Student model 2-43 fidelity: 0.9976734783479131\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (220253, 220253) entries\n",
      "Student model 2-44 trained with depth 31 and 2795 leaves:\n",
      "Student model score: 0.9891625283929705\n",
      "Student model 2-44 fidelity: 0.9891625284960118\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (221673, 221673) entries\n",
      "Student model 2-45 trained with depth 27 and 2790 leaves:\n",
      "Student model score: 0.9917353504356219\n",
      "Student model 2-45 fidelity: 0.9917353501875099\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (223093, 223093) entries\n",
      "Student model 2-46 trained with depth 33 and 2774 leaves:\n",
      "Student model score: 0.9923913812041351\n",
      "Student model 2-46 fidelity: 0.9923913811755392\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (224513, 224513) entries\n",
      "Student model 2-47 trained with depth 31 and 2823 leaves:\n",
      "Student model score: 0.9979793464517269\n",
      "Student model 2-47 fidelity: 0.9979793464506794\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (225933, 225933) entries\n",
      "Student model 2-48 trained with depth 30 and 2805 leaves:\n",
      "Student model score: 0.9969339545912695\n",
      "Student model 2-48 fidelity: 0.9969339544445386\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (227353, 227353) entries\n",
      "Student model 2-49 trained with depth 34 and 2781 leaves:\n",
      "Student model score: 0.990646284545892\n",
      "Student model 2-49 fidelity: 0.9906462845042159\n",
      "########## Outer-loop Iteration 3/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (228773, 228773) entries\n",
      "Student model 3-0 trained with depth 39 and 2776 leaves:\n",
      "Student model score: 0.9956227069326944\n",
      "Student model 3-0 fidelity: 0.995622707013327\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (230193, 230193) entries\n",
      "Student model 3-1 trained with depth 33 and 2793 leaves:\n",
      "Student model score: 0.9925416984017245\n",
      "Student model 3-1 fidelity: 0.9925416984691426\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (231613, 231613) entries\n",
      "Student model 3-2 trained with depth 35 and 2779 leaves:\n",
      "Student model score: 0.9902597618827663\n",
      "Student model 3-2 fidelity: 0.9902597618282197\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (233033, 233033) entries\n",
      "Student model 3-3 trained with depth 30 and 2791 leaves:\n",
      "Student model score: 0.9984857356289782\n",
      "Student model 3-3 fidelity: 0.998485735681512\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (234453, 234453) entries\n",
      "Student model 3-4 trained with depth 35 and 2777 leaves:\n",
      "Student model score: 0.9941252886916383\n",
      "Student model 3-4 fidelity: 0.9941252887006855\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (235873, 235873) entries\n",
      "Student model 3-5 trained with depth 34 and 2778 leaves:\n",
      "Student model score: 0.9948181377522786\n",
      "Student model 3-5 fidelity: 0.9948181372394276\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (237293, 237293) entries\n",
      "Student model 3-6 trained with depth 34 and 2795 leaves:\n",
      "Student model score: 0.9950917106985906\n",
      "Student model 3-6 fidelity: 0.9950917106930055\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (238713, 238713) entries\n",
      "Student model 3-7 trained with depth 35 and 2774 leaves:\n",
      "Student model score: 0.9964965815603338\n",
      "Student model 3-7 fidelity: 0.9964965815552641\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (240133, 240133) entries\n",
      "Student model 3-8 trained with depth 33 and 2791 leaves:\n",
      "Student model score: 0.9958481966051956\n",
      "Student model 3-8 fidelity: 0.9958481965344917\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (241553, 241553) entries\n",
      "Student model 3-9 trained with depth 29 and 2789 leaves:\n",
      "Student model score: 0.9896135467307692\n",
      "Student model 3-9 fidelity: 0.9896135467258562\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (242973, 242973) entries\n",
      "Student model 3-10 trained with depth 30 and 2815 leaves:\n",
      "Student model score: 0.9946825603751586\n",
      "Student model 3-10 fidelity: 0.9946825600681604\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (244393, 244393) entries\n",
      "Student model 3-11 trained with depth 36 and 2794 leaves:\n",
      "Student model score: 0.9925548022824182\n",
      "Student model 3-11 fidelity: 0.9925548023544618\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (245813, 245813) entries\n",
      "Student model 3-12 trained with depth 30 and 2768 leaves:\n",
      "Student model score: 0.9949435828732724\n",
      "Student model 3-12 fidelity: 0.9949435828009611\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (247233, 247233) entries\n",
      "Student model 3-13 trained with depth 32 and 2803 leaves:\n",
      "Student model score: 0.9955113513517537\n",
      "Student model 3-13 fidelity: 0.9955113513544995\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (248653, 248653) entries\n",
      "Student model 3-14 trained with depth 32 and 2773 leaves:\n",
      "Student model score: 0.9895342908529494\n",
      "Student model 3-14 fidelity: 0.9895342908472997\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (250073, 250073) entries\n",
      "Student model 3-15 trained with depth 32 and 2777 leaves:\n",
      "Student model score: 0.9982144417202756\n",
      "Student model 3-15 fidelity: 0.9982144417993682\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (251493, 251493) entries\n",
      "Student model 3-16 trained with depth 35 and 2769 leaves:\n",
      "Student model score: 0.9951516717374818\n",
      "Student model 3-16 fidelity: 0.9951516717785247\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (252913, 252913) entries\n",
      "Student model 3-17 trained with depth 36 and 2791 leaves:\n",
      "Student model score: 0.9983380289126129\n",
      "Student model 3-17 fidelity: 0.9983380288269836\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (254333, 254333) entries\n",
      "Student model 3-18 trained with depth 32 and 2805 leaves:\n",
      "Student model score: 0.984803998432928\n",
      "Student model 3-18 fidelity: 0.9848039983358486\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (255753, 255753) entries\n",
      "Student model 3-19 trained with depth 31 and 2783 leaves:\n",
      "Student model score: 0.9925235610164801\n",
      "Student model 3-19 fidelity: 0.9925235610947952\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (257173, 257173) entries\n",
      "Student model 3-20 trained with depth 29 and 2772 leaves:\n",
      "Student model score: 0.9980039956059952\n",
      "Student model 3-20 fidelity: 0.9980039955926282\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (258593, 258593) entries\n",
      "Student model 3-21 trained with depth 29 and 2773 leaves:\n",
      "Student model score: 0.9984870897202781\n",
      "Student model 3-21 fidelity: 0.998487089766845\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (260013, 260013) entries\n",
      "Student model 3-22 trained with depth 34 and 2783 leaves:\n",
      "Student model score: 0.9951422886827745\n",
      "Student model 3-22 fidelity: 0.9951422886992398\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (261433, 261433) entries\n",
      "Student model 3-23 trained with depth 31 and 2772 leaves:\n",
      "Student model score: 0.9971850751707957\n",
      "Student model 3-23 fidelity: 0.9971850751889225\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (262853, 262853) entries\n",
      "Student model 3-24 trained with depth 29 and 2802 leaves:\n",
      "Student model score: 0.9978484548645534\n",
      "Student model 3-24 fidelity: 0.9978484547085142\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (264273, 264273) entries\n",
      "Student model 3-25 trained with depth 31 and 2810 leaves:\n",
      "Student model score: 0.9956296834409781\n",
      "Student model 3-25 fidelity: 0.9956296835025579\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (265693, 265693) entries\n",
      "Student model 3-26 trained with depth 32 and 2787 leaves:\n",
      "Student model score: 0.9944056158499402\n",
      "Student model 3-26 fidelity: 0.9944056157305983\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (267113, 267113) entries\n",
      "Student model 3-27 trained with depth 28 and 2768 leaves:\n",
      "Student model score: 0.9987933451758948\n",
      "Student model 3-27 fidelity: 0.9987933452085676\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (268533, 268533) entries\n",
      "Student model 3-28 trained with depth 31 and 2768 leaves:\n",
      "Student model score: 0.9917281446836664\n",
      "Student model 3-28 fidelity: 0.9917281446663109\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (269953, 269953) entries\n",
      "Student model 3-29 trained with depth 28 and 2782 leaves:\n",
      "Student model score: 0.9800393828894257\n",
      "Student model 3-29 fidelity: 0.9800393829863175\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (271373, 271373) entries\n",
      "Student model 3-30 trained with depth 29 and 2748 leaves:\n",
      "Student model score: 0.9984243650456527\n",
      "Student model 3-30 fidelity: 0.9984243652153186\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (272793, 272793) entries\n",
      "Student model 3-31 trained with depth 31 and 2767 leaves:\n",
      "Student model score: 0.9951494340160745\n",
      "Student model 3-31 fidelity: 0.9951494340184243\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (274213, 274213) entries\n",
      "Student model 3-32 trained with depth 28 and 2771 leaves:\n",
      "Student model score: 0.9928436080149875\n",
      "Student model 3-32 fidelity: 0.9928436079277982\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (275633, 275633) entries\n",
      "Student model 3-33 trained with depth 30 and 2768 leaves:\n",
      "Student model score: 0.9978835908634971\n",
      "Student model 3-33 fidelity: 0.9978835908560193\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (277053, 277053) entries\n",
      "Student model 3-34 trained with depth 29 and 2789 leaves:\n",
      "Student model score: 0.9948526811222314\n",
      "Student model 3-34 fidelity: 0.9948526813589799\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (278473, 278473) entries\n",
      "Student model 3-35 trained with depth 32 and 2777 leaves:\n",
      "Student model score: 0.9985152287815645\n",
      "Student model 3-35 fidelity: 0.9985152289304201\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (279893, 279893) entries\n",
      "Student model 3-36 trained with depth 30 and 2801 leaves:\n",
      "Student model score: 0.9941536623630777\n",
      "Student model 3-36 fidelity: 0.9941536623552341\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (281313, 281313) entries\n",
      "Student model 3-37 trained with depth 35 and 2776 leaves:\n",
      "Student model score: 0.9927058943290842\n",
      "Student model 3-37 fidelity: 0.99270589437807\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (282733, 282733) entries\n",
      "Student model 3-38 trained with depth 34 and 2784 leaves:\n",
      "Student model score: 0.9963718091936626\n",
      "Student model 3-38 fidelity: 0.9963718090341759\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (284153, 284153) entries\n",
      "Student model 3-39 trained with depth 30 and 2777 leaves:\n",
      "Student model score: 0.9951209051873018\n",
      "Student model 3-39 fidelity: 0.9951209050190395\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (285573, 285573) entries\n",
      "Student model 3-40 trained with depth 33 and 2762 leaves:\n",
      "Student model score: 0.9952626108586493\n",
      "Student model 3-40 fidelity: 0.9952626108577787\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (286993, 286993) entries\n",
      "Student model 3-41 trained with depth 31 and 2793 leaves:\n",
      "Student model score: 0.9983119510762318\n",
      "Student model 3-41 fidelity: 0.9983119509977344\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (288413, 288413) entries\n",
      "Student model 3-42 trained with depth 32 and 2761 leaves:\n",
      "Student model score: 0.9941463302398285\n",
      "Student model 3-42 fidelity: 0.9941463302371336\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (289833, 289833) entries\n",
      "Student model 3-43 trained with depth 29 and 2795 leaves:\n",
      "Student model score: 0.9933378516654353\n",
      "Student model 3-43 fidelity: 0.9933378516539374\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (291253, 291253) entries\n",
      "Student model 3-44 trained with depth 36 and 2762 leaves:\n",
      "Student model score: 0.9953394172673224\n",
      "Student model 3-44 fidelity: 0.9953394172305604\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (292673, 292673) entries\n",
      "Student model 3-45 trained with depth 34 and 2812 leaves:\n",
      "Student model score: 0.9952873260163065\n",
      "Student model 3-45 fidelity: 0.9952873260574581\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (294093, 294093) entries\n",
      "Student model 3-46 trained with depth 29 and 2793 leaves:\n",
      "Student model score: 0.9870404543436776\n",
      "Student model 3-46 fidelity: 0.9870404545881447\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (295513, 295513) entries\n",
      "Student model 3-47 trained with depth 34 and 2775 leaves:\n",
      "Student model score: 0.9928238887146649\n",
      "Student model 3-47 fidelity: 0.9928238887116115\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (296933, 296933) entries\n",
      "Student model 3-48 trained with depth 33 and 2772 leaves:\n",
      "Student model score: 0.9988295343806778\n",
      "Student model 3-48 fidelity: 0.9988295344215264\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (298353, 298353) entries\n",
      "Student model 3-49 trained with depth 32 and 2747 leaves:\n",
      "Student model score: 0.9939286636987812\n",
      "Student model 3-49 fidelity: 0.9939286637034529\n",
      "########## Outer-loop Iteration 4/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (299773, 299773) entries\n",
      "Student model 4-0 trained with depth 29 and 2762 leaves:\n",
      "Student model score: 0.9987585597285519\n",
      "Student model 4-0 fidelity: 0.9987585597401973\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (301193, 301193) entries\n",
      "Student model 4-1 trained with depth 31 and 2761 leaves:\n",
      "Student model score: 0.9954865046643546\n",
      "Student model 4-1 fidelity: 0.995486504663457\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (302613, 302613) entries\n",
      "Student model 4-2 trained with depth 33 and 2774 leaves:\n",
      "Student model score: 0.9990300095377723\n",
      "Student model 4-2 fidelity: 0.9990300095657301\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (304033, 304033) entries\n",
      "Student model 4-3 trained with depth 33 and 2813 leaves:\n",
      "Student model score: 0.9934780827290041\n",
      "Student model 4-3 fidelity: 0.9934780827416506\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (305453, 305453) entries\n",
      "Student model 4-4 trained with depth 31 and 2767 leaves:\n",
      "Student model score: 0.9987015057694401\n",
      "Student model 4-4 fidelity: 0.9987015057099679\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (306873, 306873) entries\n",
      "Student model 4-5 trained with depth 37 and 2765 leaves:\n",
      "Student model score: 0.9959686579850309\n",
      "Student model 4-5 fidelity: 0.9959686581587863\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (308293, 308293) entries\n",
      "Student model 4-6 trained with depth 35 and 2801 leaves:\n",
      "Student model score: 0.9895671950859347\n",
      "Student model 4-6 fidelity: 0.989567195087208\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (309713, 309713) entries\n",
      "Student model 4-7 trained with depth 33 and 2797 leaves:\n",
      "Student model score: 0.9905783417594707\n",
      "Student model 4-7 fidelity: 0.9905783417830651\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (311133, 311133) entries\n",
      "Student model 4-8 trained with depth 31 and 2809 leaves:\n",
      "Student model score: 0.9964199016618721\n",
      "Student model 4-8 fidelity: 0.9964199015945443\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (312553, 312553) entries\n",
      "Student model 4-9 trained with depth 36 and 2786 leaves:\n",
      "Student model score: 0.9979386434720439\n",
      "Student model 4-9 fidelity: 0.9979386433462815\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (313973, 313973) entries\n",
      "Student model 4-10 trained with depth 32 and 2771 leaves:\n",
      "Student model score: 0.9956451405289058\n",
      "Student model 4-10 fidelity: 0.9956451405340053\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (315393, 315393) entries\n",
      "Student model 4-11 trained with depth 34 and 2770 leaves:\n",
      "Student model score: 0.995515148750764\n",
      "Student model 4-11 fidelity: 0.9955151487421828\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (316813, 316813) entries\n",
      "Student model 4-12 trained with depth 34 and 2778 leaves:\n",
      "Student model score: 0.9946533031698622\n",
      "Student model 4-12 fidelity: 0.9946533032860366\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (318233, 318233) entries\n",
      "Student model 4-13 trained with depth 32 and 2770 leaves:\n",
      "Student model score: 0.9954221837578736\n",
      "Student model 4-13 fidelity: 0.9954221838506355\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (319653, 319653) entries\n",
      "Student model 4-14 trained with depth 41 and 2760 leaves:\n",
      "Student model score: 0.9871478109631205\n",
      "Student model 4-14 fidelity: 0.9871478109600272\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (321073, 321073) entries\n",
      "Student model 4-15 trained with depth 30 and 2744 leaves:\n",
      "Student model score: 0.9920855521913183\n",
      "Student model 4-15 fidelity: 0.9920855522428158\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (322493, 322493) entries\n",
      "Student model 4-16 trained with depth 34 and 2784 leaves:\n",
      "Student model score: 0.9970072044272901\n",
      "Student model 4-16 fidelity: 0.997007204371587\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (323913, 323913) entries\n",
      "Student model 4-17 trained with depth 34 and 2794 leaves:\n",
      "Student model score: 0.997852112443058\n",
      "Student model 4-17 fidelity: 0.9978521124772844\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (325333, 325333) entries\n",
      "Student model 4-18 trained with depth 34 and 2785 leaves:\n",
      "Student model score: 0.9957139464968503\n",
      "Student model 4-18 fidelity: 0.995713946505811\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (326753, 326753) entries\n",
      "Student model 4-19 trained with depth 34 and 2765 leaves:\n",
      "Student model score: 0.9904023533295012\n",
      "Student model 4-19 fidelity: 0.9904023532776204\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (328173, 328173) entries\n",
      "Student model 4-20 trained with depth 31 and 2753 leaves:\n",
      "Student model score: 0.9894450266021931\n",
      "Student model 4-20 fidelity: 0.9894450266098817\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (329593, 329593) entries\n",
      "Student model 4-21 trained with depth 32 and 2790 leaves:\n",
      "Student model score: 0.9986472673803671\n",
      "Student model 4-21 fidelity: 0.9986472671572854\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (331013, 331013) entries\n",
      "Student model 4-22 trained with depth 33 and 2800 leaves:\n",
      "Student model score: 0.9921957546751198\n",
      "Student model 4-22 fidelity: 0.9921957547101808\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (332433, 332433) entries\n",
      "Student model 4-23 trained with depth 33 and 2827 leaves:\n",
      "Student model score: 0.9909332706318804\n",
      "Student model 4-23 fidelity: 0.99093327059789\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (333853, 333853) entries\n",
      "Student model 4-24 trained with depth 29 and 2777 leaves:\n",
      "Student model score: 0.9984552639358593\n",
      "Student model 4-24 fidelity: 0.9984552639439309\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (335273, 335273) entries\n",
      "Student model 4-25 trained with depth 31 and 2778 leaves:\n",
      "Student model score: 0.9879152308645517\n",
      "Student model 4-25 fidelity: 0.987915230805273\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (336693, 336693) entries\n",
      "Student model 4-26 trained with depth 32 and 2795 leaves:\n",
      "Student model score: 0.9946655293082716\n",
      "Student model 4-26 fidelity: 0.9946655293114306\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (338113, 338113) entries\n",
      "Student model 4-27 trained with depth 32 and 2760 leaves:\n",
      "Student model score: 0.99126862169427\n",
      "Student model 4-27 fidelity: 0.9912686216921471\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (339533, 339533) entries\n",
      "Student model 4-28 trained with depth 30 and 2744 leaves:\n",
      "Student model score: 0.998078165448685\n",
      "Student model 4-28 fidelity: 0.9980781654009508\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (340953, 340953) entries\n",
      "Student model 4-29 trained with depth 31 and 2813 leaves:\n",
      "Student model score: 0.9982762872735265\n",
      "Student model 4-29 fidelity: 0.9982762872381183\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (342373, 342373) entries\n",
      "Student model 4-30 trained with depth 31 and 2786 leaves:\n",
      "Student model score: 0.9901573633063938\n",
      "Student model 4-30 fidelity: 0.990157363256056\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (343793, 343793) entries\n",
      "Student model 4-31 trained with depth 29 and 2761 leaves:\n",
      "Student model score: 0.9983587740262683\n",
      "Student model 4-31 fidelity: 0.9983587740056578\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (345213, 345213) entries\n",
      "Student model 4-32 trained with depth 32 and 2768 leaves:\n",
      "Student model score: 0.995100023421273\n",
      "Student model 4-32 fidelity: 0.9951000234262474\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (346633, 346633) entries\n",
      "Student model 4-33 trained with depth 28 and 2773 leaves:\n",
      "Student model score: 0.9980005112541527\n",
      "Student model 4-33 fidelity: 0.9980005112475784\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (348053, 348053) entries\n",
      "Student model 4-34 trained with depth 34 and 2790 leaves:\n",
      "Student model score: 0.9883782987848841\n",
      "Student model 4-34 fidelity: 0.9883782988320898\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (349473, 349473) entries\n",
      "Student model 4-35 trained with depth 35 and 2759 leaves:\n",
      "Student model score: 0.998759410489989\n",
      "Student model 4-35 fidelity: 0.9987594105645813\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (350893, 350893) entries\n",
      "Student model 4-36 trained with depth 34 and 2794 leaves:\n",
      "Student model score: 0.9955921443870999\n",
      "Student model 4-36 fidelity: 0.995592144474274\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (352313, 352313) entries\n",
      "Student model 4-37 trained with depth 31 and 2774 leaves:\n",
      "Student model score: 0.9980608738495027\n",
      "Student model 4-37 fidelity: 0.9980608737783371\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (353733, 353733) entries\n",
      "Student model 4-38 trained with depth 32 and 2795 leaves:\n",
      "Student model score: 0.9899568574334421\n",
      "Student model 4-38 fidelity: 0.9899568574372996\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (355153, 355153) entries\n",
      "Student model 4-39 trained with depth 33 and 2803 leaves:\n",
      "Student model score: 0.9949921376543408\n",
      "Student model 4-39 fidelity: 0.9949921377462748\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (356573, 356573) entries\n",
      "Student model 4-40 trained with depth 29 and 2762 leaves:\n",
      "Student model score: 0.9956008717497649\n",
      "Student model 4-40 fidelity: 0.995600871738008\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (357993, 357993) entries\n",
      "Student model 4-41 trained with depth 34 and 2752 leaves:\n",
      "Student model score: 0.9933538514551106\n",
      "Student model 4-41 fidelity: 0.9933538514675773\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (359413, 359413) entries\n",
      "Student model 4-42 trained with depth 34 and 2784 leaves:\n",
      "Student model score: 0.9975146365891123\n",
      "Student model 4-42 fidelity: 0.9975146366200304\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (360833, 360833) entries\n",
      "Student model 4-43 trained with depth 31 and 2761 leaves:\n",
      "Student model score: 0.9953013532103303\n",
      "Student model 4-43 fidelity: 0.995301353207451\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (362253, 362253) entries\n",
      "Student model 4-44 trained with depth 34 and 2781 leaves:\n",
      "Student model score: 0.9976665019408172\n",
      "Student model 4-44 fidelity: 0.9976665019070191\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (363673, 363673) entries\n",
      "Student model 4-45 trained with depth 32 and 2779 leaves:\n",
      "Student model score: 0.9970928712863296\n",
      "Student model 4-45 fidelity: 0.9970928712928895\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (365093, 365093) entries\n",
      "Student model 4-46 trained with depth 33 and 2758 leaves:\n",
      "Student model score: 0.9989043665643966\n",
      "Student model 4-46 fidelity: 0.9989043665540857\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (366513, 366513) entries\n",
      "Student model 4-47 trained with depth 31 and 2754 leaves:\n",
      "Student model score: 0.9986363241183042\n",
      "Student model 4-47 fidelity: 0.9986363241176294\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (367933, 367933) entries\n",
      "Student model 4-48 trained with depth 35 and 2773 leaves:\n",
      "Student model score: 0.9984854056700084\n",
      "Student model 4-48 fidelity: 0.9984854056649951\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (369353, 369353) entries\n",
      "Student model 4-49 trained with depth 31 and 2737 leaves:\n",
      "Student model score: 0.9982197915672077\n",
      "Student model 4-49 fidelity: 0.9982197915841752\n",
      "########## Outer-loop Iteration 5/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (370773, 370773) entries\n",
      "Student model 5-0 trained with depth 31 and 2787 leaves:\n",
      "Student model score: 0.9891406927942162\n",
      "Student model 5-0 fidelity: 0.9891406927368082\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (372193, 372193) entries\n",
      "Student model 5-1 trained with depth 32 and 2774 leaves:\n",
      "Student model score: 0.9973230799415039\n",
      "Student model 5-1 fidelity: 0.9973230799438301\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (373613, 373613) entries\n",
      "Student model 5-2 trained with depth 30 and 2775 leaves:\n",
      "Student model score: 0.9978299388804673\n",
      "Student model 5-2 fidelity: 0.9978299390145637\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (375033, 375033) entries\n",
      "Student model 5-3 trained with depth 32 and 2781 leaves:\n",
      "Student model score: 0.9981209209763262\n",
      "Student model 5-3 fidelity: 0.9981209210884909\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (376453, 376453) entries\n",
      "Student model 5-4 trained with depth 29 and 2780 leaves:\n",
      "Student model score: 0.9907733078620414\n",
      "Student model 5-4 fidelity: 0.9907733078506583\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (377873, 377873) entries\n",
      "Student model 5-5 trained with depth 30 and 2775 leaves:\n",
      "Student model score: 0.9951179959474465\n",
      "Student model 5-5 fidelity: 0.9951179959725238\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (379293, 379293) entries\n",
      "Student model 5-6 trained with depth 30 and 2767 leaves:\n",
      "Student model score: 0.9947949690772697\n",
      "Student model 5-6 fidelity: 0.9947949690506528\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (380713, 380713) entries\n",
      "Student model 5-7 trained with depth 32 and 2780 leaves:\n",
      "Student model score: 0.9979581923926575\n",
      "Student model 5-7 fidelity: 0.9979581923975064\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (382133, 382133) entries\n",
      "Student model 5-8 trained with depth 29 and 2742 leaves:\n",
      "Student model score: 0.9929663987733243\n",
      "Student model 5-8 fidelity: 0.9929663987690428\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (383553, 383553) entries\n",
      "Student model 5-9 trained with depth 31 and 2766 leaves:\n",
      "Student model score: 0.9974502259499042\n",
      "Student model 5-9 fidelity: 0.997450225980957\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (384973, 384973) entries\n",
      "Student model 5-10 trained with depth 32 and 2757 leaves:\n",
      "Student model score: 0.9982520167060897\n",
      "Student model 5-10 fidelity: 0.9982520166989362\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (386393, 386393) entries\n",
      "Student model 5-11 trained with depth 34 and 2771 leaves:\n",
      "Student model score: 0.9981609375395999\n",
      "Student model 5-11 fidelity: 0.9981609375275798\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (387813, 387813) entries\n",
      "Student model 5-12 trained with depth 31 and 2767 leaves:\n",
      "Student model score: 0.9974740886206297\n",
      "Student model 5-12 fidelity: 0.9974740886224638\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (389233, 389233) entries\n",
      "Student model 5-13 trained with depth 30 and 2792 leaves:\n",
      "Student model score: 0.9861849838801757\n",
      "Student model 5-13 fidelity: 0.9861849840213748\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (390653, 390653) entries\n",
      "Student model 5-14 trained with depth 37 and 2755 leaves:\n",
      "Student model score: 0.9985899164911681\n",
      "Student model 5-14 fidelity: 0.9985899165533026\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (392073, 392073) entries\n",
      "Student model 5-15 trained with depth 32 and 2804 leaves:\n",
      "Student model score: 0.9979284296899462\n",
      "Student model 5-15 fidelity: 0.9979284297935986\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (393493, 393493) entries\n",
      "Student model 5-16 trained with depth 33 and 2772 leaves:\n",
      "Student model score: 0.9945165821772918\n",
      "Student model 5-16 fidelity: 0.9945165820885452\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (394913, 394913) entries\n",
      "Student model 5-17 trained with depth 29 and 2786 leaves:\n",
      "Student model score: 0.9973587912658467\n",
      "Student model 5-17 fidelity: 0.9973587913603801\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (396333, 396333) entries\n",
      "Student model 5-18 trained with depth 29 and 2775 leaves:\n",
      "Student model score: 0.9978155800663967\n",
      "Student model 5-18 fidelity: 0.9978155801172908\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (397753, 397753) entries\n",
      "Student model 5-19 trained with depth 29 and 2776 leaves:\n",
      "Student model score: 0.9981190404420828\n",
      "Student model 5-19 fidelity: 0.9981190403642967\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (399173, 399173) entries\n",
      "Student model 5-20 trained with depth 27 and 2771 leaves:\n",
      "Student model score: 0.9993622709783381\n",
      "Student model 5-20 fidelity: 0.999362270976042\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (400593, 400593) entries\n",
      "Student model 5-21 trained with depth 35 and 2767 leaves:\n",
      "Student model score: 0.996529826774549\n",
      "Student model 5-21 fidelity: 0.9965298268084837\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (402013, 402013) entries\n",
      "Student model 5-22 trained with depth 32 and 2787 leaves:\n",
      "Student model score: 0.9974175857743125\n",
      "Student model 5-22 fidelity: 0.9974175857728281\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (403433, 403433) entries\n",
      "Student model 5-23 trained with depth 31 and 2754 leaves:\n",
      "Student model score: 0.9827823787631623\n",
      "Student model 5-23 fidelity: 0.9827823787514837\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (404853, 404853) entries\n",
      "Student model 5-24 trained with depth 34 and 2811 leaves:\n",
      "Student model score: 0.9959340211888915\n",
      "Student model 5-24 fidelity: 0.9959340212585366\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (406273, 406273) entries\n",
      "Student model 5-25 trained with depth 30 and 2808 leaves:\n",
      "Student model score: 0.9975924244244277\n",
      "Student model 5-25 fidelity: 0.9975924245419987\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (407693, 407693) entries\n",
      "Student model 5-26 trained with depth 31 and 2756 leaves:\n",
      "Student model score: 0.9973370745431843\n",
      "Student model 5-26 fidelity: 0.9973370743055353\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (409113, 409113) entries\n",
      "Student model 5-27 trained with depth 37 and 2784 leaves:\n",
      "Student model score: 0.9975830419554648\n",
      "Student model 5-27 fidelity: 0.9975830419595103\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (410533, 410533) entries\n",
      "Student model 5-28 trained with depth 33 and 2762 leaves:\n",
      "Student model score: 0.9990385383032292\n",
      "Student model 5-28 fidelity: 0.9990385383043178\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (411953, 411953) entries\n",
      "Student model 5-29 trained with depth 33 and 2776 leaves:\n",
      "Student model score: 0.992039052503647\n",
      "Student model 5-29 fidelity: 0.9920390525108879\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (413373, 413373) entries\n",
      "Student model 5-30 trained with depth 32 and 2761 leaves:\n",
      "Student model score: 0.9957797222285465\n",
      "Student model 5-30 fidelity: 0.9957797222255251\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (414793, 414793) entries\n",
      "Student model 5-31 trained with depth 35 and 2770 leaves:\n",
      "Student model score: 0.9951402238513569\n",
      "Student model 5-31 fidelity: 0.9951402238182839\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (416213, 416213) entries\n",
      "Student model 5-32 trained with depth 30 and 2785 leaves:\n",
      "Student model score: 0.9958628756999998\n",
      "Student model 5-32 fidelity: 0.99586287568286\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (417633, 417633) entries\n",
      "Student model 5-33 trained with depth 30 and 2760 leaves:\n",
      "Student model score: 0.9920184815313574\n",
      "Student model 5-33 fidelity: 0.9920184816504746\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (419053, 419053) entries\n",
      "Student model 5-34 trained with depth 34 and 2773 leaves:\n",
      "Student model score: 0.9980163128938973\n",
      "Student model 5-34 fidelity: 0.9980163128913253\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (420473, 420473) entries\n",
      "Student model 5-35 trained with depth 31 and 2763 leaves:\n",
      "Student model score: 0.9848266941775999\n",
      "Student model 5-35 fidelity: 0.9848266938882284\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (421893, 421893) entries\n",
      "Student model 5-36 trained with depth 32 and 2765 leaves:\n",
      "Student model score: 0.9953502161689605\n",
      "Student model 5-36 fidelity: 0.995350216157703\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (423313, 423313) entries\n",
      "Student model 5-37 trained with depth 35 and 2804 leaves:\n",
      "Student model score: 0.9983197679466531\n",
      "Student model 5-37 fidelity: 0.9983197679321374\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (424733, 424733) entries\n",
      "Student model 5-38 trained with depth 30 and 2769 leaves:\n",
      "Student model score: 0.9972957218621668\n",
      "Student model 5-38 fidelity: 0.9972957218603546\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (426153, 426153) entries\n",
      "Student model 5-39 trained with depth 26 and 2751 leaves:\n",
      "Student model score: 0.9986537957584073\n",
      "Student model 5-39 fidelity: 0.9986537957630457\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (427573, 427573) entries\n",
      "Student model 5-40 trained with depth 29 and 2776 leaves:\n",
      "Student model score: 0.9989364536320396\n",
      "Student model 5-40 fidelity: 0.9989364536319802\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (428993, 428993) entries\n",
      "Student model 5-41 trained with depth 30 and 2771 leaves:\n",
      "Student model score: 0.9973921400242989\n",
      "Student model 5-41 fidelity: 0.997392140023759\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (430413, 430413) entries\n",
      "Student model 5-42 trained with depth 34 and 2768 leaves:\n",
      "Student model score: 0.9925200730382228\n",
      "Student model 5-42 fidelity: 0.9925200730452557\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (431833, 431833) entries\n",
      "Student model 5-43 trained with depth 34 and 2771 leaves:\n",
      "Student model score: 0.9930971931282404\n",
      "Student model 5-43 fidelity: 0.9930971931219813\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (433253, 433253) entries\n",
      "Student model 5-44 trained with depth 33 and 2766 leaves:\n",
      "Student model score: 0.9917580570124119\n",
      "Student model 5-44 fidelity: 0.9917580569968253\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (434673, 434673) entries\n",
      "Student model 5-45 trained with depth 36 and 2735 leaves:\n",
      "Student model score: 0.9904367580226298\n",
      "Student model 5-45 fidelity: 0.9904367578807142\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (436093, 436093) entries\n",
      "Student model 5-46 trained with depth 28 and 2766 leaves:\n",
      "Student model score: 0.997775270293556\n",
      "Student model 5-46 fidelity: 0.9977752701070729\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (437513, 437513) entries\n",
      "Student model 5-47 trained with depth 32 and 2786 leaves:\n",
      "Student model score: 0.9974402525392141\n",
      "Student model 5-47 fidelity: 0.997440252538197\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (438933, 438933) entries\n",
      "Student model 5-48 trained with depth 36 and 2736 leaves:\n",
      "Student model score: 0.9855242957308687\n",
      "Student model 5-48 fidelity: 0.9855242957234938\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (440353, 440353) entries\n",
      "Student model 5-49 trained with depth 31 and 2746 leaves:\n",
      "Student model score: 0.9982230614657127\n",
      "Student model 5-49 fidelity: 0.9982230614125819\n",
      "########## Outer-loop Iteration 6/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (441773, 441773) entries\n",
      "Student model 6-0 trained with depth 30 and 2771 leaves:\n",
      "Student model score: 0.9945838350421774\n",
      "Student model 6-0 fidelity: 0.9945838348725479\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (443193, 443193) entries\n",
      "Student model 6-1 trained with depth 32 and 2784 leaves:\n",
      "Student model score: 0.9900060198231814\n",
      "Student model 6-1 fidelity: 0.9900060197971298\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (444613, 444613) entries\n",
      "Student model 6-2 trained with depth 31 and 2720 leaves:\n",
      "Student model score: 0.993028460031461\n",
      "Student model 6-2 fidelity: 0.9930284598741379\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (446033, 446033) entries\n",
      "Student model 6-3 trained with depth 33 and 2763 leaves:\n",
      "Student model score: 0.995955741658049\n",
      "Student model 6-3 fidelity: 0.9959557416596782\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (447453, 447453) entries\n",
      "Student model 6-4 trained with depth 32 and 2754 leaves:\n",
      "Student model score: 0.9976179547086375\n",
      "Student model 6-4 fidelity: 0.9976179547009852\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (448873, 448873) entries\n",
      "Student model 6-5 trained with depth 28 and 2788 leaves:\n",
      "Student model score: 0.9944341484712844\n",
      "Student model 6-5 fidelity: 0.9944341484734024\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (450293, 450293) entries\n",
      "Student model 6-6 trained with depth 32 and 2765 leaves:\n",
      "Student model score: 0.9982554119970781\n",
      "Student model 6-6 fidelity: 0.9982554119962141\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (451713, 451713) entries\n",
      "Student model 6-7 trained with depth 34 and 2749 leaves:\n",
      "Student model score: 0.9984570024529571\n",
      "Student model 6-7 fidelity: 0.9984570024510733\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (453133, 453133) entries\n",
      "Student model 6-8 trained with depth 32 and 2747 leaves:\n",
      "Student model score: 0.9982844646319543\n",
      "Student model 6-8 fidelity: 0.9982844645771382\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (454553, 454553) entries\n",
      "Student model 6-9 trained with depth 34 and 2750 leaves:\n",
      "Student model score: 0.9889514271324358\n",
      "Student model 6-9 fidelity: 0.9889514271337696\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (455973, 455973) entries\n",
      "Student model 6-10 trained with depth 31 and 2785 leaves:\n",
      "Student model score: 0.9985236858394406\n",
      "Student model 6-10 fidelity: 0.9985236858374917\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (457393, 457393) entries\n",
      "Student model 6-11 trained with depth 32 and 2785 leaves:\n",
      "Student model score: 0.9943423835294202\n",
      "Student model 6-11 fidelity: 0.994342383537675\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (458813, 458813) entries\n",
      "Student model 6-12 trained with depth 34 and 2747 leaves:\n",
      "Student model score: 0.9982044068406967\n",
      "Student model 6-12 fidelity: 0.9982044068285609\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (460233, 460233) entries\n",
      "Student model 6-13 trained with depth 33 and 2790 leaves:\n",
      "Student model score: 0.9949619313709187\n",
      "Student model 6-13 fidelity: 0.9949619313441288\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (461653, 461653) entries\n",
      "Student model 6-14 trained with depth 32 and 2747 leaves:\n",
      "Student model score: 0.9951746197628336\n",
      "Student model 6-14 fidelity: 0.995174619761628\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (463073, 463073) entries\n",
      "Student model 6-15 trained with depth 32 and 2770 leaves:\n",
      "Student model score: 0.9898067589045739\n",
      "Student model 6-15 fidelity: 0.9898067588604244\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (464493, 464493) entries\n",
      "Student model 6-16 trained with depth 34 and 2765 leaves:\n",
      "Student model score: 0.9847049001099868\n",
      "Student model 6-16 fidelity: 0.9847049001799076\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (465913, 465913) entries\n",
      "Student model 6-17 trained with depth 35 and 2765 leaves:\n",
      "Student model score: 0.9950600249230426\n",
      "Student model 6-17 fidelity: 0.9950600248586345\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (467333, 467333) entries\n",
      "Student model 6-18 trained with depth 33 and 2776 leaves:\n",
      "Student model score: 0.995211406164712\n",
      "Student model 6-18 fidelity: 0.9952114061592648\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (468753, 468753) entries\n",
      "Student model 6-19 trained with depth 32 and 2743 leaves:\n",
      "Student model score: 0.9977708622922842\n",
      "Student model 6-19 fidelity: 0.9977708623281344\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (470173, 470173) entries\n",
      "Student model 6-20 trained with depth 29 and 2772 leaves:\n",
      "Student model score: 0.9919055276739679\n",
      "Student model 6-20 fidelity: 0.9919055276533085\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (471593, 471593) entries\n",
      "Student model 6-21 trained with depth 30 and 2800 leaves:\n",
      "Student model score: 0.9976981148748455\n",
      "Student model 6-21 fidelity: 0.9976981148717162\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (473013, 473013) entries\n",
      "Student model 6-22 trained with depth 31 and 2765 leaves:\n",
      "Student model score: 0.9949120118837768\n",
      "Student model 6-22 fidelity: 0.9949120117987891\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (474433, 474433) entries\n",
      "Student model 6-23 trained with depth 34 and 2758 leaves:\n",
      "Student model score: 0.9983541485912595\n",
      "Student model 6-23 fidelity: 0.9983541485963165\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (475853, 475853) entries\n",
      "Student model 6-24 trained with depth 31 and 2725 leaves:\n",
      "Student model score: 0.9987524835722266\n",
      "Student model 6-24 fidelity: 0.998752483568459\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (477273, 477273) entries\n",
      "Student model 6-25 trained with depth 31 and 2748 leaves:\n",
      "Student model score: 0.9985485629314903\n",
      "Student model 6-25 fidelity: 0.998548562927552\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (478693, 478693) entries\n",
      "Student model 6-26 trained with depth 31 and 2747 leaves:\n",
      "Student model score: 0.9985935392901124\n",
      "Student model 6-26 fidelity: 0.9985935392885211\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (480113, 480113) entries\n",
      "Student model 6-27 trained with depth 27 and 2743 leaves:\n",
      "Student model score: 0.9959243710189953\n",
      "Student model 6-27 fidelity: 0.9959243710170668\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (481533, 481533) entries\n",
      "Student model 6-28 trained with depth 31 and 2764 leaves:\n",
      "Student model score: 0.9989389789131053\n",
      "Student model 6-28 fidelity: 0.9989389789255849\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (482953, 482953) entries\n",
      "Student model 6-29 trained with depth 32 and 2776 leaves:\n",
      "Student model score: 0.9924645941099791\n",
      "Student model 6-29 fidelity: 0.9924645941113521\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (484373, 484373) entries\n",
      "Student model 6-30 trained with depth 35 and 2796 leaves:\n",
      "Student model score: 0.9962925217213868\n",
      "Student model 6-30 fidelity: 0.9962925217147542\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (485793, 485793) entries\n",
      "Student model 6-31 trained with depth 31 and 2752 leaves:\n",
      "Student model score: 0.995171557397325\n",
      "Student model 6-31 fidelity: 0.9951715573922033\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (487213, 487213) entries\n",
      "Student model 6-32 trained with depth 31 and 2806 leaves:\n",
      "Student model score: 0.9953846987915818\n",
      "Student model 6-32 fidelity: 0.9953846987955148\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (488633, 488633) entries\n",
      "Student model 6-33 trained with depth 32 and 2763 leaves:\n",
      "Student model score: 0.9888827861724203\n",
      "Student model 6-33 fidelity: 0.9888827861732372\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (490053, 490053) entries\n",
      "Student model 6-34 trained with depth 30 and 2759 leaves:\n",
      "Student model score: 0.9955773076630762\n",
      "Student model 6-34 fidelity: 0.9955773076596987\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (491473, 491473) entries\n",
      "Student model 6-35 trained with depth 33 and 2771 leaves:\n",
      "Student model score: 0.9960144835505393\n",
      "Student model 6-35 fidelity: 0.9960144835463041\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (492893, 492893) entries\n",
      "Student model 6-36 trained with depth 32 and 2778 leaves:\n",
      "Student model score: 0.9944432182119173\n",
      "Student model 6-36 fidelity: 0.9944432182129516\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (494313, 494313) entries\n",
      "Student model 6-37 trained with depth 29 and 2758 leaves:\n",
      "Student model score: 0.9951457483583623\n",
      "Student model 6-37 fidelity: 0.9951457483582627\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (495733, 495733) entries\n",
      "Student model 6-38 trained with depth 30 and 2714 leaves:\n",
      "Student model score: 0.9987575256068776\n",
      "Student model 6-38 fidelity: 0.9987575256108057\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (497153, 497153) entries\n",
      "Student model 6-39 trained with depth 30 and 2780 leaves:\n",
      "Student model score: 0.9930987910471056\n",
      "Student model 6-39 fidelity: 0.9930987910321036\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (498573, 498573) entries\n",
      "Student model 6-40 trained with depth 34 and 2783 leaves:\n",
      "Student model score: 0.9913730985583705\n",
      "Student model 6-40 fidelity: 0.991373098544571\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (499993, 499993) entries\n",
      "Student model 6-41 trained with depth 32 and 2800 leaves:\n",
      "Student model score: 0.9954470408792013\n",
      "Student model 6-41 fidelity: 0.995447040892374\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (501413, 501413) entries\n",
      "Student model 6-42 trained with depth 29 and 2788 leaves:\n",
      "Student model score: 0.9888129284123327\n",
      "Student model 6-42 fidelity: 0.9888129284074754\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (502833, 502833) entries\n",
      "Student model 6-43 trained with depth 30 and 2750 leaves:\n",
      "Student model score: 0.9984600235551874\n",
      "Student model 6-43 fidelity: 0.9984600235555126\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (504253, 504253) entries\n",
      "Student model 6-44 trained with depth 33 and 2769 leaves:\n",
      "Student model score: 0.9953966508774558\n",
      "Student model 6-44 fidelity: 0.9953966506698652\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (505673, 505673) entries\n",
      "Student model 6-45 trained with depth 29 and 2760 leaves:\n",
      "Student model score: 0.9958702502990776\n",
      "Student model 6-45 fidelity: 0.9958702502992772\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (507093, 507093) entries\n",
      "Student model 6-46 trained with depth 38 and 2793 leaves:\n",
      "Student model score: 0.9952695299785315\n",
      "Student model 6-46 fidelity: 0.99526952997025\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (508513, 508513) entries\n",
      "Student model 6-47 trained with depth 30 and 2769 leaves:\n",
      "Student model score: 0.9917988256126714\n",
      "Student model 6-47 fidelity: 0.991798825604584\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (509933, 509933) entries\n",
      "Student model 6-48 trained with depth 30 and 2779 leaves:\n",
      "Student model score: 0.9984901475868758\n",
      "Student model 6-48 fidelity: 0.9984901476506111\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (511353, 511353) entries\n",
      "Student model 6-49 trained with depth 35 and 2789 leaves:\n",
      "Student model score: 0.9970032210295962\n",
      "Student model 6-49 fidelity: 0.9970032209378794\n",
      "########## Outer-loop Iteration 7/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (512773, 512773) entries\n",
      "Student model 7-0 trained with depth 32 and 2771 leaves:\n",
      "Student model score: 0.9950801944190774\n",
      "Student model 7-0 fidelity: 0.9950801944131832\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (514193, 514193) entries\n",
      "Student model 7-1 trained with depth 31 and 2769 leaves:\n",
      "Student model score: 0.9951624845797924\n",
      "Student model 7-1 fidelity: 0.99516248459276\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (515613, 515613) entries\n",
      "Student model 7-2 trained with depth 31 and 2775 leaves:\n",
      "Student model score: 0.9982272006847014\n",
      "Student model 7-2 fidelity: 0.9982272006125182\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (517033, 517033) entries\n",
      "Student model 7-3 trained with depth 30 and 2754 leaves:\n",
      "Student model score: 0.9921615579527183\n",
      "Student model 7-3 fidelity: 0.992161557861035\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (518453, 518453) entries\n",
      "Student model 7-4 trained with depth 35 and 2810 leaves:\n",
      "Student model score: 0.998823753602967\n",
      "Student model 7-4 fidelity: 0.9988237536135581\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (519873, 519873) entries\n",
      "Student model 7-5 trained with depth 31 and 2791 leaves:\n",
      "Student model score: 0.9952631468629762\n",
      "Student model 7-5 fidelity: 0.9952631467769396\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (521293, 521293) entries\n",
      "Student model 7-6 trained with depth 34 and 2751 leaves:\n",
      "Student model score: 0.9954368291384225\n",
      "Student model 7-6 fidelity: 0.9954368291597548\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (522713, 522713) entries\n",
      "Student model 7-7 trained with depth 30 and 2769 leaves:\n",
      "Student model score: 0.9983152580697244\n",
      "Student model 7-7 fidelity: 0.9983152580614134\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (524133, 524133) entries\n",
      "Student model 7-8 trained with depth 34 and 2797 leaves:\n",
      "Student model score: 0.9867727062299512\n",
      "Student model 7-8 fidelity: 0.9867727062088707\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (525553, 525553) entries\n",
      "Student model 7-9 trained with depth 31 and 2786 leaves:\n",
      "Student model score: 0.9985777208917571\n",
      "Student model 7-9 fidelity: 0.9985777209182828\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (526973, 526973) entries\n",
      "Student model 7-10 trained with depth 32 and 2748 leaves:\n",
      "Student model score: 0.9975823029089517\n",
      "Student model 7-10 fidelity: 0.9975823027287165\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (528393, 528393) entries\n",
      "Student model 7-11 trained with depth 33 and 2765 leaves:\n",
      "Student model score: 0.9944368155263448\n",
      "Student model 7-11 fidelity: 0.9944368154717298\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (529813, 529813) entries\n",
      "Student model 7-12 trained with depth 32 and 2760 leaves:\n",
      "Student model score: 0.9951048852451605\n",
      "Student model 7-12 fidelity: 0.9951048850831727\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (531233, 531233) entries\n",
      "Student model 7-13 trained with depth 29 and 2759 leaves:\n",
      "Student model score: 0.9957050652573688\n",
      "Student model 7-13 fidelity: 0.9957050653033594\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (532653, 532653) entries\n",
      "Student model 7-14 trained with depth 33 and 2771 leaves:\n",
      "Student model score: 0.9952157464405785\n",
      "Student model 7-14 fidelity: 0.9952157464922082\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (534073, 534073) entries\n",
      "Student model 7-15 trained with depth 34 and 2761 leaves:\n",
      "Student model score: 0.9985670698769064\n",
      "Student model 7-15 fidelity: 0.9985670698884777\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (535493, 535493) entries\n",
      "Student model 7-16 trained with depth 31 and 2755 leaves:\n",
      "Student model score: 0.9985214722349074\n",
      "Student model 7-16 fidelity: 0.9985214722297798\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (536913, 536913) entries\n",
      "Student model 7-17 trained with depth 31 and 2808 leaves:\n",
      "Student model score: 0.9932145080707897\n",
      "Student model 7-17 fidelity: 0.9932145080671371\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (538333, 538333) entries\n",
      "Student model 7-18 trained with depth 30 and 2767 leaves:\n",
      "Student model score: 0.9991262090476755\n",
      "Student model 7-18 fidelity: 0.9991262091275072\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (539753, 539753) entries\n",
      "Student model 7-19 trained with depth 36 and 2790 leaves:\n",
      "Student model score: 0.9888971510737619\n",
      "Student model 7-19 fidelity: 0.9888971512839947\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (541173, 541173) entries\n",
      "Student model 7-20 trained with depth 36 and 2804 leaves:\n",
      "Student model score: 0.995740377541769\n",
      "Student model 7-20 fidelity: 0.9957403775435503\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (542593, 542593) entries\n",
      "Student model 7-21 trained with depth 31 and 2795 leaves:\n",
      "Student model score: 0.9981765312262945\n",
      "Student model 7-21 fidelity: 0.9981765311980187\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (544013, 544013) entries\n",
      "Student model 7-22 trained with depth 35 and 2786 leaves:\n",
      "Student model score: 0.9917485944486397\n",
      "Student model 7-22 fidelity: 0.9917485944271706\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (545433, 545433) entries\n",
      "Student model 7-23 trained with depth 30 and 2745 leaves:\n",
      "Student model score: 0.9973757237373679\n",
      "Student model 7-23 fidelity: 0.9973757235938885\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (546853, 546853) entries\n",
      "Student model 7-24 trained with depth 32 and 2764 leaves:\n",
      "Student model score: 0.9898693047300325\n",
      "Student model 7-24 fidelity: 0.9898693052271063\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (548273, 548273) entries\n",
      "Student model 7-25 trained with depth 33 and 2760 leaves:\n",
      "Student model score: 0.998198225281036\n",
      "Student model 7-25 fidelity: 0.9981982252817261\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (549693, 549693) entries\n",
      "Student model 7-26 trained with depth 29 and 2773 leaves:\n",
      "Student model score: 0.9897394331211584\n",
      "Student model 7-26 fidelity: 0.989739433000112\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (551113, 551113) entries\n",
      "Student model 7-27 trained with depth 31 and 2771 leaves:\n",
      "Student model score: 0.9982146193206166\n",
      "Student model 7-27 fidelity: 0.9982146193256952\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (552533, 552533) entries\n",
      "Student model 7-28 trained with depth 28 and 2784 leaves:\n",
      "Student model score: 0.9933484838852417\n",
      "Student model 7-28 fidelity: 0.9933484839347744\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (553953, 553953) entries\n",
      "Student model 7-29 trained with depth 31 and 2741 leaves:\n",
      "Student model score: 0.9890976173929482\n",
      "Student model 7-29 fidelity: 0.9890976173911811\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (555373, 555373) entries\n",
      "Student model 7-30 trained with depth 30 and 2779 leaves:\n",
      "Student model score: 0.9945717655994069\n",
      "Student model 7-30 fidelity: 0.9945717655906998\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (556793, 556793) entries\n",
      "Student model 7-31 trained with depth 33 and 2755 leaves:\n",
      "Student model score: 0.999115312209515\n",
      "Student model 7-31 fidelity: 0.9991153121894718\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (558213, 558213) entries\n",
      "Student model 7-32 trained with depth 30 and 2759 leaves:\n",
      "Student model score: 0.9958414537262373\n",
      "Student model 7-32 fidelity: 0.9958414537218248\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (559633, 559633) entries\n",
      "Student model 7-33 trained with depth 31 and 2788 leaves:\n",
      "Student model score: 0.9920936649247353\n",
      "Student model 7-33 fidelity: 0.9920936649469103\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (561053, 561053) entries\n",
      "Student model 7-34 trained with depth 33 and 2761 leaves:\n",
      "Student model score: 0.997602556270117\n",
      "Student model 7-34 fidelity: 0.997602556380408\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (562473, 562473) entries\n",
      "Student model 7-35 trained with depth 36 and 2772 leaves:\n",
      "Student model score: 0.998520203047031\n",
      "Student model 7-35 fidelity: 0.9985202030645077\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (563893, 563893) entries\n",
      "Student model 7-36 trained with depth 34 and 2777 leaves:\n",
      "Student model score: 0.9951301293101706\n",
      "Student model 7-36 fidelity: 0.9951301293170639\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (565313, 565313) entries\n",
      "Student model 7-37 trained with depth 32 and 2778 leaves:\n",
      "Student model score: 0.9978630532380667\n",
      "Student model 7-37 fidelity: 0.9978630532396723\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (566733, 566733) entries\n",
      "Student model 7-38 trained with depth 32 and 2779 leaves:\n",
      "Student model score: 0.9851324409162241\n",
      "Student model 7-38 fidelity: 0.9851324407774809\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (568153, 568153) entries\n",
      "Student model 7-39 trained with depth 31 and 2742 leaves:\n",
      "Student model score: 0.9951170512402733\n",
      "Student model 7-39 fidelity: 0.9951170512429712\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (569573, 569573) entries\n",
      "Student model 7-40 trained with depth 30 and 2750 leaves:\n",
      "Student model score: 0.9919561389506545\n",
      "Student model 7-40 fidelity: 0.9919561388224667\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (570993, 570993) entries\n",
      "Student model 7-41 trained with depth 34 and 2784 leaves:\n",
      "Student model score: 0.993870775384683\n",
      "Student model 7-41 fidelity: 0.9938707753813391\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (572413, 572413) entries\n",
      "Student model 7-42 trained with depth 32 and 2771 leaves:\n",
      "Student model score: 0.98935835768631\n",
      "Student model 7-42 fidelity: 0.9893583577461383\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (573833, 573833) entries\n",
      "Student model 7-43 trained with depth 30 and 2779 leaves:\n",
      "Student model score: 0.9959754385805676\n",
      "Student model 7-43 fidelity: 0.995975438588971\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (575253, 575253) entries\n",
      "Student model 7-44 trained with depth 34 and 2773 leaves:\n",
      "Student model score: 0.9979255514259934\n",
      "Student model 7-44 fidelity: 0.9979255514252711\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (576673, 576673) entries\n",
      "Student model 7-45 trained with depth 30 and 2763 leaves:\n",
      "Student model score: 0.9898104351281225\n",
      "Student model 7-45 fidelity: 0.9898104349981398\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (578093, 578093) entries\n",
      "Student model 7-46 trained with depth 38 and 2770 leaves:\n",
      "Student model score: 0.9951512641991579\n",
      "Student model 7-46 fidelity: 0.9951512642003055\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (579513, 579513) entries\n",
      "Student model 7-47 trained with depth 36 and 2748 leaves:\n",
      "Student model score: 0.985871803700948\n",
      "Student model 7-47 fidelity: 0.9858718037044933\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (580933, 580933) entries\n",
      "Student model 7-48 trained with depth 36 and 2771 leaves:\n",
      "Student model score: 0.9891760299108984\n",
      "Student model 7-48 fidelity: 0.989176029796809\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (582353, 582353) entries\n",
      "Student model 7-49 trained with depth 32 and 2770 leaves:\n",
      "Student model score: 0.988764240674415\n",
      "Student model 7-49 fidelity: 0.9887642406605548\n",
      "########## Outer-loop Iteration 8/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (583773, 583773) entries\n",
      "Student model 8-0 trained with depth 32 and 2754 leaves:\n",
      "Student model score: 0.9811293316844056\n",
      "Student model 8-0 fidelity: 0.9811293315069429\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (585193, 585193) entries\n",
      "Student model 8-1 trained with depth 31 and 2744 leaves:\n",
      "Student model score: 0.9971543703171287\n",
      "Student model 8-1 fidelity: 0.9971543703215038\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (586613, 586613) entries\n",
      "Student model 8-2 trained with depth 35 and 2769 leaves:\n",
      "Student model score: 0.9983212451913838\n",
      "Student model 8-2 fidelity: 0.998321245314849\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (588033, 588033) entries\n",
      "Student model 8-3 trained with depth 32 and 2743 leaves:\n",
      "Student model score: 0.9849672027246049\n",
      "Student model 8-3 fidelity: 0.9849672027142644\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (589453, 589453) entries\n",
      "Student model 8-4 trained with depth 29 and 2787 leaves:\n",
      "Student model score: 0.9987927220738274\n",
      "Student model 8-4 fidelity: 0.9987927220732075\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (590873, 590873) entries\n",
      "Student model 8-5 trained with depth 31 and 2777 leaves:\n",
      "Student model score: 0.99845760537939\n",
      "Student model 8-5 fidelity: 0.9984576053797736\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (592293, 592293) entries\n",
      "Student model 8-6 trained with depth 33 and 2778 leaves:\n",
      "Student model score: 0.9944141020895566\n",
      "Student model 8-6 fidelity: 0.9944141021096149\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (593713, 593713) entries\n",
      "Student model 8-7 trained with depth 32 and 2756 leaves:\n",
      "Student model score: 0.9953461453144082\n",
      "Student model 8-7 fidelity: 0.9953461453161145\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (595133, 595133) entries\n",
      "Student model 8-8 trained with depth 33 and 2774 leaves:\n",
      "Student model score: 0.9985145979039939\n",
      "Student model 8-8 fidelity: 0.9985145979339036\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (596553, 596553) entries\n",
      "Student model 8-9 trained with depth 29 and 2781 leaves:\n",
      "Student model score: 0.9980248726789692\n",
      "Student model 8-9 fidelity: 0.9980248728023344\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (597973, 597973) entries\n",
      "Student model 8-10 trained with depth 31 and 2766 leaves:\n",
      "Student model score: 0.9968429685082488\n",
      "Student model 8-10 fidelity: 0.9968429687092084\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (599393, 599393) entries\n",
      "Student model 8-11 trained with depth 35 and 2739 leaves:\n",
      "Student model score: 0.9985775950453251\n",
      "Student model 8-11 fidelity: 0.9985775950193094\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (600813, 600813) entries\n",
      "Student model 8-12 trained with depth 37 and 2764 leaves:\n",
      "Student model score: 0.9941723618446012\n",
      "Student model 8-12 fidelity: 0.9941723618868772\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (602233, 602233) entries\n",
      "Student model 8-13 trained with depth 30 and 2773 leaves:\n",
      "Student model score: 0.9944888678758607\n",
      "Student model 8-13 fidelity: 0.9944888678835044\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (603653, 603653) entries\n",
      "Student model 8-14 trained with depth 32 and 2762 leaves:\n",
      "Student model score: 0.998273257475016\n",
      "Student model 8-14 fidelity: 0.9982732574759074\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (605073, 605073) entries\n",
      "Student model 8-15 trained with depth 35 and 2758 leaves:\n",
      "Student model score: 0.9964868137595408\n",
      "Student model 8-15 fidelity: 0.9964868137603782\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (606493, 606493) entries\n",
      "Student model 8-16 trained with depth 32 and 2778 leaves:\n",
      "Student model score: 0.9985062387746282\n",
      "Student model 8-16 fidelity: 0.998506238775257\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (607913, 607913) entries\n",
      "Student model 8-17 trained with depth 32 and 2787 leaves:\n",
      "Student model score: 0.9976639116959074\n",
      "Student model 8-17 fidelity: 0.9976639114203761\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (609333, 609333) entries\n",
      "Student model 8-18 trained with depth 30 and 2745 leaves:\n",
      "Student model score: 0.9982595149146415\n",
      "Student model 8-18 fidelity: 0.9982595148907131\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (610753, 610753) entries\n",
      "Student model 8-19 trained with depth 33 and 2745 leaves:\n",
      "Student model score: 0.9925159759358497\n",
      "Student model 8-19 fidelity: 0.9925159759442663\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (612173, 612173) entries\n",
      "Student model 8-20 trained with depth 36 and 2782 leaves:\n",
      "Student model score: 0.989420873635906\n",
      "Student model 8-20 fidelity: 0.9894208736084015\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (613593, 613593) entries\n",
      "Student model 8-21 trained with depth 30 and 2787 leaves:\n",
      "Student model score: 0.9988360185442938\n",
      "Student model 8-21 fidelity: 0.9988360185332952\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (615013, 615013) entries\n",
      "Student model 8-22 trained with depth 32 and 2770 leaves:\n",
      "Student model score: 0.9985671215241101\n",
      "Student model 8-22 fidelity: 0.9985671214095194\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (616433, 616433) entries\n",
      "Student model 8-23 trained with depth 31 and 2768 leaves:\n",
      "Student model score: 0.99450520444178\n",
      "Student model 8-23 fidelity: 0.9945052044411731\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (617853, 617853) entries\n",
      "Student model 8-24 trained with depth 30 and 2774 leaves:\n",
      "Student model score: 0.9971185583456925\n",
      "Student model 8-24 fidelity: 0.9971185583484437\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (619273, 619273) entries\n",
      "Student model 8-25 trained with depth 33 and 2742 leaves:\n",
      "Student model score: 0.990807725256905\n",
      "Student model 8-25 fidelity: 0.9908077252515145\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (620693, 620693) entries\n",
      "Student model 8-26 trained with depth 28 and 2771 leaves:\n",
      "Student model score: 0.9982150534058314\n",
      "Student model 8-26 fidelity: 0.998215053408806\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (622113, 622113) entries\n",
      "Student model 8-27 trained with depth 34 and 2808 leaves:\n",
      "Student model score: 0.9954094297636706\n",
      "Student model 8-27 fidelity: 0.9954094297751249\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (623533, 623533) entries\n",
      "Student model 8-28 trained with depth 36 and 2740 leaves:\n",
      "Student model score: 0.9950105312409949\n",
      "Student model 8-28 fidelity: 0.99501053116989\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (624953, 624953) entries\n",
      "Student model 8-29 trained with depth 34 and 2792 leaves:\n",
      "Student model score: 0.9926804951858377\n",
      "Student model 8-29 fidelity: 0.9926804951838316\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (626373, 626373) entries\n",
      "Student model 8-30 trained with depth 31 and 2717 leaves:\n",
      "Student model score: 0.9977928472868939\n",
      "Student model 8-30 fidelity: 0.9977928472942571\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (627793, 627793) entries\n",
      "Student model 8-31 trained with depth 30 and 2738 leaves:\n",
      "Student model score: 0.9941707286608801\n",
      "Student model 8-31 fidelity: 0.9941707286593817\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (629213, 629213) entries\n",
      "Student model 8-32 trained with depth 34 and 2751 leaves:\n",
      "Student model score: 0.9980683419987216\n",
      "Student model 8-32 fidelity: 0.9980683419975416\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (630633, 630633) entries\n",
      "Student model 8-33 trained with depth 32 and 2750 leaves:\n",
      "Student model score: 0.9941590184538306\n",
      "Student model 8-33 fidelity: 0.9941590184310416\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (632053, 632053) entries\n",
      "Student model 8-34 trained with depth 30 and 2782 leaves:\n",
      "Student model score: 0.9928607923083371\n",
      "Student model 8-34 fidelity: 0.9928607923104179\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (633473, 633473) entries\n",
      "Student model 8-35 trained with depth 34 and 2746 leaves:\n",
      "Student model score: 0.9977594435214482\n",
      "Student model 8-35 fidelity: 0.9977594435203595\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (634893, 634893) entries\n",
      "Student model 8-36 trained with depth 29 and 2784 leaves:\n",
      "Student model score: 0.9929610327342714\n",
      "Student model 8-36 fidelity: 0.9929610327319383\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (636313, 636313) entries\n",
      "Student model 8-37 trained with depth 34 and 2735 leaves:\n",
      "Student model score: 0.9978451891496672\n",
      "Student model 8-37 fidelity: 0.9978451891485404\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (637733, 637733) entries\n",
      "Student model 8-38 trained with depth 34 and 2772 leaves:\n",
      "Student model score: 0.9947293593214854\n",
      "Student model 8-38 fidelity: 0.9947293594697638\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (639153, 639153) entries\n",
      "Student model 8-39 trained with depth 33 and 2805 leaves:\n",
      "Student model score: 0.9957512991016269\n",
      "Student model 8-39 fidelity: 0.9957512991153038\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (640573, 640573) entries\n",
      "Student model 8-40 trained with depth 36 and 2758 leaves:\n",
      "Student model score: 0.9942588890256578\n",
      "Student model 8-40 fidelity: 0.9942588890643979\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (641993, 641993) entries\n",
      "Student model 8-41 trained with depth 30 and 2760 leaves:\n",
      "Student model score: 0.9947974392288381\n",
      "Student model 8-41 fidelity: 0.9947974392217124\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (643413, 643413) entries\n",
      "Student model 8-42 trained with depth 32 and 2775 leaves:\n",
      "Student model score: 0.9986591527007874\n",
      "Student model 8-42 fidelity: 0.9986591525799445\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (644833, 644833) entries\n",
      "Student model 8-43 trained with depth 32 and 2762 leaves:\n",
      "Student model score: 0.998498213198985\n",
      "Student model 8-43 fidelity: 0.9984982131243372\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (646253, 646253) entries\n",
      "Student model 8-44 trained with depth 33 and 2766 leaves:\n",
      "Student model score: 0.9962875835909318\n",
      "Student model 8-44 fidelity: 0.9962875835908244\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (647673, 647673) entries\n",
      "Student model 8-45 trained with depth 31 and 2738 leaves:\n",
      "Student model score: 0.992614635165013\n",
      "Student model 8-45 fidelity: 0.9926146351652657\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (649093, 649093) entries\n",
      "Student model 8-46 trained with depth 32 and 2721 leaves:\n",
      "Student model score: 0.9955048225598505\n",
      "Student model 8-46 fidelity: 0.9955048225621931\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (650513, 650513) entries\n",
      "Student model 8-47 trained with depth 36 and 2776 leaves:\n",
      "Student model score: 0.9985308032051585\n",
      "Student model 8-47 fidelity: 0.9985308032039267\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (651933, 651933) entries\n",
      "Student model 8-48 trained with depth 30 and 2761 leaves:\n",
      "Student model score: 0.9957421331545352\n",
      "Student model 8-48 fidelity: 0.9957421331703779\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (653353, 653353) entries\n",
      "Student model 8-49 trained with depth 31 and 2736 leaves:\n",
      "Student model score: 0.9976629973359524\n",
      "Student model 8-49 fidelity: 0.9976629973407799\n",
      "########## Outer-loop Iteration 9/10 ##########\n",
      "Initializing Trustee inner-loop with 10 iterations\n",
      "########## Inner-loop Iteration 0/50 ##########\n",
      "Sampling 4731 points from training dataset with (654773, 654773) entries\n",
      "Student model 9-0 trained with depth 31 and 2763 leaves:\n",
      "Student model score: 0.9924715626088451\n",
      "Student model 9-0 fidelity: 0.9924715626164501\n",
      "########## Inner-loop Iteration 1/50 ##########\n",
      "Sampling 4731 points from training dataset with (656193, 656193) entries\n",
      "Student model 9-1 trained with depth 30 and 2804 leaves:\n",
      "Student model score: 0.997528266143447\n",
      "Student model 9-1 fidelity: 0.9975282661424859\n",
      "########## Inner-loop Iteration 2/50 ##########\n",
      "Sampling 4731 points from training dataset with (657613, 657613) entries\n",
      "Student model 9-2 trained with depth 33 and 2776 leaves:\n",
      "Student model score: 0.9924465165667431\n",
      "Student model 9-2 fidelity: 0.9924465165664667\n",
      "########## Inner-loop Iteration 3/50 ##########\n",
      "Sampling 4731 points from training dataset with (659033, 659033) entries\n",
      "Student model 9-3 trained with depth 30 and 2738 leaves:\n",
      "Student model score: 0.9949690872638035\n",
      "Student model 9-3 fidelity: 0.994969087264203\n",
      "########## Inner-loop Iteration 4/50 ##########\n",
      "Sampling 4731 points from training dataset with (660453, 660453) entries\n",
      "Student model 9-4 trained with depth 35 and 2767 leaves:\n",
      "Student model score: 0.9929198107597902\n",
      "Student model 9-4 fidelity: 0.9929198107581534\n",
      "########## Inner-loop Iteration 5/50 ##########\n",
      "Sampling 4731 points from training dataset with (661873, 661873) entries\n",
      "Student model 9-5 trained with depth 29 and 2762 leaves:\n",
      "Student model score: 0.9974121936823809\n",
      "Student model 9-5 fidelity: 0.997412193693817\n",
      "########## Inner-loop Iteration 6/50 ##########\n",
      "Sampling 4731 points from training dataset with (663293, 663293) entries\n",
      "Student model 9-6 trained with depth 31 and 2793 leaves:\n",
      "Student model score: 0.9987272177916816\n",
      "Student model 9-6 fidelity: 0.9987272177606658\n",
      "########## Inner-loop Iteration 7/50 ##########\n",
      "Sampling 4731 points from training dataset with (664713, 664713) entries\n",
      "Student model 9-7 trained with depth 32 and 2767 leaves:\n",
      "Student model score: 0.9926564300659695\n",
      "Student model 9-7 fidelity: 0.9926564301212107\n",
      "########## Inner-loop Iteration 8/50 ##########\n",
      "Sampling 4731 points from training dataset with (666133, 666133) entries\n",
      "Student model 9-8 trained with depth 31 and 2740 leaves:\n",
      "Student model score: 0.9980356990957886\n",
      "Student model 9-8 fidelity: 0.9980356990691589\n",
      "########## Inner-loop Iteration 9/50 ##########\n",
      "Sampling 4731 points from training dataset with (667553, 667553) entries\n",
      "Student model 9-9 trained with depth 33 and 2781 leaves:\n",
      "Student model score: 0.9954133113927229\n",
      "Student model 9-9 fidelity: 0.995413311389395\n",
      "########## Inner-loop Iteration 10/50 ##########\n",
      "Sampling 4731 points from training dataset with (668973, 668973) entries\n",
      "Student model 9-10 trained with depth 31 and 2761 leaves:\n",
      "Student model score: 0.9963604348309602\n",
      "Student model 9-10 fidelity: 0.9963604348223464\n",
      "########## Inner-loop Iteration 11/50 ##########\n",
      "Sampling 4731 points from training dataset with (670393, 670393) entries\n",
      "Student model 9-11 trained with depth 39 and 2738 leaves:\n",
      "Student model score: 0.9922061799740828\n",
      "Student model 9-11 fidelity: 0.9922061799602548\n",
      "########## Inner-loop Iteration 12/50 ##########\n",
      "Sampling 4731 points from training dataset with (671813, 671813) entries\n",
      "Student model 9-12 trained with depth 31 and 2754 leaves:\n",
      "Student model score: 0.998015448045608\n",
      "Student model 9-12 fidelity: 0.9980154480469021\n",
      "########## Inner-loop Iteration 13/50 ##########\n",
      "Sampling 4731 points from training dataset with (673233, 673233) entries\n",
      "Student model 9-13 trained with depth 34 and 2787 leaves:\n",
      "Student model score: 0.987914968732617\n",
      "Student model 9-13 fidelity: 0.9879149687666787\n",
      "########## Inner-loop Iteration 14/50 ##########\n",
      "Sampling 4731 points from training dataset with (674653, 674653) entries\n",
      "Student model 9-14 trained with depth 31 and 2765 leaves:\n",
      "Student model score: 0.9954367784985905\n",
      "Student model 9-14 fidelity: 0.995436778513628\n",
      "########## Inner-loop Iteration 15/50 ##########\n",
      "Sampling 4731 points from training dataset with (676073, 676073) entries\n",
      "Student model 9-15 trained with depth 32 and 2771 leaves:\n",
      "Student model score: 0.9923935440410838\n",
      "Student model 9-15 fidelity: 0.992393544039925\n",
      "########## Inner-loop Iteration 16/50 ##########\n",
      "Sampling 4731 points from training dataset with (677493, 677493) entries\n",
      "Student model 9-16 trained with depth 32 and 2786 leaves:\n",
      "Student model score: 0.9943520252292725\n",
      "Student model 9-16 fidelity: 0.9943520252274626\n",
      "########## Inner-loop Iteration 17/50 ##########\n",
      "Sampling 4731 points from training dataset with (678913, 678913) entries\n",
      "Student model 9-17 trained with depth 29 and 2763 leaves:\n",
      "Student model score: 0.9987982241126121\n",
      "Student model 9-17 fidelity: 0.9987982241150117\n",
      "########## Inner-loop Iteration 18/50 ##########\n",
      "Sampling 4731 points from training dataset with (680333, 680333) entries\n",
      "Student model 9-18 trained with depth 30 and 2753 leaves:\n",
      "Student model score: 0.9893683726561247\n",
      "Student model 9-18 fidelity: 0.9893683726666392\n",
      "########## Inner-loop Iteration 19/50 ##########\n",
      "Sampling 4731 points from training dataset with (681753, 681753) entries\n",
      "Student model 9-19 trained with depth 36 and 2760 leaves:\n",
      "Student model score: 0.9983331567269969\n",
      "Student model 9-19 fidelity: 0.998333156892317\n",
      "########## Inner-loop Iteration 20/50 ##########\n",
      "Sampling 4731 points from training dataset with (683173, 683173) entries\n",
      "Student model 9-20 trained with depth 35 and 2742 leaves:\n",
      "Student model score: 0.998808183728187\n",
      "Student model 9-20 fidelity: 0.9988081837526077\n",
      "########## Inner-loop Iteration 21/50 ##########\n",
      "Sampling 4731 points from training dataset with (684593, 684593) entries\n",
      "Student model 9-21 trained with depth 29 and 2774 leaves:\n",
      "Student model score: 0.9974099481935531\n",
      "Student model 9-21 fidelity: 0.9974099481933058\n",
      "########## Inner-loop Iteration 22/50 ##########\n",
      "Sampling 4731 points from training dataset with (686013, 686013) entries\n",
      "Student model 9-22 trained with depth 35 and 2795 leaves:\n",
      "Student model score: 0.9982050311961083\n",
      "Student model 9-22 fidelity: 0.9982050311563407\n",
      "########## Inner-loop Iteration 23/50 ##########\n",
      "Sampling 4731 points from training dataset with (687433, 687433) entries\n",
      "Student model 9-23 trained with depth 35 and 2785 leaves:\n",
      "Student model score: 0.9990066063973253\n",
      "Student model 9-23 fidelity: 0.9990066063945106\n",
      "########## Inner-loop Iteration 24/50 ##########\n",
      "Sampling 4731 points from training dataset with (688853, 688853) entries\n",
      "Student model 9-24 trained with depth 30 and 2752 leaves:\n",
      "Student model score: 0.9989526485248269\n",
      "Student model 9-24 fidelity: 0.9989526485316186\n",
      "########## Inner-loop Iteration 25/50 ##########\n",
      "Sampling 4731 points from training dataset with (690273, 690273) entries\n",
      "Student model 9-25 trained with depth 32 and 2767 leaves:\n",
      "Student model score: 0.9930741932124646\n",
      "Student model 9-25 fidelity: 0.9930741932123393\n",
      "########## Inner-loop Iteration 26/50 ##########\n",
      "Sampling 4731 points from training dataset with (691693, 691693) entries\n",
      "Student model 9-26 trained with depth 30 and 2764 leaves:\n",
      "Student model score: 0.9986525605747002\n",
      "Student model 9-26 fidelity: 0.9986525605777609\n",
      "########## Inner-loop Iteration 27/50 ##########\n",
      "Sampling 4731 points from training dataset with (693113, 693113) entries\n",
      "Student model 9-27 trained with depth 30 and 2755 leaves:\n",
      "Student model score: 0.9968998821679685\n",
      "Student model 9-27 fidelity: 0.9968998821680541\n",
      "########## Inner-loop Iteration 28/50 ##########\n",
      "Sampling 4731 points from training dataset with (694533, 694533) entries\n",
      "Student model 9-28 trained with depth 36 and 2763 leaves:\n",
      "Student model score: 0.998670210480445\n",
      "Student model 9-28 fidelity: 0.9986702104835877\n",
      "########## Inner-loop Iteration 29/50 ##########\n",
      "Sampling 4731 points from training dataset with (695953, 695953) entries\n",
      "Student model 9-29 trained with depth 29 and 2792 leaves:\n",
      "Student model score: 0.9949908556274282\n",
      "Student model 9-29 fidelity: 0.994990855620887\n",
      "########## Inner-loop Iteration 30/50 ##########\n",
      "Sampling 4731 points from training dataset with (697373, 697373) entries\n",
      "Student model 9-30 trained with depth 31 and 2732 leaves:\n",
      "Student model score: 0.9925420830897045\n",
      "Student model 9-30 fidelity: 0.9925420830392349\n",
      "########## Inner-loop Iteration 31/50 ##########\n",
      "Sampling 4731 points from training dataset with (698793, 698793) entries\n",
      "Student model 9-31 trained with depth 33 and 2745 leaves:\n",
      "Student model score: 0.9945793744850241\n",
      "Student model 9-31 fidelity: 0.994579374495095\n",
      "########## Inner-loop Iteration 32/50 ##########\n",
      "Sampling 4731 points from training dataset with (700213, 700213) entries\n",
      "Student model 9-32 trained with depth 31 and 2768 leaves:\n",
      "Student model score: 0.9892673846497101\n",
      "Student model 9-32 fidelity: 0.9892673846503849\n",
      "########## Inner-loop Iteration 33/50 ##########\n",
      "Sampling 4731 points from training dataset with (701633, 701633) entries\n",
      "Student model 9-33 trained with depth 30 and 2767 leaves:\n",
      "Student model score: 0.9987216166326004\n",
      "Student model 9-33 fidelity: 0.9987216166259406\n",
      "########## Inner-loop Iteration 34/50 ##########\n",
      "Sampling 4731 points from training dataset with (703053, 703053) entries\n",
      "Student model 9-34 trained with depth 31 and 2762 leaves:\n",
      "Student model score: 0.9947415681681961\n",
      "Student model 9-34 fidelity: 0.9947415681702625\n",
      "########## Inner-loop Iteration 35/50 ##########\n",
      "Sampling 4731 points from training dataset with (704473, 704473) entries\n",
      "Student model 9-35 trained with depth 30 and 2784 leaves:\n",
      "Student model score: 0.9945413995916824\n",
      "Student model 9-35 fidelity: 0.9945413995838687\n",
      "########## Inner-loop Iteration 36/50 ##########\n",
      "Sampling 4731 points from training dataset with (705893, 705893) entries\n",
      "Student model 9-36 trained with depth 33 and 2751 leaves:\n",
      "Student model score: 0.9986843436523828\n",
      "Student model 9-36 fidelity: 0.9986843436552243\n",
      "########## Inner-loop Iteration 37/50 ##########\n",
      "Sampling 4731 points from training dataset with (707313, 707313) entries\n",
      "Student model 9-37 trained with depth 31 and 2727 leaves:\n",
      "Student model score: 0.992615663817292\n",
      "Student model 9-37 fidelity: 0.9926156638122495\n",
      "########## Inner-loop Iteration 38/50 ##########\n",
      "Sampling 4731 points from training dataset with (708733, 708733) entries\n",
      "Student model 9-38 trained with depth 31 and 2782 leaves:\n",
      "Student model score: 0.9935137998248477\n",
      "Student model 9-38 fidelity: 0.9935137998280721\n",
      "########## Inner-loop Iteration 39/50 ##########\n",
      "Sampling 4731 points from training dataset with (710153, 710153) entries\n",
      "Student model 9-39 trained with depth 30 and 2772 leaves:\n",
      "Student model score: 0.9986083921747703\n",
      "Student model 9-39 fidelity: 0.9986083924273526\n",
      "########## Inner-loop Iteration 40/50 ##########\n",
      "Sampling 4731 points from training dataset with (711573, 711573) entries\n",
      "Student model 9-40 trained with depth 33 and 2782 leaves:\n",
      "Student model score: 0.9943410575938763\n",
      "Student model 9-40 fidelity: 0.9943410575943062\n",
      "########## Inner-loop Iteration 41/50 ##########\n",
      "Sampling 4731 points from training dataset with (712993, 712993) entries\n",
      "Student model 9-41 trained with depth 31 and 2770 leaves:\n",
      "Student model score: 0.9930427648549149\n",
      "Student model 9-41 fidelity: 0.9930427647698247\n",
      "########## Inner-loop Iteration 42/50 ##########\n",
      "Sampling 4731 points from training dataset with (714413, 714413) entries\n",
      "Student model 9-42 trained with depth 32 and 2751 leaves:\n",
      "Student model score: 0.9980027539642223\n",
      "Student model 9-42 fidelity: 0.9980027539435898\n",
      "########## Inner-loop Iteration 43/50 ##########\n",
      "Sampling 4731 points from training dataset with (715833, 715833) entries\n",
      "Student model 9-43 trained with depth 33 and 2798 leaves:\n",
      "Student model score: 0.9984927595046741\n",
      "Student model 9-43 fidelity: 0.9984927593987998\n",
      "########## Inner-loop Iteration 44/50 ##########\n",
      "Sampling 4731 points from training dataset with (717253, 717253) entries\n",
      "Student model 9-44 trained with depth 33 and 2772 leaves:\n",
      "Student model score: 0.9926257382489375\n",
      "Student model 9-44 fidelity: 0.99262573824977\n",
      "########## Inner-loop Iteration 45/50 ##########\n",
      "Sampling 4731 points from training dataset with (718673, 718673) entries\n",
      "Student model 9-45 trained with depth 33 and 2742 leaves:\n",
      "Student model score: 0.9932102549662402\n",
      "Student model 9-45 fidelity: 0.9932102549874172\n",
      "########## Inner-loop Iteration 46/50 ##########\n",
      "Sampling 4731 points from training dataset with (720093, 720093) entries\n",
      "Student model 9-46 trained with depth 32 and 2788 leaves:\n",
      "Student model score: 0.9977494068547709\n",
      "Student model 9-46 fidelity: 0.997749406890177\n",
      "########## Inner-loop Iteration 47/50 ##########\n",
      "Sampling 4731 points from training dataset with (721513, 721513) entries\n",
      "Student model 9-47 trained with depth 29 and 2775 leaves:\n",
      "Student model score: 0.9887416966863795\n",
      "Student model 9-47 fidelity: 0.9887416966695205\n",
      "########## Inner-loop Iteration 48/50 ##########\n",
      "Sampling 4731 points from training dataset with (722933, 722933) entries\n",
      "Student model 9-48 trained with depth 38 and 2740 leaves:\n",
      "Student model score: 0.9942798212793711\n",
      "Student model 9-48 fidelity: 0.9942798212857855\n",
      "########## Inner-loop Iteration 49/50 ##########\n",
      "Sampling 4731 points from training dataset with (724353, 724353) entries\n",
      "Student model 9-49 trained with depth 32 and 2752 leaves:\n",
      "Student model score: 0.9905476016397321\n",
      "Student model 9-49 fidelity: 0.9905476016450694\n"
     ]
    }
   ],
   "source": [
    "trustee = RegressionTrustee(expert=regression)\n",
    "trustee.fit(scaler.transform(X), y, num_iter=50, num_stability_iter=10, samples_size=0.3, top_k=5, verbose=True, predict_method_name='predict_trustee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "258e5d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model explanation training (agreement, fidelity): (0.9581595067212831, 0.9990066063945106)\n",
      "Model Explanation size: 5569\n",
      "Top-k Prunned Model explanation size: 115\n"
     ]
    }
   ],
   "source": [
    "dt, pruned_dt, agreement, reward = trustee.explain()\n",
    "print(f\"Model explanation training (agreement, fidelity): ({agreement}, {reward})\")\n",
    "print(f\"Model Explanation size: {dt.tree_.node_count}\")\n",
    "print(f\"Top-k Prunned Model explanation size: {pruned_dt.tree_.node_count}\")\n",
    "\n",
    "# Use explanations to make predictions\n",
    "dt_y_pred = dt.predict(X_test)\n",
    "pruned_dt_y_pred = pruned_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ab901a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"2317pt\" height=\"2661pt\"\n",
       " viewBox=\"0.00 0.00 2316.50 2661.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2657)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-2657 2312.5,-2657 2312.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#f9e2d2\" stroke=\"black\" d=\"M1303,-2653C1303,-2653 1108,-2653 1108,-2653 1102,-2653 1096,-2647 1096,-2641 1096,-2641 1096,-2597 1096,-2597 1096,-2591 1102,-2585 1108,-2585 1108,-2585 1303,-2585 1303,-2585 1309,-2585 1315,-2591 1315,-2597 1315,-2597 1315,-2641 1315,-2641 1315,-2647 1309,-2653 1303,-2653\"/>\n",
       "<text text-anchor=\"start\" x=\"1145.5\" y=\"-2637.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent â‰¤ &#45;0.1</text>\n",
       "<text text-anchor=\"start\" x=\"1104\" y=\"-2622.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 158158.378</text>\n",
       "<text text-anchor=\"start\" x=\"1148\" y=\"-2607.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3311</text>\n",
       "<text text-anchor=\"start\" x=\"1146.5\" y=\"-2592.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 215.949</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#fefbf9\" stroke=\"black\" d=\"M1182,-2549C1182,-2549 1005,-2549 1005,-2549 999,-2549 993,-2543 993,-2537 993,-2537 993,-2493 993,-2493 993,-2487 999,-2481 1005,-2481 1005,-2481 1182,-2481 1182,-2481 1188,-2481 1194,-2487 1194,-2493 1194,-2493 1194,-2537 1194,-2537 1194,-2543 1188,-2549 1182,-2549\"/>\n",
       "<text text-anchor=\"start\" x=\"1047\" y=\"-2533.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ &#45;0.845</text>\n",
       "<text text-anchor=\"start\" x=\"1001\" y=\"-2518.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 8247.964</text>\n",
       "<text text-anchor=\"start\" x=\"1036\" y=\"-2503.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2636</text>\n",
       "<text text-anchor=\"start\" x=\"1039\" y=\"-2488.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 29.924</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1169.14,-2584.88C1159.06,-2575.71 1148.02,-2565.65 1137.56,-2556.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1139.81,-2553.44 1130.06,-2549.3 1135.1,-2558.62 1139.81,-2553.44\"/>\n",
       "<text text-anchor=\"middle\" x=\"1131.23\" y=\"-2570.57\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 114 -->\n",
       "<g id=\"node115\" class=\"node\">\n",
       "<title>114</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M1410.5,-2541.5C1410.5,-2541.5 1224.5,-2541.5 1224.5,-2541.5 1218.5,-2541.5 1212.5,-2535.5 1212.5,-2529.5 1212.5,-2529.5 1212.5,-2500.5 1212.5,-2500.5 1212.5,-2494.5 1218.5,-2488.5 1224.5,-2488.5 1224.5,-2488.5 1410.5,-2488.5 1410.5,-2488.5 1416.5,-2488.5 1422.5,-2494.5 1422.5,-2500.5 1422.5,-2500.5 1422.5,-2529.5 1422.5,-2529.5 1422.5,-2535.5 1416.5,-2541.5 1410.5,-2541.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1220.5\" y=\"-2526.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 80696.739</text>\n",
       "<text text-anchor=\"start\" x=\"1264.5\" y=\"-2511.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 675</text>\n",
       "<text text-anchor=\"start\" x=\"1258.5\" y=\"-2496.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 942.413</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;114 -->\n",
       "<g id=\"edge114\" class=\"edge\">\n",
       "<title>0&#45;&gt;114</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1241.86,-2584.88C1254.66,-2573.23 1269.03,-2560.14 1281.78,-2548.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1284.44,-2550.84 1289.48,-2541.52 1279.73,-2545.67 1284.44,-2550.84\"/>\n",
       "<text text-anchor=\"middle\" x=\"1288.31\" y=\"-2562.79\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#e6843d\" stroke=\"black\" d=\"M1070,-2437.5C1070,-2437.5 911,-2437.5 911,-2437.5 905,-2437.5 899,-2431.5 899,-2425.5 899,-2425.5 899,-2396.5 899,-2396.5 899,-2390.5 905,-2384.5 911,-2384.5 911,-2384.5 1070,-2384.5 1070,-2384.5 1076,-2384.5 1082,-2390.5 1082,-2396.5 1082,-2396.5 1082,-2425.5 1082,-2425.5 1082,-2431.5 1076,-2437.5 1070,-2437.5\"/>\n",
       "<text text-anchor=\"start\" x=\"907\" y=\"-2422.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 98.042</text>\n",
       "<text text-anchor=\"start\" x=\"942\" y=\"-2407.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 11</text>\n",
       "<text text-anchor=\"start\" x=\"931.5\" y=\"-2392.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 923.424</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1060.06,-2480.88C1048.4,-2469.34 1035.32,-2456.39 1023.68,-2444.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1025.84,-2442.07 1016.27,-2437.52 1020.91,-2447.04 1025.84,-2442.07\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#fefcfa\" stroke=\"black\" d=\"M1280.5,-2445C1280.5,-2445 1112.5,-2445 1112.5,-2445 1106.5,-2445 1100.5,-2439 1100.5,-2433 1100.5,-2433 1100.5,-2389 1100.5,-2389 1100.5,-2383 1106.5,-2377 1112.5,-2377 1112.5,-2377 1280.5,-2377 1280.5,-2377 1286.5,-2377 1292.5,-2383 1292.5,-2389 1292.5,-2389 1292.5,-2433 1292.5,-2433 1292.5,-2439 1286.5,-2445 1280.5,-2445\"/>\n",
       "<text text-anchor=\"start\" x=\"1109\" y=\"-2429.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesRetrans_3 â‰¤ &#45;0.082</text>\n",
       "<text text-anchor=\"start\" x=\"1108.5\" y=\"-2414.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 4922.66</text>\n",
       "<text text-anchor=\"start\" x=\"1139\" y=\"-2399.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2625</text>\n",
       "<text text-anchor=\"start\" x=\"1146.5\" y=\"-2384.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 26.18</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1126.94,-2480.88C1136.12,-2471.8 1146.17,-2461.85 1155.7,-2452.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1158.23,-2454.82 1162.87,-2445.3 1153.31,-2449.85 1158.23,-2454.82\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#fefcfb\" stroke=\"black\" d=\"M1180,-2341C1180,-2341 1003,-2341 1003,-2341 997,-2341 991,-2335 991,-2329 991,-2329 991,-2285 991,-2285 991,-2279 997,-2273 1003,-2273 1003,-2273 1180,-2273 1180,-2273 1186,-2273 1192,-2279 1192,-2285 1192,-2285 1192,-2329 1192,-2329 1192,-2335 1186,-2341 1180,-2341\"/>\n",
       "<text text-anchor=\"start\" x=\"1022.5\" y=\"-2325.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent â‰¤ &#45;0.418</text>\n",
       "<text text-anchor=\"start\" x=\"999\" y=\"-2310.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1896.155</text>\n",
       "<text text-anchor=\"start\" x=\"1034\" y=\"-2295.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2615</text>\n",
       "<text text-anchor=\"start\" x=\"1037\" y=\"-2280.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 22.774</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1162.41,-2376.88C1153.06,-2367.8 1142.81,-2357.85 1133.09,-2348.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1135.39,-2345.76 1125.78,-2341.3 1130.51,-2350.78 1135.39,-2345.76\"/>\n",
       "</g>\n",
       "<!-- 113 -->\n",
       "<g id=\"node114\" class=\"node\">\n",
       "<title>113</title>\n",
       "<path fill=\"#e6843e\" stroke=\"black\" d=\"M1381,-2333.5C1381,-2333.5 1222,-2333.5 1222,-2333.5 1216,-2333.5 1210,-2327.5 1210,-2321.5 1210,-2321.5 1210,-2292.5 1210,-2292.5 1210,-2286.5 1216,-2280.5 1222,-2280.5 1222,-2280.5 1381,-2280.5 1381,-2280.5 1387,-2280.5 1393,-2286.5 1393,-2292.5 1393,-2292.5 1393,-2321.5 1393,-2321.5 1393,-2327.5 1387,-2333.5 1381,-2333.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1218\" y=\"-2318.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 61.299</text>\n",
       "<text text-anchor=\"start\" x=\"1253\" y=\"-2303.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n",
       "<text text-anchor=\"start\" x=\"1242.5\" y=\"-2288.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 916.831</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;113 -->\n",
       "<g id=\"edge113\" class=\"edge\">\n",
       "<title>3&#45;&gt;113</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1230.59,-2376.88C1242.59,-2365.23 1256.06,-2352.14 1268.01,-2340.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1270.5,-2343 1275.23,-2333.52 1265.62,-2337.98 1270.5,-2343\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M1070.5,-2237C1070.5,-2237 902.5,-2237 902.5,-2237 896.5,-2237 890.5,-2231 890.5,-2225 890.5,-2225 890.5,-2181 890.5,-2181 890.5,-2175 896.5,-2169 902.5,-2169 902.5,-2169 1070.5,-2169 1070.5,-2169 1076.5,-2169 1082.5,-2175 1082.5,-2181 1082.5,-2181 1082.5,-2225 1082.5,-2225 1082.5,-2231 1076.5,-2237 1070.5,-2237\"/>\n",
       "<text text-anchor=\"start\" x=\"911\" y=\"-2221.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime_1 â‰¤ &#45;1.532</text>\n",
       "<text text-anchor=\"start\" x=\"898.5\" y=\"-2206.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 287.641</text>\n",
       "<text text-anchor=\"start\" x=\"929\" y=\"-2191.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2310</text>\n",
       "<text text-anchor=\"start\" x=\"936.5\" y=\"-2176.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.813</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1057.41,-2272.88C1048.06,-2263.8 1037.81,-2253.85 1028.09,-2244.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1030.39,-2241.76 1020.78,-2237.3 1025.51,-2246.78 1030.39,-2241.76\"/>\n",
       "</g>\n",
       "<!-- 112 -->\n",
       "<g id=\"node113\" class=\"node\">\n",
       "<title>112</title>\n",
       "<path fill=\"#fcefe6\" stroke=\"black\" d=\"M1280.5,-2229.5C1280.5,-2229.5 1112.5,-2229.5 1112.5,-2229.5 1106.5,-2229.5 1100.5,-2223.5 1100.5,-2217.5 1100.5,-2217.5 1100.5,-2188.5 1100.5,-2188.5 1100.5,-2182.5 1106.5,-2176.5 1112.5,-2176.5 1112.5,-2176.5 1280.5,-2176.5 1280.5,-2176.5 1286.5,-2176.5 1292.5,-2182.5 1292.5,-2188.5 1292.5,-2188.5 1292.5,-2217.5 1292.5,-2217.5 1292.5,-2223.5 1286.5,-2229.5 1280.5,-2229.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1108.5\" y=\"-2214.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 3169.96</text>\n",
       "<text text-anchor=\"start\" x=\"1143.5\" y=\"-2199.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 305</text>\n",
       "<text text-anchor=\"start\" x=\"1137.5\" y=\"-2184.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 120.939</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;112 -->\n",
       "<g id=\"edge112\" class=\"edge\">\n",
       "<title>4&#45;&gt;112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1125.59,-2272.88C1137.59,-2261.23 1151.06,-2248.14 1163.01,-2236.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1165.5,-2239 1170.23,-2229.52 1160.62,-2233.98 1165.5,-2239\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#fefaf8\" stroke=\"black\" d=\"M970,-2125.5C970,-2125.5 793,-2125.5 793,-2125.5 787,-2125.5 781,-2119.5 781,-2113.5 781,-2113.5 781,-2084.5 781,-2084.5 781,-2078.5 787,-2072.5 793,-2072.5 793,-2072.5 970,-2072.5 970,-2072.5 976,-2072.5 982,-2078.5 982,-2084.5 982,-2084.5 982,-2113.5 982,-2113.5 982,-2119.5 976,-2125.5 970,-2125.5\"/>\n",
       "<text text-anchor=\"start\" x=\"789\" y=\"-2110.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2791.309</text>\n",
       "<text text-anchor=\"start\" x=\"828.5\" y=\"-2095.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 169</text>\n",
       "<text text-anchor=\"start\" x=\"831.5\" y=\"-2080.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 36.93</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M952.41,-2168.88C940.41,-2157.23 926.94,-2144.14 914.99,-2132.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"917.38,-2129.98 907.77,-2125.52 912.5,-2135 917.38,-2129.98\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1171,-2133C1171,-2133 1012,-2133 1012,-2133 1006,-2133 1000,-2127 1000,-2121 1000,-2121 1000,-2077 1000,-2077 1000,-2071 1006,-2065 1012,-2065 1012,-2065 1171,-2065 1171,-2065 1177,-2065 1183,-2071 1183,-2077 1183,-2077 1183,-2121 1183,-2121 1183,-2127 1177,-2133 1171,-2133\"/>\n",
       "<text text-anchor=\"start\" x=\"1037\" y=\"-2117.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_1 â‰¤ &#45;0.149</text>\n",
       "<text text-anchor=\"start\" x=\"1008\" y=\"-2102.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 27.389</text>\n",
       "<text text-anchor=\"start\" x=\"1034\" y=\"-2087.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2141</text>\n",
       "<text text-anchor=\"start\" x=\"1041.5\" y=\"-2072.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.672</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1020.59,-2168.88C1029.94,-2159.8 1040.19,-2149.85 1049.91,-2140.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1052.49,-2142.78 1057.22,-2133.3 1047.61,-2137.76 1052.49,-2142.78\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M1067.5,-2029C1067.5,-2029 903.5,-2029 903.5,-2029 897.5,-2029 891.5,-2023 891.5,-2017 891.5,-2017 891.5,-1973 891.5,-1973 891.5,-1967 897.5,-1961 903.5,-1961 903.5,-1961 1067.5,-1961 1067.5,-1961 1073.5,-1961 1079.5,-1967 1079.5,-1973 1079.5,-1973 1079.5,-2017 1079.5,-2017 1079.5,-2023 1073.5,-2029 1067.5,-2029\"/>\n",
       "<text text-anchor=\"start\" x=\"899.5\" y=\"-2013.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_1 â‰¤ &#45;1.421</text>\n",
       "<text text-anchor=\"start\" x=\"902\" y=\"-1998.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 41.546</text>\n",
       "<text text-anchor=\"start\" x=\"932.5\" y=\"-1983.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 782</text>\n",
       "<text text-anchor=\"start\" x=\"931\" y=\"-1968.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 11.638</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1057.08,-2064.88C1047.64,-2055.8 1037.3,-2045.85 1027.48,-2036.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1029.74,-2033.71 1020.11,-2029.3 1024.88,-2038.76 1029.74,-2033.71\"/>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>35</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1266.5,-2029C1266.5,-2029 1116.5,-2029 1116.5,-2029 1110.5,-2029 1104.5,-2023 1104.5,-2017 1104.5,-2017 1104.5,-1973 1104.5,-1973 1104.5,-1967 1110.5,-1961 1116.5,-1961 1116.5,-1961 1266.5,-1961 1266.5,-1961 1272.5,-1961 1278.5,-1967 1278.5,-1973 1278.5,-1973 1278.5,-2017 1278.5,-2017 1278.5,-2023 1272.5,-2029 1266.5,-2029\"/>\n",
       "<text text-anchor=\"start\" x=\"1140\" y=\"-2013.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">MinRTT â‰¤ 1.73</text>\n",
       "<text text-anchor=\"start\" x=\"1112.5\" y=\"-1998.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 4.984</text>\n",
       "<text text-anchor=\"start\" x=\"1134\" y=\"-1983.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1359</text>\n",
       "<text text-anchor=\"start\" x=\"1146\" y=\"-1968.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.39</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;35 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>7&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1123.97,-2064.88C1132.79,-2055.89 1142.44,-2046.04 1151.62,-2036.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1154.35,-2038.89 1158.85,-2029.3 1149.35,-2033.99 1154.35,-2038.89\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#fefbf9\" stroke=\"black\" d=\"M866,-1917.5C866,-1917.5 689,-1917.5 689,-1917.5 683,-1917.5 677,-1911.5 677,-1905.5 677,-1905.5 677,-1876.5 677,-1876.5 677,-1870.5 683,-1864.5 689,-1864.5 689,-1864.5 866,-1864.5 866,-1864.5 872,-1864.5 878,-1870.5 878,-1876.5 878,-1876.5 878,-1905.5 878,-1905.5 878,-1911.5 872,-1917.5 866,-1917.5\"/>\n",
       "<text text-anchor=\"start\" x=\"685\" y=\"-1902.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1041.114</text>\n",
       "<text text-anchor=\"start\" x=\"729\" y=\"-1887.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 18</text>\n",
       "<text text-anchor=\"start\" x=\"723\" y=\"-1872.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 30.937</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M917.97,-1960.88C892.4,-1948.35 863.47,-1934.16 838.53,-1921.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"840.06,-1918.78 829.54,-1917.52 836.98,-1925.07 840.06,-1918.78\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M1063,-1925C1063,-1925 908,-1925 908,-1925 902,-1925 896,-1919 896,-1913 896,-1913 896,-1869 896,-1869 896,-1863 902,-1857 908,-1857 908,-1857 1063,-1857 1063,-1857 1069,-1857 1075,-1863 1075,-1869 1075,-1869 1075,-1913 1075,-1913 1075,-1919 1069,-1925 1063,-1925\"/>\n",
       "<text text-anchor=\"start\" x=\"904\" y=\"-1909.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesAcked_2 â‰¤ &#45;0.432</text>\n",
       "<text text-anchor=\"start\" x=\"906.5\" y=\"-1894.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 9.014</text>\n",
       "<text text-anchor=\"start\" x=\"932.5\" y=\"-1879.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 764</text>\n",
       "<text text-anchor=\"start\" x=\"931\" y=\"-1864.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 11.184</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M985.5,-1960.88C985.5,-1952.78 985.5,-1943.98 985.5,-1935.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"989,-1935.3 985.5,-1925.3 982,-1935.3 989,-1935.3\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M772,-1821C772,-1821 631,-1821 631,-1821 625,-1821 619,-1815 619,-1809 619,-1809 619,-1765 619,-1765 619,-1759 625,-1753 631,-1753 631,-1753 772,-1753 772,-1753 778,-1753 784,-1759 784,-1765 784,-1765 784,-1809 784,-1809 784,-1815 778,-1821 772,-1821\"/>\n",
       "<text text-anchor=\"start\" x=\"636\" y=\"-1805.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_4 â‰¤ &#45;0.122</text>\n",
       "<text text-anchor=\"start\" x=\"627\" y=\"-1790.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 8.68</text>\n",
       "<text text-anchor=\"start\" x=\"648.5\" y=\"-1775.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 547</text>\n",
       "<text text-anchor=\"start\" x=\"647\" y=\"-1760.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.223</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M895.64,-1857.73C863.1,-1846.04 826.32,-1832.83 793.59,-1821.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"794.62,-1817.73 784.03,-1817.64 792.25,-1824.31 794.62,-1817.73\"/>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>34</title>\n",
       "<path fill=\"#fffdfd\" stroke=\"black\" d=\"M1060.5,-1813.5C1060.5,-1813.5 910.5,-1813.5 910.5,-1813.5 904.5,-1813.5 898.5,-1807.5 898.5,-1801.5 898.5,-1801.5 898.5,-1772.5 898.5,-1772.5 898.5,-1766.5 904.5,-1760.5 910.5,-1760.5 910.5,-1760.5 1060.5,-1760.5 1060.5,-1760.5 1066.5,-1760.5 1072.5,-1766.5 1072.5,-1772.5 1072.5,-1772.5 1072.5,-1801.5 1072.5,-1801.5 1072.5,-1807.5 1066.5,-1813.5 1060.5,-1813.5\"/>\n",
       "<text text-anchor=\"start\" x=\"906.5\" y=\"-1798.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.669</text>\n",
       "<text text-anchor=\"start\" x=\"932.5\" y=\"-1783.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 217</text>\n",
       "<text text-anchor=\"start\" x=\"931\" y=\"-1768.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 13.605</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;34 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>10&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M985.5,-1856.88C985.5,-1846.33 985.5,-1834.6 985.5,-1823.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"989,-1823.52 985.5,-1813.52 982,-1823.52 989,-1823.52\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M564.5,-1717C564.5,-1717 414.5,-1717 414.5,-1717 408.5,-1717 402.5,-1711 402.5,-1705 402.5,-1705 402.5,-1661 402.5,-1661 402.5,-1655 408.5,-1649 414.5,-1649 414.5,-1649 564.5,-1649 564.5,-1649 570.5,-1649 576.5,-1655 576.5,-1661 576.5,-1661 576.5,-1705 576.5,-1705 576.5,-1711 570.5,-1717 564.5,-1717\"/>\n",
       "<text text-anchor=\"start\" x=\"443\" y=\"-1701.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ &#45;0.508</text>\n",
       "<text text-anchor=\"start\" x=\"410.5\" y=\"-1686.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 6.768</text>\n",
       "<text text-anchor=\"start\" x=\"436.5\" y=\"-1671.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 477</text>\n",
       "<text text-anchor=\"start\" x=\"435\" y=\"-1656.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.754</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M632.67,-1752.88C611.83,-1742.86 588.79,-1731.77 567.4,-1721.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"568.88,-1718.31 558.35,-1717.12 565.84,-1724.61 568.88,-1718.31\"/>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>33</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M772,-1709.5C772,-1709.5 631,-1709.5 631,-1709.5 625,-1709.5 619,-1703.5 619,-1697.5 619,-1697.5 619,-1668.5 619,-1668.5 619,-1662.5 625,-1656.5 631,-1656.5 631,-1656.5 772,-1656.5 772,-1656.5 778,-1656.5 784,-1662.5 784,-1668.5 784,-1668.5 784,-1697.5 784,-1697.5 784,-1703.5 778,-1709.5 772,-1709.5\"/>\n",
       "<text text-anchor=\"start\" x=\"627\" y=\"-1694.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 6.67</text>\n",
       "<text text-anchor=\"start\" x=\"653\" y=\"-1679.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 70</text>\n",
       "<text text-anchor=\"start\" x=\"651.5\" y=\"-1664.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.602</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;33 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>11&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M701.5,-1752.88C701.5,-1742.33 701.5,-1730.6 701.5,-1719.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"705,-1719.52 701.5,-1709.52 698,-1719.52 705,-1719.52\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M372.5,-1605.5C372.5,-1605.5 222.5,-1605.5 222.5,-1605.5 216.5,-1605.5 210.5,-1599.5 210.5,-1593.5 210.5,-1593.5 210.5,-1564.5 210.5,-1564.5 210.5,-1558.5 216.5,-1552.5 222.5,-1552.5 222.5,-1552.5 372.5,-1552.5 372.5,-1552.5 378.5,-1552.5 384.5,-1558.5 384.5,-1564.5 384.5,-1564.5 384.5,-1593.5 384.5,-1593.5 384.5,-1599.5 378.5,-1605.5 372.5,-1605.5\"/>\n",
       "<text text-anchor=\"start\" x=\"218.5\" y=\"-1590.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 6.658</text>\n",
       "<text text-anchor=\"start\" x=\"244.5\" y=\"-1575.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 266</text>\n",
       "<text text-anchor=\"start\" x=\"247.5\" y=\"-1560.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 11.59</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M427.16,-1648.88C403.77,-1636.46 377.32,-1622.41 354.44,-1610.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"356.01,-1607.12 345.54,-1605.52 352.73,-1613.3 356.01,-1607.12\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M564.5,-1613C564.5,-1613 414.5,-1613 414.5,-1613 408.5,-1613 402.5,-1607 402.5,-1601 402.5,-1601 402.5,-1557 402.5,-1557 402.5,-1551 408.5,-1545 414.5,-1545 414.5,-1545 564.5,-1545 564.5,-1545 570.5,-1545 576.5,-1551 576.5,-1557 576.5,-1557 576.5,-1601 576.5,-1601 576.5,-1607 570.5,-1613 564.5,-1613\"/>\n",
       "<text text-anchor=\"start\" x=\"424\" y=\"-1597.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_3 â‰¤ &#45;0.256</text>\n",
       "<text text-anchor=\"start\" x=\"410.5\" y=\"-1582.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 4.916</text>\n",
       "<text text-anchor=\"start\" x=\"436.5\" y=\"-1567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 211</text>\n",
       "<text text-anchor=\"start\" x=\"439.5\" y=\"-1552.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.701</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M489.5,-1648.88C489.5,-1640.78 489.5,-1631.98 489.5,-1623.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"493,-1623.3 489.5,-1613.3 486,-1623.3 493,-1623.3\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M372.5,-1509C372.5,-1509 222.5,-1509 222.5,-1509 216.5,-1509 210.5,-1503 210.5,-1497 210.5,-1497 210.5,-1453 210.5,-1453 210.5,-1447 216.5,-1441 222.5,-1441 222.5,-1441 372.5,-1441 372.5,-1441 378.5,-1441 384.5,-1447 384.5,-1453 384.5,-1453 384.5,-1497 384.5,-1497 384.5,-1503 378.5,-1509 372.5,-1509\"/>\n",
       "<text text-anchor=\"start\" x=\"242.5\" y=\"-1493.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar â‰¤ 0.135</text>\n",
       "<text text-anchor=\"start\" x=\"218.5\" y=\"-1478.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 3.674</text>\n",
       "<text text-anchor=\"start\" x=\"244.5\" y=\"-1463.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 153</text>\n",
       "<text text-anchor=\"start\" x=\"243\" y=\"-1448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.268</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M427.16,-1544.88C408.54,-1534.99 387.98,-1524.07 368.83,-1513.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"370.33,-1510.73 359.85,-1509.12 367.04,-1516.91 370.33,-1510.73\"/>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>32</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M564.5,-1501.5C564.5,-1501.5 414.5,-1501.5 414.5,-1501.5 408.5,-1501.5 402.5,-1495.5 402.5,-1489.5 402.5,-1489.5 402.5,-1460.5 402.5,-1460.5 402.5,-1454.5 408.5,-1448.5 414.5,-1448.5 414.5,-1448.5 564.5,-1448.5 564.5,-1448.5 570.5,-1448.5 576.5,-1454.5 576.5,-1460.5 576.5,-1460.5 576.5,-1489.5 576.5,-1489.5 576.5,-1495.5 570.5,-1501.5 564.5,-1501.5\"/>\n",
       "<text text-anchor=\"start\" x=\"410.5\" y=\"-1486.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 5.102</text>\n",
       "<text text-anchor=\"start\" x=\"441\" y=\"-1471.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 58</text>\n",
       "<text text-anchor=\"start\" x=\"439.5\" y=\"-1456.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.203</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;32 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>14&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M489.5,-1544.88C489.5,-1534.33 489.5,-1522.6 489.5,-1511.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"493,-1511.52 489.5,-1501.52 486,-1511.52 493,-1511.52\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M194.5,-1405C194.5,-1405 30.5,-1405 30.5,-1405 24.5,-1405 18.5,-1399 18.5,-1393 18.5,-1393 18.5,-1349 18.5,-1349 18.5,-1343 24.5,-1337 30.5,-1337 30.5,-1337 194.5,-1337 194.5,-1337 200.5,-1337 206.5,-1343 206.5,-1349 206.5,-1349 206.5,-1393 206.5,-1393 206.5,-1399 200.5,-1405 194.5,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"26.5\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_3 â‰¤ &#45;0.787</text>\n",
       "<text text-anchor=\"start\" x=\"33.5\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.958</text>\n",
       "<text text-anchor=\"start\" x=\"59.5\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 145</text>\n",
       "<text text-anchor=\"start\" x=\"58\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.459</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.44,-1440.88C219.57,-1431.04 199.86,-1420.17 181.48,-1410.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.03,-1406.89 172.58,-1405.12 179.65,-1413.02 183.03,-1406.89\"/>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>31</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M378,-1397.5C378,-1397.5 237,-1397.5 237,-1397.5 231,-1397.5 225,-1391.5 225,-1385.5 225,-1385.5 225,-1356.5 225,-1356.5 225,-1350.5 231,-1344.5 237,-1344.5 237,-1344.5 378,-1344.5 378,-1344.5 384,-1344.5 390,-1350.5 390,-1356.5 390,-1356.5 390,-1385.5 390,-1385.5 390,-1391.5 384,-1397.5 378,-1397.5\"/>\n",
       "<text text-anchor=\"start\" x=\"233\" y=\"-1382.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 4.06</text>\n",
       "<text text-anchor=\"start\" x=\"263.5\" y=\"-1367.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n",
       "<text text-anchor=\"start\" x=\"257.5\" y=\"-1352.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.815</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;31 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>15&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.75,-1440.88C301.79,-1430.22 302.96,-1418.35 304.02,-1407.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"307.51,-1407.81 305,-1397.52 300.54,-1407.13 307.51,-1407.81\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M153,-1293.5C153,-1293.5 12,-1293.5 12,-1293.5 6,-1293.5 0,-1287.5 0,-1281.5 0,-1281.5 0,-1252.5 0,-1252.5 0,-1246.5 6,-1240.5 12,-1240.5 12,-1240.5 153,-1240.5 153,-1240.5 159,-1240.5 165,-1246.5 165,-1252.5 165,-1252.5 165,-1281.5 165,-1281.5 165,-1287.5 159,-1293.5 153,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 4.55</text>\n",
       "<text text-anchor=\"start\" x=\"34\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\n",
       "<text text-anchor=\"start\" x=\"28\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 12.969</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.76,-1336.88C99.59,-1326.11 96.06,-1314.11 92.85,-1303.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.19,-1302.13 90.01,-1293.52 89.47,-1304.1 96.19,-1302.13\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M336,-1301C336,-1301 195,-1301 195,-1301 189,-1301 183,-1295 183,-1289 183,-1289 183,-1245 183,-1245 183,-1239 189,-1233 195,-1233 195,-1233 336,-1233 336,-1233 342,-1233 348,-1239 348,-1245 348,-1245 348,-1289 348,-1289 348,-1295 342,-1301 336,-1301\"/>\n",
       "<text text-anchor=\"start\" x=\"193\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent_2 â‰¤ &#45;0.44</text>\n",
       "<text text-anchor=\"start\" x=\"191\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.12</text>\n",
       "<text text-anchor=\"start\" x=\"212.5\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 132</text>\n",
       "<text text-anchor=\"start\" x=\"211\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.212</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>16&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.17,-1336.88C176.54,-1327.3 192.36,-1316.76 207.2,-1306.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"209.43,-1309.58 215.81,-1301.12 205.55,-1303.76 209.43,-1309.58\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M190.5,-1189.5C190.5,-1189.5 40.5,-1189.5 40.5,-1189.5 34.5,-1189.5 28.5,-1183.5 28.5,-1177.5 28.5,-1177.5 28.5,-1148.5 28.5,-1148.5 28.5,-1142.5 34.5,-1136.5 40.5,-1136.5 40.5,-1136.5 190.5,-1136.5 190.5,-1136.5 196.5,-1136.5 202.5,-1142.5 202.5,-1148.5 202.5,-1148.5 202.5,-1177.5 202.5,-1177.5 202.5,-1183.5 196.5,-1189.5 190.5,-1189.5\"/>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-1174.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.446</text>\n",
       "<text text-anchor=\"start\" x=\"67\" y=\"-1159.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 18</text>\n",
       "<text text-anchor=\"start\" x=\"65.5\" y=\"-1144.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.483</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M216.8,-1232.88C199.01,-1220.79 178.96,-1207.15 161.42,-1195.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.27,-1192.25 153.03,-1189.52 159.33,-1198.04 163.27,-1192.25\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M374,-1197C374,-1197 233,-1197 233,-1197 227,-1197 221,-1191 221,-1185 221,-1185 221,-1141 221,-1141 221,-1135 227,-1129 233,-1129 233,-1129 374,-1129 374,-1129 380,-1129 386,-1135 386,-1141 386,-1141 386,-1185 386,-1185 386,-1191 380,-1197 374,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"238.5\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime â‰¤ 0.285</text>\n",
       "<text text-anchor=\"start\" x=\"229\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.68</text>\n",
       "<text text-anchor=\"start\" x=\"250.5\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 114</text>\n",
       "<text text-anchor=\"start\" x=\"249\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.485</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>18&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.84,-1232.88C280.92,-1224.6 284.28,-1215.6 287.51,-1206.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.88,-1207.89 291.09,-1197.3 284.32,-1205.45 290.88,-1207.89\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M205.5,-1085.5C205.5,-1085.5 55.5,-1085.5 55.5,-1085.5 49.5,-1085.5 43.5,-1079.5 43.5,-1073.5 43.5,-1073.5 43.5,-1044.5 43.5,-1044.5 43.5,-1038.5 49.5,-1032.5 55.5,-1032.5 55.5,-1032.5 205.5,-1032.5 205.5,-1032.5 211.5,-1032.5 217.5,-1038.5 217.5,-1044.5 217.5,-1044.5 217.5,-1073.5 217.5,-1073.5 217.5,-1079.5 211.5,-1085.5 205.5,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"51.5\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.593</text>\n",
       "<text text-anchor=\"start\" x=\"82\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 45</text>\n",
       "<text text-anchor=\"start\" x=\"76\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 11.219</text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>20&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M247.33,-1128.88C226.53,-1116.62 203.06,-1102.78 182.63,-1090.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.18,-1087.58 173.78,-1085.52 180.62,-1093.61 184.18,-1087.58\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M397.5,-1093C397.5,-1093 247.5,-1093 247.5,-1093 241.5,-1093 235.5,-1087 235.5,-1081 235.5,-1081 235.5,-1037 235.5,-1037 235.5,-1031 241.5,-1025 247.5,-1025 247.5,-1025 397.5,-1025 397.5,-1025 403.5,-1025 409.5,-1031 409.5,-1037 409.5,-1037 409.5,-1081 409.5,-1081 409.5,-1087 403.5,-1093 397.5,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"245.5\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesAcked_2 â‰¤ &#45;0.44</text>\n",
       "<text text-anchor=\"start\" x=\"243.5\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.155</text>\n",
       "<text text-anchor=\"start\" x=\"274\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 69</text>\n",
       "<text text-anchor=\"start\" x=\"268\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.005</text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>20&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M309.67,-1128.88C311.2,-1120.69 312.85,-1111.79 314.45,-1103.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"317.91,-1103.77 316.3,-1093.3 311.03,-1102.49 317.91,-1103.77\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M393.5,-989C393.5,-989 247.5,-989 247.5,-989 241.5,-989 235.5,-983 235.5,-977 235.5,-977 235.5,-933 235.5,-933 235.5,-927 241.5,-921 247.5,-921 247.5,-921 393.5,-921 393.5,-921 399.5,-921 405.5,-927 405.5,-933 405.5,-933 405.5,-977 405.5,-977 405.5,-983 399.5,-989 393.5,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"243.5\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent_3 â‰¤ &#45;0.433</text>\n",
       "<text text-anchor=\"start\" x=\"246\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.91</text>\n",
       "<text text-anchor=\"start\" x=\"272\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 21</text>\n",
       "<text text-anchor=\"start\" x=\"270.5\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.189</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;23 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>22&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.85,-1024.88C321.69,-1016.78 321.52,-1007.98 321.35,-999.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"324.85,-999.23 321.15,-989.3 317.85,-999.37 324.85,-999.23\"/>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M585.5,-981.5C585.5,-981.5 435.5,-981.5 435.5,-981.5 429.5,-981.5 423.5,-975.5 423.5,-969.5 423.5,-969.5 423.5,-940.5 423.5,-940.5 423.5,-934.5 429.5,-928.5 435.5,-928.5 435.5,-928.5 585.5,-928.5 585.5,-928.5 591.5,-928.5 597.5,-934.5 597.5,-940.5 597.5,-940.5 597.5,-969.5 597.5,-969.5 597.5,-975.5 591.5,-981.5 585.5,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.843</text>\n",
       "<text text-anchor=\"start\" x=\"462\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n",
       "<text text-anchor=\"start\" x=\"456\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.363</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>22&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M383.54,-1024.88C406.34,-1012.51 432.11,-998.53 454.44,-986.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.34,-989.37 463.46,-981.52 453,-983.21 456.34,-989.37\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M304.5,-885C304.5,-885 154.5,-885 154.5,-885 148.5,-885 142.5,-879 142.5,-873 142.5,-873 142.5,-829 142.5,-829 142.5,-823 148.5,-817 154.5,-817 154.5,-817 304.5,-817 304.5,-817 310.5,-817 316.5,-823 316.5,-829 316.5,-829 316.5,-873 316.5,-873 316.5,-879 310.5,-885 304.5,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"183\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ &#45;0.479</text>\n",
       "<text text-anchor=\"start\" x=\"150.5\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.616</text>\n",
       "<text text-anchor=\"start\" x=\"181\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 20</text>\n",
       "<text text-anchor=\"start\" x=\"179.5\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.316</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>23&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.95,-920.88C283.01,-911.98 274.32,-902.24 266.04,-892.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.48,-890.43 259.21,-885.3 263.25,-895.09 268.48,-890.43\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M478.5,-877.5C478.5,-877.5 346.5,-877.5 346.5,-877.5 340.5,-877.5 334.5,-871.5 334.5,-865.5 334.5,-865.5 334.5,-836.5 334.5,-836.5 334.5,-830.5 340.5,-824.5 346.5,-824.5 346.5,-824.5 478.5,-824.5 478.5,-824.5 484.5,-824.5 490.5,-830.5 490.5,-836.5 490.5,-836.5 490.5,-865.5 490.5,-865.5 490.5,-871.5 484.5,-877.5 478.5,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"342.5\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"368.5\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"362.5\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.646</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>23&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M350.37,-920.88C360.69,-909.45 372.24,-896.63 382.56,-885.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.38,-887.29 389.48,-877.52 380.19,-882.6 385.38,-887.29\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M258.5,-773.5C258.5,-773.5 108.5,-773.5 108.5,-773.5 102.5,-773.5 96.5,-767.5 96.5,-761.5 96.5,-761.5 96.5,-732.5 96.5,-732.5 96.5,-726.5 102.5,-720.5 108.5,-720.5 108.5,-720.5 258.5,-720.5 258.5,-720.5 264.5,-720.5 270.5,-726.5 270.5,-732.5 270.5,-732.5 270.5,-761.5 270.5,-761.5 270.5,-767.5 264.5,-773.5 258.5,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.294</text>\n",
       "<text text-anchor=\"start\" x=\"139.5\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"133.5\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.29</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.57,-816.88C209.65,-806 204.18,-793.86 199.22,-782.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"202.31,-781.2 195.01,-773.52 195.93,-784.07 202.31,-781.2\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M450.5,-781C450.5,-781 300.5,-781 300.5,-781 294.5,-781 288.5,-775 288.5,-769 288.5,-769 288.5,-725 288.5,-725 288.5,-719 294.5,-713 300.5,-713 300.5,-713 450.5,-713 450.5,-713 456.5,-713 462.5,-719 462.5,-725 462.5,-725 462.5,-769 462.5,-769 462.5,-775 456.5,-781 450.5,-781\"/>\n",
       "<text text-anchor=\"start\" x=\"321\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_3 â‰¤ &#45;0.332</text>\n",
       "<text text-anchor=\"start\" x=\"296.5\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.173</text>\n",
       "<text text-anchor=\"start\" x=\"327\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 14</text>\n",
       "<text text-anchor=\"start\" x=\"325.5\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.898</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>24&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276.9,-816.88C290.42,-807.44 305.27,-797.06 319.26,-787.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"321.64,-789.9 327.84,-781.3 317.63,-784.16 321.64,-789.9\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M449.5,-669.5C449.5,-669.5 299.5,-669.5 299.5,-669.5 293.5,-669.5 287.5,-663.5 287.5,-657.5 287.5,-657.5 287.5,-628.5 287.5,-628.5 287.5,-622.5 293.5,-616.5 299.5,-616.5 299.5,-616.5 449.5,-616.5 449.5,-616.5 455.5,-616.5 461.5,-622.5 461.5,-628.5 461.5,-628.5 461.5,-657.5 461.5,-657.5 461.5,-663.5 455.5,-669.5 449.5,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"295.5\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.048</text>\n",
       "<text text-anchor=\"start\" x=\"330.5\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"324.5\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.171</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M375.18,-712.88C375.07,-702.33 374.96,-690.6 374.85,-679.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.35,-679.49 374.75,-669.52 371.35,-679.55 378.35,-679.49\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M633,-669.5C633,-669.5 492,-669.5 492,-669.5 486,-669.5 480,-663.5 480,-657.5 480,-657.5 480,-628.5 480,-628.5 480,-622.5 486,-616.5 492,-616.5 492,-616.5 633,-616.5 633,-616.5 639,-616.5 645,-622.5 645,-628.5 645,-628.5 645,-657.5 645,-657.5 645,-663.5 639,-669.5 633,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"488\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.02</text>\n",
       "<text text-anchor=\"start\" x=\"518.5\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"512.5\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.405</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>26&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.21,-712.88C458.9,-700.51 484.53,-686.53 506.74,-674.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"508.61,-677.38 515.71,-669.52 505.26,-671.24 508.61,-677.38\"/>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>36</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1266.5,-1925C1266.5,-1925 1116.5,-1925 1116.5,-1925 1110.5,-1925 1104.5,-1919 1104.5,-1913 1104.5,-1913 1104.5,-1869 1104.5,-1869 1104.5,-1863 1110.5,-1857 1116.5,-1857 1116.5,-1857 1266.5,-1857 1266.5,-1857 1272.5,-1857 1278.5,-1863 1278.5,-1869 1278.5,-1869 1278.5,-1913 1278.5,-1913 1278.5,-1919 1272.5,-1925 1266.5,-1925\"/>\n",
       "<text text-anchor=\"start\" x=\"1140\" y=\"-1909.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_1 â‰¤ 0.219</text>\n",
       "<text text-anchor=\"start\" x=\"1112.5\" y=\"-1894.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.586</text>\n",
       "<text text-anchor=\"start\" x=\"1134\" y=\"-1879.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1038</text>\n",
       "<text text-anchor=\"start\" x=\"1141.5\" y=\"-1864.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.349</text>\n",
       "</g>\n",
       "<!-- 35&#45;&gt;36 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>35&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1191.5,-1960.88C1191.5,-1952.78 1191.5,-1943.98 1191.5,-1935.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1195,-1935.3 1191.5,-1925.3 1188,-1935.3 1195,-1935.3\"/>\n",
       "</g>\n",
       "<!-- 111 -->\n",
       "<g id=\"node112\" class=\"node\">\n",
       "<title>111</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1450,-1917.5C1450,-1917.5 1309,-1917.5 1309,-1917.5 1303,-1917.5 1297,-1911.5 1297,-1905.5 1297,-1905.5 1297,-1876.5 1297,-1876.5 1297,-1870.5 1303,-1864.5 1309,-1864.5 1309,-1864.5 1450,-1864.5 1450,-1864.5 1456,-1864.5 1462,-1870.5 1462,-1876.5 1462,-1876.5 1462,-1905.5 1462,-1905.5 1462,-1911.5 1456,-1917.5 1450,-1917.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1305\" y=\"-1902.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.16</text>\n",
       "<text text-anchor=\"start\" x=\"1326.5\" y=\"-1887.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 321</text>\n",
       "<text text-anchor=\"start\" x=\"1329.5\" y=\"-1872.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.291</text>\n",
       "</g>\n",
       "<!-- 35&#45;&gt;111 -->\n",
       "<g id=\"edge111\" class=\"edge\">\n",
       "<title>35&#45;&gt;111</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1252.54,-1960.88C1275.34,-1948.51 1301.11,-1934.53 1323.44,-1922.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1325.34,-1925.37 1332.46,-1917.52 1322,-1919.21 1325.34,-1925.37\"/>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>37</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1266.5,-1821C1266.5,-1821 1116.5,-1821 1116.5,-1821 1110.5,-1821 1104.5,-1815 1104.5,-1809 1104.5,-1809 1104.5,-1765 1104.5,-1765 1104.5,-1759 1110.5,-1753 1116.5,-1753 1116.5,-1753 1266.5,-1753 1266.5,-1753 1272.5,-1753 1278.5,-1759 1278.5,-1765 1278.5,-1765 1278.5,-1809 1278.5,-1809 1278.5,-1815 1272.5,-1821 1266.5,-1821\"/>\n",
       "<text text-anchor=\"start\" x=\"1128.5\" y=\"-1805.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_4 â‰¤ 0.167</text>\n",
       "<text text-anchor=\"start\" x=\"1112.5\" y=\"-1790.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.979</text>\n",
       "<text text-anchor=\"start\" x=\"1138.5\" y=\"-1775.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 344</text>\n",
       "<text text-anchor=\"start\" x=\"1146\" y=\"-1760.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.66</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;37 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>36&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1191.5,-1856.88C1191.5,-1848.78 1191.5,-1839.98 1191.5,-1831.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1195,-1831.3 1191.5,-1821.3 1188,-1831.3 1195,-1831.3\"/>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>52</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1524.5,-1821C1524.5,-1821 1374.5,-1821 1374.5,-1821 1368.5,-1821 1362.5,-1815 1362.5,-1809 1362.5,-1809 1362.5,-1765 1362.5,-1765 1362.5,-1759 1368.5,-1753 1374.5,-1753 1374.5,-1753 1524.5,-1753 1524.5,-1753 1530.5,-1753 1536.5,-1759 1536.5,-1765 1536.5,-1765 1536.5,-1809 1536.5,-1809 1536.5,-1815 1530.5,-1821 1524.5,-1821\"/>\n",
       "<text text-anchor=\"start\" x=\"1377\" y=\"-1805.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent_2 â‰¤ &#45;0.44</text>\n",
       "<text text-anchor=\"start\" x=\"1370.5\" y=\"-1790.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.116</text>\n",
       "<text text-anchor=\"start\" x=\"1396.5\" y=\"-1775.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 694</text>\n",
       "<text text-anchor=\"start\" x=\"1399.5\" y=\"-1760.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.699</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;52 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>36&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1275.27,-1856.88C1301.08,-1846.68 1329.66,-1835.38 1356.09,-1824.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1357.7,-1828.06 1365.71,-1821.12 1355.13,-1821.55 1357.7,-1828.06\"/>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>38</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1086.5,-1717C1086.5,-1717 936.5,-1717 936.5,-1717 930.5,-1717 924.5,-1711 924.5,-1705 924.5,-1705 924.5,-1661 924.5,-1661 924.5,-1655 930.5,-1649 936.5,-1649 936.5,-1649 1086.5,-1649 1086.5,-1649 1092.5,-1649 1098.5,-1655 1098.5,-1661 1098.5,-1661 1098.5,-1705 1098.5,-1705 1098.5,-1711 1092.5,-1717 1086.5,-1717\"/>\n",
       "<text text-anchor=\"start\" x=\"946\" y=\"-1701.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_3 â‰¤ &#45;0.241</text>\n",
       "<text text-anchor=\"start\" x=\"932.5\" y=\"-1686.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.028</text>\n",
       "<text text-anchor=\"start\" x=\"958.5\" y=\"-1671.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 260</text>\n",
       "<text text-anchor=\"start\" x=\"961.5\" y=\"-1656.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.221</text>\n",
       "</g>\n",
       "<!-- 37&#45;&gt;38 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>37&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1133.06,-1752.88C1115.84,-1743.12 1096.85,-1732.37 1079.11,-1722.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1080.38,-1719.01 1069.96,-1717.12 1076.93,-1725.1 1080.38,-1719.01\"/>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>51</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1278.5,-1709.5C1278.5,-1709.5 1128.5,-1709.5 1128.5,-1709.5 1122.5,-1709.5 1116.5,-1703.5 1116.5,-1697.5 1116.5,-1697.5 1116.5,-1668.5 1116.5,-1668.5 1116.5,-1662.5 1122.5,-1656.5 1128.5,-1656.5 1128.5,-1656.5 1278.5,-1656.5 1278.5,-1656.5 1284.5,-1656.5 1290.5,-1662.5 1290.5,-1668.5 1290.5,-1668.5 1290.5,-1697.5 1290.5,-1697.5 1290.5,-1703.5 1284.5,-1709.5 1278.5,-1709.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1124.5\" y=\"-1694.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.929</text>\n",
       "<text text-anchor=\"start\" x=\"1155\" y=\"-1679.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 84</text>\n",
       "<text text-anchor=\"start\" x=\"1153.5\" y=\"-1664.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.922</text>\n",
       "</g>\n",
       "<!-- 37&#45;&gt;51 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>37&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1195.4,-1752.88C1196.65,-1742.22 1198.05,-1730.35 1199.32,-1719.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1202.81,-1719.86 1200.5,-1709.52 1195.85,-1719.04 1202.81,-1719.86\"/>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>39</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M900.5,-1613C900.5,-1613 750.5,-1613 750.5,-1613 744.5,-1613 738.5,-1607 738.5,-1601 738.5,-1601 738.5,-1557 738.5,-1557 738.5,-1551 744.5,-1545 750.5,-1545 750.5,-1545 900.5,-1545 900.5,-1545 906.5,-1545 912.5,-1551 912.5,-1557 912.5,-1557 912.5,-1601 912.5,-1601 912.5,-1607 906.5,-1613 900.5,-1613\"/>\n",
       "<text text-anchor=\"start\" x=\"750\" y=\"-1597.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime_2 â‰¤ &#45;0.549</text>\n",
       "<text text-anchor=\"start\" x=\"746.5\" y=\"-1582.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.722</text>\n",
       "<text text-anchor=\"start\" x=\"772.5\" y=\"-1567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 138</text>\n",
       "<text text-anchor=\"start\" x=\"775.5\" y=\"-1552.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.885</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;39 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>38&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M951.11,-1648.88C933.15,-1639.04 913.34,-1628.17 894.86,-1618.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"896.35,-1614.86 885.9,-1613.12 892.99,-1621 896.35,-1614.86\"/>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>50</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1092.5,-1605.5C1092.5,-1605.5 942.5,-1605.5 942.5,-1605.5 936.5,-1605.5 930.5,-1599.5 930.5,-1593.5 930.5,-1593.5 930.5,-1564.5 930.5,-1564.5 930.5,-1558.5 936.5,-1552.5 942.5,-1552.5 942.5,-1552.5 1092.5,-1552.5 1092.5,-1552.5 1098.5,-1552.5 1104.5,-1558.5 1104.5,-1564.5 1104.5,-1564.5 1104.5,-1593.5 1104.5,-1593.5 1104.5,-1599.5 1098.5,-1605.5 1092.5,-1605.5\"/>\n",
       "<text text-anchor=\"start\" x=\"938.5\" y=\"-1590.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.311</text>\n",
       "<text text-anchor=\"start\" x=\"964.5\" y=\"-1575.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 122</text>\n",
       "<text text-anchor=\"start\" x=\"972\" y=\"-1560.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.47</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;50 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>38&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1013.45,-1648.88C1014.08,-1638.22 1014.77,-1626.35 1015.41,-1615.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1018.91,-1615.71 1016,-1605.52 1011.92,-1615.3 1018.91,-1615.71\"/>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>40</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M756.5,-1501.5C756.5,-1501.5 606.5,-1501.5 606.5,-1501.5 600.5,-1501.5 594.5,-1495.5 594.5,-1489.5 594.5,-1489.5 594.5,-1460.5 594.5,-1460.5 594.5,-1454.5 600.5,-1448.5 606.5,-1448.5 606.5,-1448.5 756.5,-1448.5 756.5,-1448.5 762.5,-1448.5 768.5,-1454.5 768.5,-1460.5 768.5,-1460.5 768.5,-1489.5 768.5,-1489.5 768.5,-1495.5 762.5,-1501.5 756.5,-1501.5\"/>\n",
       "<text text-anchor=\"start\" x=\"602.5\" y=\"-1486.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 2.534</text>\n",
       "<text text-anchor=\"start\" x=\"633\" y=\"-1471.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"start\" x=\"631.5\" y=\"-1456.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 9.727</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;40 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>39&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M778.75,-1544.88C761.82,-1532.9 742.77,-1519.4 726.04,-1507.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"727.71,-1504.44 717.53,-1501.52 723.67,-1510.16 727.71,-1504.44\"/>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>41</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M948.5,-1509C948.5,-1509 798.5,-1509 798.5,-1509 792.5,-1509 786.5,-1503 786.5,-1497 786.5,-1497 786.5,-1453 786.5,-1453 786.5,-1447 792.5,-1441 798.5,-1441 798.5,-1441 948.5,-1441 948.5,-1441 954.5,-1441 960.5,-1447 960.5,-1453 960.5,-1453 960.5,-1497 960.5,-1497 960.5,-1503 954.5,-1509 948.5,-1509\"/>\n",
       "<text text-anchor=\"start\" x=\"808.5\" y=\"-1493.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime â‰¤ 1.511</text>\n",
       "<text text-anchor=\"start\" x=\"794.5\" y=\"-1478.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.784</text>\n",
       "<text text-anchor=\"start\" x=\"825\" y=\"-1463.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 92</text>\n",
       "<text text-anchor=\"start\" x=\"823.5\" y=\"-1448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.464</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;41 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>39&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M841.08,-1544.88C845.07,-1536.42 849.4,-1527.21 853.57,-1518.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"856.74,-1519.84 857.83,-1509.3 850.4,-1516.86 856.74,-1519.84\"/>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>42</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M756.5,-1405C756.5,-1405 606.5,-1405 606.5,-1405 600.5,-1405 594.5,-1399 594.5,-1393 594.5,-1393 594.5,-1349 594.5,-1349 594.5,-1343 600.5,-1337 606.5,-1337 606.5,-1337 756.5,-1337 756.5,-1337 762.5,-1337 768.5,-1343 768.5,-1349 768.5,-1349 768.5,-1393 768.5,-1393 768.5,-1399 762.5,-1405 756.5,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"635\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ &#45;0.042</text>\n",
       "<text text-anchor=\"start\" x=\"602.5\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.542</text>\n",
       "<text text-anchor=\"start\" x=\"633\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 87</text>\n",
       "<text text-anchor=\"start\" x=\"631.5\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.343</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;42 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>41&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M811.16,-1440.88C792.54,-1430.99 771.98,-1420.07 752.83,-1409.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"754.33,-1406.73 743.85,-1405.12 751.04,-1412.91 754.33,-1406.73\"/>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>49</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M948.5,-1397.5C948.5,-1397.5 798.5,-1397.5 798.5,-1397.5 792.5,-1397.5 786.5,-1391.5 786.5,-1385.5 786.5,-1385.5 786.5,-1356.5 786.5,-1356.5 786.5,-1350.5 792.5,-1344.5 798.5,-1344.5 798.5,-1344.5 948.5,-1344.5 948.5,-1344.5 954.5,-1344.5 960.5,-1350.5 960.5,-1356.5 960.5,-1356.5 960.5,-1385.5 960.5,-1385.5 960.5,-1391.5 954.5,-1397.5 948.5,-1397.5\"/>\n",
       "<text text-anchor=\"start\" x=\"794.5\" y=\"-1382.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.285</text>\n",
       "<text text-anchor=\"start\" x=\"829.5\" y=\"-1367.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"819\" y=\"-1352.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 10.576</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;49 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>41&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M873.5,-1440.88C873.5,-1430.33 873.5,-1418.6 873.5,-1407.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"877,-1407.52 873.5,-1397.52 870,-1407.52 877,-1407.52\"/>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>43</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M646.5,-1301C646.5,-1301 496.5,-1301 496.5,-1301 490.5,-1301 484.5,-1295 484.5,-1289 484.5,-1289 484.5,-1245 484.5,-1245 484.5,-1239 490.5,-1233 496.5,-1233 496.5,-1233 646.5,-1233 646.5,-1233 652.5,-1233 658.5,-1239 658.5,-1245 658.5,-1245 658.5,-1289 658.5,-1289 658.5,-1295 652.5,-1301 646.5,-1301\"/>\n",
       "<text text-anchor=\"start\" x=\"499\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent_1 â‰¤ &#45;0.44</text>\n",
       "<text text-anchor=\"start\" x=\"492.5\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.391</text>\n",
       "<text text-anchor=\"start\" x=\"523\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"521.5\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.667</text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;43 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>42&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M645.79,-1336.88C635.89,-1327.71 625.04,-1317.65 614.77,-1308.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"617.12,-1305.53 607.41,-1301.3 612.36,-1310.67 617.12,-1305.53\"/>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>48</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M838.5,-1293.5C838.5,-1293.5 688.5,-1293.5 688.5,-1293.5 682.5,-1293.5 676.5,-1287.5 676.5,-1281.5 676.5,-1281.5 676.5,-1252.5 676.5,-1252.5 676.5,-1246.5 682.5,-1240.5 688.5,-1240.5 688.5,-1240.5 838.5,-1240.5 838.5,-1240.5 844.5,-1240.5 850.5,-1246.5 850.5,-1252.5 850.5,-1252.5 850.5,-1281.5 850.5,-1281.5 850.5,-1287.5 844.5,-1293.5 838.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"684.5\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.412</text>\n",
       "<text text-anchor=\"start\" x=\"715\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\n",
       "<text text-anchor=\"start\" x=\"713.5\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.905</text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;48 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>42&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M708.12,-1336.88C717.23,-1325.56 727.42,-1312.88 736.55,-1301.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"739.45,-1303.51 742.98,-1293.52 733.99,-1299.12 739.45,-1303.51\"/>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>44</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M637.5,-1197C637.5,-1197 487.5,-1197 487.5,-1197 481.5,-1197 475.5,-1191 475.5,-1185 475.5,-1185 475.5,-1141 475.5,-1141 475.5,-1135 481.5,-1129 487.5,-1129 487.5,-1129 637.5,-1129 637.5,-1129 643.5,-1129 649.5,-1135 649.5,-1141 649.5,-1141 649.5,-1185 649.5,-1185 649.5,-1191 643.5,-1197 637.5,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"508\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_4 â‰¤ &#45;0.016</text>\n",
       "<text text-anchor=\"start\" x=\"483.5\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.113</text>\n",
       "<text text-anchor=\"start\" x=\"514\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 16</text>\n",
       "<text text-anchor=\"start\" x=\"517\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.11</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;44 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>43&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M568.58,-1232.88C567.86,-1224.78 567.09,-1215.98 566.34,-1207.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"569.8,-1206.95 565.44,-1197.3 562.83,-1207.57 569.8,-1206.95\"/>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>47</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M829.5,-1189.5C829.5,-1189.5 679.5,-1189.5 679.5,-1189.5 673.5,-1189.5 667.5,-1183.5 667.5,-1177.5 667.5,-1177.5 667.5,-1148.5 667.5,-1148.5 667.5,-1142.5 673.5,-1136.5 679.5,-1136.5 679.5,-1136.5 829.5,-1136.5 829.5,-1136.5 835.5,-1136.5 841.5,-1142.5 841.5,-1148.5 841.5,-1148.5 841.5,-1177.5 841.5,-1177.5 841.5,-1183.5 835.5,-1189.5 829.5,-1189.5\"/>\n",
       "<text text-anchor=\"start\" x=\"675.5\" y=\"-1174.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.306</text>\n",
       "<text text-anchor=\"start\" x=\"706\" y=\"-1159.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 34</text>\n",
       "<text text-anchor=\"start\" x=\"709\" y=\"-1144.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.93</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;47 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>43&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M630.91,-1232.88C653.11,-1220.51 678.2,-1206.53 699.94,-1194.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"701.68,-1197.45 708.71,-1189.52 698.28,-1191.33 701.68,-1197.45\"/>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>45</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M636.5,-1085.5C636.5,-1085.5 486.5,-1085.5 486.5,-1085.5 480.5,-1085.5 474.5,-1079.5 474.5,-1073.5 474.5,-1073.5 474.5,-1044.5 474.5,-1044.5 474.5,-1038.5 480.5,-1032.5 486.5,-1032.5 486.5,-1032.5 636.5,-1032.5 636.5,-1032.5 642.5,-1032.5 648.5,-1038.5 648.5,-1044.5 648.5,-1044.5 648.5,-1073.5 648.5,-1073.5 648.5,-1079.5 642.5,-1085.5 636.5,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"482.5\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.067</text>\n",
       "<text text-anchor=\"start\" x=\"517.5\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"511.5\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.871</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;45 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>44&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M562.18,-1128.88C562.07,-1118.33 561.96,-1106.6 561.85,-1095.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"565.35,-1095.49 561.75,-1085.52 558.35,-1095.55 565.35,-1095.49\"/>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>46</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M828.5,-1085.5C828.5,-1085.5 678.5,-1085.5 678.5,-1085.5 672.5,-1085.5 666.5,-1079.5 666.5,-1073.5 666.5,-1073.5 666.5,-1044.5 666.5,-1044.5 666.5,-1038.5 672.5,-1032.5 678.5,-1032.5 678.5,-1032.5 828.5,-1032.5 828.5,-1032.5 834.5,-1032.5 840.5,-1038.5 840.5,-1044.5 840.5,-1044.5 840.5,-1073.5 840.5,-1073.5 840.5,-1079.5 834.5,-1085.5 828.5,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"674.5\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.003</text>\n",
       "<text text-anchor=\"start\" x=\"709.5\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 7</text>\n",
       "<text text-anchor=\"start\" x=\"703.5\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.417</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;46 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>44&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M624.51,-1128.88C647.78,-1116.46 674.09,-1102.41 696.85,-1090.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"698.54,-1093.32 705.71,-1085.52 695.24,-1087.14 698.54,-1093.32\"/>\n",
       "</g>\n",
       "<!-- 53 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>53</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1524.5,-1717C1524.5,-1717 1374.5,-1717 1374.5,-1717 1368.5,-1717 1362.5,-1711 1362.5,-1705 1362.5,-1705 1362.5,-1661 1362.5,-1661 1362.5,-1655 1368.5,-1649 1374.5,-1649 1374.5,-1649 1524.5,-1649 1524.5,-1649 1530.5,-1649 1536.5,-1655 1536.5,-1661 1536.5,-1661 1536.5,-1705 1536.5,-1705 1536.5,-1711 1530.5,-1717 1524.5,-1717\"/>\n",
       "<text text-anchor=\"start\" x=\"1394.5\" y=\"-1701.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar â‰¤ 0.716</text>\n",
       "<text text-anchor=\"start\" x=\"1370.5\" y=\"-1686.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.688</text>\n",
       "<text text-anchor=\"start\" x=\"1396.5\" y=\"-1671.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 520</text>\n",
       "<text text-anchor=\"start\" x=\"1399.5\" y=\"-1656.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.354</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;53 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>52&#45;&gt;53</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1449.5,-1752.88C1449.5,-1744.78 1449.5,-1735.98 1449.5,-1727.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1453,-1727.3 1449.5,-1717.3 1446,-1727.3 1453,-1727.3\"/>\n",
       "</g>\n",
       "<!-- 102 -->\n",
       "<g id=\"node103\" class=\"node\">\n",
       "<title>102</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1909.5,-1717C1909.5,-1717 1757.5,-1717 1757.5,-1717 1751.5,-1717 1745.5,-1711 1745.5,-1705 1745.5,-1705 1745.5,-1661 1745.5,-1661 1745.5,-1655 1751.5,-1649 1757.5,-1649 1757.5,-1649 1909.5,-1649 1909.5,-1649 1915.5,-1649 1921.5,-1655 1921.5,-1661 1921.5,-1661 1921.5,-1705 1921.5,-1705 1921.5,-1711 1915.5,-1717 1909.5,-1717\"/>\n",
       "<text text-anchor=\"start\" x=\"1753.5\" y=\"-1701.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesRetrans â‰¤ &#45;0.216</text>\n",
       "<text text-anchor=\"start\" x=\"1754.5\" y=\"-1686.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.978</text>\n",
       "<text text-anchor=\"start\" x=\"1780.5\" y=\"-1671.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 174</text>\n",
       "<text text-anchor=\"start\" x=\"1788\" y=\"-1656.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.73</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;102 -->\n",
       "<g id=\"edge102\" class=\"edge\">\n",
       "<title>52&#45;&gt;102</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1536.64,-1762.85C1595.87,-1747.12 1674.31,-1726.29 1735.68,-1709.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1736.61,-1713.36 1745.38,-1707.41 1734.82,-1706.59 1736.61,-1713.36\"/>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>54</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1332.5,-1613C1332.5,-1613 1182.5,-1613 1182.5,-1613 1176.5,-1613 1170.5,-1607 1170.5,-1601 1170.5,-1601 1170.5,-1557 1170.5,-1557 1170.5,-1551 1176.5,-1545 1182.5,-1545 1182.5,-1545 1332.5,-1545 1332.5,-1545 1338.5,-1545 1344.5,-1551 1344.5,-1557 1344.5,-1557 1344.5,-1601 1344.5,-1601 1344.5,-1607 1338.5,-1613 1332.5,-1613\"/>\n",
       "<text text-anchor=\"start\" x=\"1184.5\" y=\"-1597.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime_4 â‰¤ 1.233</text>\n",
       "<text text-anchor=\"start\" x=\"1178.5\" y=\"-1582.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.671</text>\n",
       "<text text-anchor=\"start\" x=\"1204.5\" y=\"-1567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 375</text>\n",
       "<text text-anchor=\"start\" x=\"1207.5\" y=\"-1552.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.176</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;54 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>53&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1387.16,-1648.88C1368.54,-1638.99 1347.98,-1628.07 1328.83,-1617.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1330.33,-1614.73 1319.85,-1613.12 1327.04,-1620.91 1330.33,-1614.73\"/>\n",
       "</g>\n",
       "<!-- 85 -->\n",
       "<g id=\"node86\" class=\"node\">\n",
       "<title>85</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1524.5,-1613C1524.5,-1613 1374.5,-1613 1374.5,-1613 1368.5,-1613 1362.5,-1607 1362.5,-1601 1362.5,-1601 1362.5,-1557 1362.5,-1557 1362.5,-1551 1368.5,-1545 1374.5,-1545 1374.5,-1545 1524.5,-1545 1524.5,-1545 1530.5,-1545 1536.5,-1551 1536.5,-1557 1536.5,-1557 1536.5,-1601 1536.5,-1601 1536.5,-1607 1530.5,-1613 1524.5,-1613\"/>\n",
       "<text text-anchor=\"start\" x=\"1386.5\" y=\"-1597.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 4.458</text>\n",
       "<text text-anchor=\"start\" x=\"1370.5\" y=\"-1582.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.437</text>\n",
       "<text text-anchor=\"start\" x=\"1396.5\" y=\"-1567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 145</text>\n",
       "<text text-anchor=\"start\" x=\"1399.5\" y=\"-1552.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.813</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;85 -->\n",
       "<g id=\"edge85\" class=\"edge\">\n",
       "<title>53&#45;&gt;85</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1449.5,-1648.88C1449.5,-1640.78 1449.5,-1631.98 1449.5,-1623.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1453,-1623.3 1449.5,-1613.3 1446,-1623.3 1453,-1623.3\"/>\n",
       "</g>\n",
       "<!-- 55 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>55</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1140.5,-1509C1140.5,-1509 990.5,-1509 990.5,-1509 984.5,-1509 978.5,-1503 978.5,-1497 978.5,-1497 978.5,-1453 978.5,-1453 978.5,-1447 984.5,-1441 990.5,-1441 990.5,-1441 1140.5,-1441 1140.5,-1441 1146.5,-1441 1152.5,-1447 1152.5,-1453 1152.5,-1453 1152.5,-1497 1152.5,-1497 1152.5,-1503 1146.5,-1509 1140.5,-1509\"/>\n",
       "<text text-anchor=\"start\" x=\"1009.5\" y=\"-1493.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">MinRTT â‰¤ 0.149</text>\n",
       "<text text-anchor=\"start\" x=\"986.5\" y=\"-1478.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.554</text>\n",
       "<text text-anchor=\"start\" x=\"1012.5\" y=\"-1463.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 365</text>\n",
       "<text text-anchor=\"start\" x=\"1015.5\" y=\"-1448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.123</text>\n",
       "</g>\n",
       "<!-- 54&#45;&gt;55 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>54&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1195.16,-1544.88C1176.54,-1534.99 1155.98,-1524.07 1136.83,-1513.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1138.33,-1510.73 1127.85,-1509.12 1135.04,-1516.91 1138.33,-1510.73\"/>\n",
       "</g>\n",
       "<!-- 84 -->\n",
       "<g id=\"node85\" class=\"node\">\n",
       "<title>84</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1332.5,-1501.5C1332.5,-1501.5 1182.5,-1501.5 1182.5,-1501.5 1176.5,-1501.5 1170.5,-1495.5 1170.5,-1489.5 1170.5,-1489.5 1170.5,-1460.5 1170.5,-1460.5 1170.5,-1454.5 1176.5,-1448.5 1182.5,-1448.5 1182.5,-1448.5 1332.5,-1448.5 1332.5,-1448.5 1338.5,-1448.5 1344.5,-1454.5 1344.5,-1460.5 1344.5,-1460.5 1344.5,-1489.5 1344.5,-1489.5 1344.5,-1495.5 1338.5,-1501.5 1332.5,-1501.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1178.5\" y=\"-1486.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.062</text>\n",
       "<text text-anchor=\"start\" x=\"1209\" y=\"-1471.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n",
       "<text text-anchor=\"start\" x=\"1207.5\" y=\"-1456.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.119</text>\n",
       "</g>\n",
       "<!-- 54&#45;&gt;84 -->\n",
       "<g id=\"edge84\" class=\"edge\">\n",
       "<title>54&#45;&gt;84</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1257.5,-1544.88C1257.5,-1534.33 1257.5,-1522.6 1257.5,-1511.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1261,-1511.52 1257.5,-1501.52 1254,-1511.52 1261,-1511.52\"/>\n",
       "</g>\n",
       "<!-- 56 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>56</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1140.5,-1405C1140.5,-1405 990.5,-1405 990.5,-1405 984.5,-1405 978.5,-1399 978.5,-1393 978.5,-1393 978.5,-1349 978.5,-1349 978.5,-1343 984.5,-1337 990.5,-1337 990.5,-1337 1140.5,-1337 1140.5,-1337 1146.5,-1337 1152.5,-1343 1152.5,-1349 1152.5,-1349 1152.5,-1393 1152.5,-1393 1152.5,-1399 1146.5,-1405 1140.5,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"996.5\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent â‰¤ &#45;0.453</text>\n",
       "<text text-anchor=\"start\" x=\"986.5\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.504</text>\n",
       "<text text-anchor=\"start\" x=\"1012.5\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 361</text>\n",
       "<text text-anchor=\"start\" x=\"1015.5\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.147</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;56 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>55&#45;&gt;56</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1065.5,-1440.88C1065.5,-1432.78 1065.5,-1423.98 1065.5,-1415.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1069,-1415.3 1065.5,-1405.3 1062,-1415.3 1069,-1415.3\"/>\n",
       "</g>\n",
       "<!-- 83 -->\n",
       "<g id=\"node84\" class=\"node\">\n",
       "<title>83</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1324,-1397.5C1324,-1397.5 1183,-1397.5 1183,-1397.5 1177,-1397.5 1171,-1391.5 1171,-1385.5 1171,-1385.5 1171,-1356.5 1171,-1356.5 1171,-1350.5 1177,-1344.5 1183,-1344.5 1183,-1344.5 1324,-1344.5 1324,-1344.5 1330,-1344.5 1336,-1350.5 1336,-1356.5 1336,-1356.5 1336,-1385.5 1336,-1385.5 1336,-1391.5 1330,-1397.5 1324,-1397.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1179\" y=\"-1382.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.19</text>\n",
       "<text text-anchor=\"start\" x=\"1209.5\" y=\"-1367.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"1203.5\" y=\"-1352.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.919</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;83 -->\n",
       "<g id=\"edge83\" class=\"edge\">\n",
       "<title>55&#45;&gt;83</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1126.54,-1440.88C1149.34,-1428.51 1175.11,-1414.53 1197.44,-1402.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1199.34,-1405.37 1206.46,-1397.52 1196,-1399.21 1199.34,-1405.37\"/>\n",
       "</g>\n",
       "<!-- 57 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>57</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1035.5,-1293.5C1035.5,-1293.5 885.5,-1293.5 885.5,-1293.5 879.5,-1293.5 873.5,-1287.5 873.5,-1281.5 873.5,-1281.5 873.5,-1252.5 873.5,-1252.5 873.5,-1246.5 879.5,-1240.5 885.5,-1240.5 885.5,-1240.5 1035.5,-1240.5 1035.5,-1240.5 1041.5,-1240.5 1047.5,-1246.5 1047.5,-1252.5 1047.5,-1252.5 1047.5,-1281.5 1047.5,-1281.5 1047.5,-1287.5 1041.5,-1293.5 1035.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"881.5\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.475</text>\n",
       "<text text-anchor=\"start\" x=\"912\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 34</text>\n",
       "<text text-anchor=\"start\" x=\"910.5\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.546</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;57 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>56&#45;&gt;57</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1031.41,-1336.88C1019.41,-1325.23 1005.94,-1312.14 993.99,-1300.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"996.38,-1297.98 986.77,-1293.52 991.5,-1303 996.38,-1297.98\"/>\n",
       "</g>\n",
       "<!-- 58 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>58</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1241.5,-1301C1241.5,-1301 1077.5,-1301 1077.5,-1301 1071.5,-1301 1065.5,-1295 1065.5,-1289 1065.5,-1289 1065.5,-1245 1065.5,-1245 1065.5,-1239 1071.5,-1233 1077.5,-1233 1077.5,-1233 1241.5,-1233 1241.5,-1233 1247.5,-1233 1253.5,-1239 1253.5,-1245 1253.5,-1245 1253.5,-1289 1253.5,-1289 1253.5,-1295 1247.5,-1301 1241.5,-1301\"/>\n",
       "<text text-anchor=\"start\" x=\"1073.5\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ElapsedTime_2 â‰¤ &#45;1.421</text>\n",
       "<text text-anchor=\"start\" x=\"1080.5\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.465</text>\n",
       "<text text-anchor=\"start\" x=\"1106.5\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 327</text>\n",
       "<text text-anchor=\"start\" x=\"1114\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.21</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;58 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>56&#45;&gt;58</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1096.02,-1336.88C1104.31,-1327.89 1113.38,-1318.04 1122.01,-1308.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1124.61,-1311.02 1128.81,-1301.3 1119.46,-1306.28 1124.61,-1311.02\"/>\n",
       "</g>\n",
       "<!-- 59 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>59</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1043.5,-1189.5C1043.5,-1189.5 911.5,-1189.5 911.5,-1189.5 905.5,-1189.5 899.5,-1183.5 899.5,-1177.5 899.5,-1177.5 899.5,-1148.5 899.5,-1148.5 899.5,-1142.5 905.5,-1136.5 911.5,-1136.5 911.5,-1136.5 1043.5,-1136.5 1043.5,-1136.5 1049.5,-1136.5 1055.5,-1142.5 1055.5,-1148.5 1055.5,-1148.5 1055.5,-1177.5 1055.5,-1177.5 1055.5,-1183.5 1049.5,-1189.5 1043.5,-1189.5\"/>\n",
       "<text text-anchor=\"start\" x=\"907.5\" y=\"-1174.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"933.5\" y=\"-1159.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"927.5\" y=\"-1144.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.416</text>\n",
       "</g>\n",
       "<!-- 58&#45;&gt;59 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>58&#45;&gt;59</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1100.41,-1232.88C1078.33,-1220.51 1053.39,-1206.53 1031.77,-1194.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1033.47,-1191.36 1023.04,-1189.52 1030.05,-1197.46 1033.47,-1191.36\"/>\n",
       "</g>\n",
       "<!-- 60 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>60</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1235.5,-1197C1235.5,-1197 1085.5,-1197 1085.5,-1197 1079.5,-1197 1073.5,-1191 1073.5,-1185 1073.5,-1185 1073.5,-1141 1073.5,-1141 1073.5,-1135 1079.5,-1129 1085.5,-1129 1085.5,-1129 1235.5,-1129 1235.5,-1129 1241.5,-1129 1247.5,-1135 1247.5,-1141 1247.5,-1141 1247.5,-1185 1247.5,-1185 1247.5,-1191 1241.5,-1197 1235.5,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"1101.5\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">MinRTT â‰¤ &#45;0.209</text>\n",
       "<text text-anchor=\"start\" x=\"1081.5\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.435</text>\n",
       "<text text-anchor=\"start\" x=\"1107.5\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 326</text>\n",
       "<text text-anchor=\"start\" x=\"1119.5\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.2</text>\n",
       "</g>\n",
       "<!-- 58&#45;&gt;60 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>58&#45;&gt;60</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1159.82,-1232.88C1159.9,-1224.78 1159.99,-1215.98 1160.07,-1207.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1163.58,-1207.33 1160.17,-1197.3 1156.58,-1207.26 1163.58,-1207.33\"/>\n",
       "</g>\n",
       "<!-- 61 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>61</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1043.5,-1093C1043.5,-1093 893.5,-1093 893.5,-1093 887.5,-1093 881.5,-1087 881.5,-1081 881.5,-1081 881.5,-1037 881.5,-1037 881.5,-1031 887.5,-1025 893.5,-1025 893.5,-1025 1043.5,-1025 1043.5,-1025 1049.5,-1025 1055.5,-1031 1055.5,-1037 1055.5,-1037 1055.5,-1081 1055.5,-1081 1055.5,-1087 1049.5,-1093 1043.5,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"899.5\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent â‰¤ &#45;0.448</text>\n",
       "<text text-anchor=\"start\" x=\"889.5\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.376</text>\n",
       "<text text-anchor=\"start\" x=\"915.5\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 251</text>\n",
       "<text text-anchor=\"start\" x=\"918.5\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.103</text>\n",
       "</g>\n",
       "<!-- 60&#45;&gt;61 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>60&#45;&gt;61</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1098.16,-1128.88C1079.54,-1118.99 1058.98,-1108.07 1039.83,-1097.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1041.33,-1094.73 1030.85,-1093.12 1038.04,-1100.91 1041.33,-1094.73\"/>\n",
       "</g>\n",
       "<!-- 82 -->\n",
       "<g id=\"node83\" class=\"node\">\n",
       "<title>82</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1235.5,-1085.5C1235.5,-1085.5 1085.5,-1085.5 1085.5,-1085.5 1079.5,-1085.5 1073.5,-1079.5 1073.5,-1073.5 1073.5,-1073.5 1073.5,-1044.5 1073.5,-1044.5 1073.5,-1038.5 1079.5,-1032.5 1085.5,-1032.5 1085.5,-1032.5 1235.5,-1032.5 1235.5,-1032.5 1241.5,-1032.5 1247.5,-1038.5 1247.5,-1044.5 1247.5,-1044.5 1247.5,-1073.5 1247.5,-1073.5 1247.5,-1079.5 1241.5,-1085.5 1235.5,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1081.5\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.497</text>\n",
       "<text text-anchor=\"start\" x=\"1112\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 75</text>\n",
       "<text text-anchor=\"start\" x=\"1110.5\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.524</text>\n",
       "</g>\n",
       "<!-- 60&#45;&gt;82 -->\n",
       "<g id=\"edge82\" class=\"edge\">\n",
       "<title>60&#45;&gt;82</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1160.5,-1128.88C1160.5,-1118.33 1160.5,-1106.6 1160.5,-1095.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164,-1095.52 1160.5,-1085.52 1157,-1095.52 1164,-1095.52\"/>\n",
       "</g>\n",
       "<!-- 62 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>62</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1035.5,-989C1035.5,-989 885.5,-989 885.5,-989 879.5,-989 873.5,-983 873.5,-977 873.5,-977 873.5,-933 873.5,-933 873.5,-927 879.5,-921 885.5,-921 885.5,-921 1035.5,-921 1035.5,-921 1041.5,-921 1047.5,-927 1047.5,-933 1047.5,-933 1047.5,-977 1047.5,-977 1047.5,-983 1041.5,-989 1035.5,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"897.5\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_3 â‰¤ 0.001</text>\n",
       "<text text-anchor=\"start\" x=\"881.5\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.286</text>\n",
       "<text text-anchor=\"start\" x=\"907.5\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 193</text>\n",
       "<text text-anchor=\"start\" x=\"910.5\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.999</text>\n",
       "</g>\n",
       "<!-- 61&#45;&gt;62 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>61&#45;&gt;62</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M965.9,-1024.88C965.27,-1016.78 964.58,-1007.98 963.91,-999.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"967.38,-999 963.11,-989.3 960.4,-999.54 967.38,-999\"/>\n",
       "</g>\n",
       "<!-- 81 -->\n",
       "<g id=\"node82\" class=\"node\">\n",
       "<title>81</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1227.5,-981.5C1227.5,-981.5 1077.5,-981.5 1077.5,-981.5 1071.5,-981.5 1065.5,-975.5 1065.5,-969.5 1065.5,-969.5 1065.5,-940.5 1065.5,-940.5 1065.5,-934.5 1071.5,-928.5 1077.5,-928.5 1077.5,-928.5 1227.5,-928.5 1227.5,-928.5 1233.5,-928.5 1239.5,-934.5 1239.5,-940.5 1239.5,-940.5 1239.5,-969.5 1239.5,-969.5 1239.5,-975.5 1233.5,-981.5 1227.5,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1073.5\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.518</text>\n",
       "<text text-anchor=\"start\" x=\"1104\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 58</text>\n",
       "<text text-anchor=\"start\" x=\"1102.5\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.451</text>\n",
       "</g>\n",
       "<!-- 61&#45;&gt;81 -->\n",
       "<g id=\"edge81\" class=\"edge\">\n",
       "<title>61&#45;&gt;81</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1028.24,-1024.88C1050.56,-1012.51 1075.78,-998.53 1097.64,-986.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1099.41,-989.43 1106.46,-981.52 1096.02,-983.31 1099.41,-989.43\"/>\n",
       "</g>\n",
       "<!-- 63 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>63</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M987.5,-877.5C987.5,-877.5 837.5,-877.5 837.5,-877.5 831.5,-877.5 825.5,-871.5 825.5,-865.5 825.5,-865.5 825.5,-836.5 825.5,-836.5 825.5,-830.5 831.5,-824.5 837.5,-824.5 837.5,-824.5 987.5,-824.5 987.5,-824.5 993.5,-824.5 999.5,-830.5 999.5,-836.5 999.5,-836.5 999.5,-865.5 999.5,-865.5 999.5,-871.5 993.5,-877.5 987.5,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"833.5\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.295</text>\n",
       "<text text-anchor=\"start\" x=\"864\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 47</text>\n",
       "<text text-anchor=\"start\" x=\"862.5\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.325</text>\n",
       "</g>\n",
       "<!-- 62&#45;&gt;63 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>62&#45;&gt;63</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M944.92,-920.88C939.79,-910 934.08,-897.86 928.9,-886.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"931.93,-885.08 924.51,-877.52 925.6,-888.06 931.93,-885.08\"/>\n",
       "</g>\n",
       "<!-- 64 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>64</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1179.5,-885C1179.5,-885 1029.5,-885 1029.5,-885 1023.5,-885 1017.5,-879 1017.5,-873 1017.5,-873 1017.5,-829 1017.5,-829 1017.5,-823 1023.5,-817 1029.5,-817 1029.5,-817 1179.5,-817 1179.5,-817 1185.5,-817 1191.5,-823 1191.5,-829 1191.5,-829 1191.5,-873 1191.5,-873 1191.5,-879 1185.5,-885 1179.5,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"1049.5\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar â‰¤ 0.441</text>\n",
       "<text text-anchor=\"start\" x=\"1025.5\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.238</text>\n",
       "<text text-anchor=\"start\" x=\"1051.5\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 146</text>\n",
       "<text text-anchor=\"start\" x=\"1054.5\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.894</text>\n",
       "</g>\n",
       "<!-- 62&#45;&gt;64 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>62&#45;&gt;64</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1007.25,-920.88C1020.58,-911.44 1035.23,-901.06 1049.03,-891.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1051.35,-893.94 1057.49,-885.3 1047.31,-888.22 1051.35,-893.94\"/>\n",
       "</g>\n",
       "<!-- 65 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>65</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M987.5,-781C987.5,-781 837.5,-781 837.5,-781 831.5,-781 825.5,-775 825.5,-769 825.5,-769 825.5,-725 825.5,-725 825.5,-719 831.5,-713 837.5,-713 837.5,-713 987.5,-713 987.5,-713 993.5,-713 999.5,-719 999.5,-725 999.5,-725 999.5,-769 999.5,-769 999.5,-775 993.5,-781 987.5,-781\"/>\n",
       "<text text-anchor=\"start\" x=\"861\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_2 â‰¤ 0.261</text>\n",
       "<text text-anchor=\"start\" x=\"833.5\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.199</text>\n",
       "<text text-anchor=\"start\" x=\"859.5\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 113</text>\n",
       "<text text-anchor=\"start\" x=\"862.5\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.782</text>\n",
       "</g>\n",
       "<!-- 64&#45;&gt;65 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>64&#45;&gt;65</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1042.16,-816.88C1023.54,-806.99 1002.98,-796.07 983.83,-785.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"985.33,-782.73 974.85,-781.12 982.04,-788.91 985.33,-782.73\"/>\n",
       "</g>\n",
       "<!-- 80 -->\n",
       "<g id=\"node81\" class=\"node\">\n",
       "<title>80</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1179.5,-773.5C1179.5,-773.5 1029.5,-773.5 1029.5,-773.5 1023.5,-773.5 1017.5,-767.5 1017.5,-761.5 1017.5,-761.5 1017.5,-732.5 1017.5,-732.5 1017.5,-726.5 1023.5,-720.5 1029.5,-720.5 1029.5,-720.5 1179.5,-720.5 1179.5,-720.5 1185.5,-720.5 1191.5,-726.5 1191.5,-732.5 1191.5,-732.5 1191.5,-761.5 1191.5,-761.5 1191.5,-767.5 1185.5,-773.5 1179.5,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1025.5\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.183</text>\n",
       "<text text-anchor=\"start\" x=\"1056\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n",
       "<text text-anchor=\"start\" x=\"1054.5\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.279</text>\n",
       "</g>\n",
       "<!-- 64&#45;&gt;80 -->\n",
       "<g id=\"edge80\" class=\"edge\">\n",
       "<title>64&#45;&gt;80</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1104.5,-816.88C1104.5,-806.33 1104.5,-794.6 1104.5,-783.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1108,-783.52 1104.5,-773.52 1101,-783.52 1108,-783.52\"/>\n",
       "</g>\n",
       "<!-- 66 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>66</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M819,-669.5C819,-669.5 678,-669.5 678,-669.5 672,-669.5 666,-663.5 666,-657.5 666,-657.5 666,-628.5 666,-628.5 666,-622.5 672,-616.5 678,-616.5 678,-616.5 819,-616.5 819,-616.5 825,-616.5 831,-622.5 831,-628.5 831,-628.5 831,-657.5 831,-657.5 831,-663.5 825,-669.5 819,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"674\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.34</text>\n",
       "<text text-anchor=\"start\" x=\"704.5\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"698.5\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 3.948</text>\n",
       "</g>\n",
       "<!-- 65&#45;&gt;66 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>65&#45;&gt;66</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M859.25,-712.88C839.63,-700.68 817.48,-686.9 798.18,-674.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"799.87,-671.83 789.53,-669.52 796.18,-677.77 799.87,-671.83\"/>\n",
       "</g>\n",
       "<!-- 67 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>67</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1011.5,-677C1011.5,-677 861.5,-677 861.5,-677 855.5,-677 849.5,-671 849.5,-665 849.5,-665 849.5,-621 849.5,-621 849.5,-615 855.5,-609 861.5,-609 861.5,-609 1011.5,-609 1011.5,-609 1017.5,-609 1023.5,-615 1023.5,-621 1023.5,-621 1023.5,-665 1023.5,-665 1023.5,-671 1017.5,-677 1011.5,-677\"/>\n",
       "<text text-anchor=\"start\" x=\"873.5\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_1 â‰¤ 0.535</text>\n",
       "<text text-anchor=\"start\" x=\"857.5\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.175</text>\n",
       "<text text-anchor=\"start\" x=\"883.5\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 110</text>\n",
       "<text text-anchor=\"start\" x=\"886.5\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.804</text>\n",
       "</g>\n",
       "<!-- 65&#45;&gt;67 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>65&#45;&gt;67</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M920.29,-712.88C922.22,-704.69 924.31,-695.79 926.34,-687.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"929.78,-687.84 928.66,-677.3 922.97,-686.23 929.78,-687.84\"/>\n",
       "</g>\n",
       "<!-- 68 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>68</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M917,-573C917,-573 762,-573 762,-573 756,-573 750,-567 750,-561 750,-561 750,-517 750,-517 750,-511 756,-505 762,-505 762,-505 917,-505 917,-505 923,-505 929,-511 929,-517 929,-517 929,-561 929,-561 929,-567 923,-573 917,-573\"/>\n",
       "<text text-anchor=\"start\" x=\"758\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesAcked_3 â‰¤ &#45;0.443</text>\n",
       "<text text-anchor=\"start\" x=\"760.5\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.152</text>\n",
       "<text text-anchor=\"start\" x=\"791\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 91</text>\n",
       "<text text-anchor=\"start\" x=\"789.5\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.749</text>\n",
       "</g>\n",
       "<!-- 67&#45;&gt;68 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>67&#45;&gt;68</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M905.01,-608.88C896.45,-599.89 887.09,-590.04 878.18,-580.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"880.59,-578.13 871.17,-573.3 875.52,-582.96 880.59,-578.13\"/>\n",
       "</g>\n",
       "<!-- 79 -->\n",
       "<g id=\"node80\" class=\"node\">\n",
       "<title>79</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1109.5,-565.5C1109.5,-565.5 959.5,-565.5 959.5,-565.5 953.5,-565.5 947.5,-559.5 947.5,-553.5 947.5,-553.5 947.5,-524.5 947.5,-524.5 947.5,-518.5 953.5,-512.5 959.5,-512.5 959.5,-512.5 1109.5,-512.5 1109.5,-512.5 1115.5,-512.5 1121.5,-518.5 1121.5,-524.5 1121.5,-524.5 1121.5,-553.5 1121.5,-553.5 1121.5,-559.5 1115.5,-565.5 1109.5,-565.5\"/>\n",
       "<text text-anchor=\"start\" x=\"955.5\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.204</text>\n",
       "<text text-anchor=\"start\" x=\"986\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 19</text>\n",
       "<text text-anchor=\"start\" x=\"984.5\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.068</text>\n",
       "</g>\n",
       "<!-- 67&#45;&gt;79 -->\n",
       "<g id=\"edge79\" class=\"edge\">\n",
       "<title>67&#45;&gt;79</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M968.32,-608.88C979.41,-597.34 991.85,-584.39 1002.93,-572.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1005.58,-575.16 1009.98,-565.52 1000.53,-570.31 1005.58,-575.16\"/>\n",
       "</g>\n",
       "<!-- 69 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>69</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M821.5,-469C821.5,-469 671.5,-469 671.5,-469 665.5,-469 659.5,-463 659.5,-457 659.5,-457 659.5,-413 659.5,-413 659.5,-407 665.5,-401 671.5,-401 671.5,-401 821.5,-401 821.5,-401 827.5,-401 833.5,-407 833.5,-413 833.5,-413 833.5,-457 833.5,-457 833.5,-463 827.5,-469 821.5,-469\"/>\n",
       "<text text-anchor=\"start\" x=\"695\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_4 â‰¤ 0.196</text>\n",
       "<text text-anchor=\"start\" x=\"667.5\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.139</text>\n",
       "<text text-anchor=\"start\" x=\"698\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 90</text>\n",
       "<text text-anchor=\"start\" x=\"696.5\" y=\"-408.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.762</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;69 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>68&#45;&gt;69</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M809.31,-504.88C801.18,-495.98 792.3,-486.24 783.84,-476.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"786.19,-474.33 776.86,-469.3 781.01,-479.05 786.19,-474.33\"/>\n",
       "</g>\n",
       "<!-- 78 -->\n",
       "<g id=\"node79\" class=\"node\">\n",
       "<title>78</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1001,-461.5C1001,-461.5 864,-461.5 864,-461.5 858,-461.5 852,-455.5 852,-449.5 852,-449.5 852,-420.5 852,-420.5 852,-414.5 858,-408.5 864,-408.5 864,-408.5 1001,-408.5 1001,-408.5 1007,-408.5 1013,-414.5 1013,-420.5 1013,-420.5 1013,-449.5 1013,-449.5 1013,-455.5 1007,-461.5 1001,-461.5\"/>\n",
       "<text text-anchor=\"start\" x=\"860\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = &#45;0.0</text>\n",
       "<text text-anchor=\"start\" x=\"888.5\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"882.5\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 3.636</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;78 -->\n",
       "<g id=\"edge78\" class=\"edge\">\n",
       "<title>68&#45;&gt;78</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M869.69,-504.88C880.12,-493.45 891.8,-480.63 902.24,-469.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"905.08,-471.27 909.23,-461.52 899.91,-466.55 905.08,-471.27\"/>\n",
       "</g>\n",
       "<!-- 70 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>70</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M721.5,-357.5C721.5,-357.5 589.5,-357.5 589.5,-357.5 583.5,-357.5 577.5,-351.5 577.5,-345.5 577.5,-345.5 577.5,-316.5 577.5,-316.5 577.5,-310.5 583.5,-304.5 589.5,-304.5 589.5,-304.5 721.5,-304.5 721.5,-304.5 727.5,-304.5 733.5,-310.5 733.5,-316.5 733.5,-316.5 733.5,-345.5 733.5,-345.5 733.5,-351.5 727.5,-357.5 721.5,-357.5\"/>\n",
       "<text text-anchor=\"start\" x=\"585.5\" y=\"-342.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"611.5\" y=\"-327.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"605.5\" y=\"-312.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.845</text>\n",
       "</g>\n",
       "<!-- 69&#45;&gt;70 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>69&#45;&gt;70</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M716.95,-400.88C706.75,-389.45 695.32,-376.63 685.11,-365.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"687.54,-362.65 678.27,-357.52 682.31,-367.31 687.54,-362.65\"/>\n",
       "</g>\n",
       "<!-- 71 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>71</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M913.5,-365C913.5,-365 763.5,-365 763.5,-365 757.5,-365 751.5,-359 751.5,-353 751.5,-353 751.5,-309 751.5,-309 751.5,-303 757.5,-297 763.5,-297 763.5,-297 913.5,-297 913.5,-297 919.5,-297 925.5,-303 925.5,-309 925.5,-309 925.5,-353 925.5,-353 925.5,-359 919.5,-365 913.5,-365\"/>\n",
       "<text text-anchor=\"start\" x=\"794.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT â‰¤ 1.513</text>\n",
       "<text text-anchor=\"start\" x=\"759.5\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.128</text>\n",
       "<text text-anchor=\"start\" x=\"790\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 89</text>\n",
       "<text text-anchor=\"start\" x=\"788.5\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.749</text>\n",
       "</g>\n",
       "<!-- 69&#45;&gt;71 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>69&#45;&gt;71</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M776.37,-400.88C784.4,-391.98 793.19,-382.24 801.56,-372.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"804.37,-375.07 808.47,-365.3 799.17,-370.38 804.37,-375.07\"/>\n",
       "</g>\n",
       "<!-- 72 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>72</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M817.5,-261C817.5,-261 667.5,-261 667.5,-261 661.5,-261 655.5,-255 655.5,-249 655.5,-249 655.5,-205 655.5,-205 655.5,-199 661.5,-193 667.5,-193 667.5,-193 817.5,-193 817.5,-193 823.5,-193 829.5,-199 829.5,-205 829.5,-205 829.5,-249 829.5,-249 829.5,-255 823.5,-261 817.5,-261\"/>\n",
       "<text text-anchor=\"start\" x=\"679.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_3 â‰¤ 0.229</text>\n",
       "<text text-anchor=\"start\" x=\"663.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.125</text>\n",
       "<text text-anchor=\"start\" x=\"694\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 84</text>\n",
       "<text text-anchor=\"start\" x=\"692.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.771</text>\n",
       "</g>\n",
       "<!-- 71&#45;&gt;72 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>71&#45;&gt;72</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M807.33,-296.88C798.86,-287.89 789.6,-278.04 780.79,-268.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"783.24,-266.18 773.84,-261.3 778.15,-270.98 783.24,-266.18\"/>\n",
       "</g>\n",
       "<!-- 77 -->\n",
       "<g id=\"node78\" class=\"node\">\n",
       "<title>77</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1009.5,-253.5C1009.5,-253.5 859.5,-253.5 859.5,-253.5 853.5,-253.5 847.5,-247.5 847.5,-241.5 847.5,-241.5 847.5,-212.5 847.5,-212.5 847.5,-206.5 853.5,-200.5 859.5,-200.5 859.5,-200.5 1009.5,-200.5 1009.5,-200.5 1015.5,-200.5 1021.5,-206.5 1021.5,-212.5 1021.5,-212.5 1021.5,-241.5 1021.5,-241.5 1021.5,-247.5 1015.5,-253.5 1009.5,-253.5\"/>\n",
       "<text text-anchor=\"start\" x=\"855.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.023</text>\n",
       "<text text-anchor=\"start\" x=\"890.5\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"889\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.38</text>\n",
       "</g>\n",
       "<!-- 71&#45;&gt;77 -->\n",
       "<g id=\"edge77\" class=\"edge\">\n",
       "<title>71&#45;&gt;77</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M869.67,-296.88C880.54,-285.34 892.72,-272.39 903.57,-260.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"906.18,-263.2 910.48,-253.52 901.08,-258.4 906.18,-263.2\"/>\n",
       "</g>\n",
       "<!-- 73 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>73</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M721.5,-157C721.5,-157 571.5,-157 571.5,-157 565.5,-157 559.5,-151 559.5,-145 559.5,-145 559.5,-101 559.5,-101 559.5,-95 565.5,-89 571.5,-89 571.5,-89 721.5,-89 721.5,-89 727.5,-89 733.5,-95 733.5,-101 733.5,-101 733.5,-145 733.5,-145 733.5,-151 727.5,-157 721.5,-157\"/>\n",
       "<text text-anchor=\"start\" x=\"583.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 0.182</text>\n",
       "<text text-anchor=\"start\" x=\"567.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.104</text>\n",
       "<text text-anchor=\"start\" x=\"598\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 34</text>\n",
       "<text text-anchor=\"start\" x=\"596.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.909</text>\n",
       "</g>\n",
       "<!-- 72&#45;&gt;73 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>72&#45;&gt;73</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M711.33,-192.88C702.86,-183.89 693.6,-174.04 684.79,-164.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"687.24,-162.18 677.84,-157.3 682.15,-166.98 687.24,-162.18\"/>\n",
       "</g>\n",
       "<!-- 76 -->\n",
       "<g id=\"node77\" class=\"node\">\n",
       "<title>76</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M913.5,-149.5C913.5,-149.5 763.5,-149.5 763.5,-149.5 757.5,-149.5 751.5,-143.5 751.5,-137.5 751.5,-137.5 751.5,-108.5 751.5,-108.5 751.5,-102.5 757.5,-96.5 763.5,-96.5 763.5,-96.5 913.5,-96.5 913.5,-96.5 919.5,-96.5 925.5,-102.5 925.5,-108.5 925.5,-108.5 925.5,-137.5 925.5,-137.5 925.5,-143.5 919.5,-149.5 913.5,-149.5\"/>\n",
       "<text text-anchor=\"start\" x=\"759.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.118</text>\n",
       "<text text-anchor=\"start\" x=\"790\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"788.5\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.678</text>\n",
       "</g>\n",
       "<!-- 72&#45;&gt;76 -->\n",
       "<g id=\"edge76\" class=\"edge\">\n",
       "<title>72&#45;&gt;76</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M773.67,-192.88C784.54,-181.34 796.72,-168.39 807.57,-156.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"810.18,-159.2 814.48,-149.52 805.08,-154.4 810.18,-159.2\"/>\n",
       "</g>\n",
       "<!-- 74 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>74</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M625.5,-53C625.5,-53 475.5,-53 475.5,-53 469.5,-53 463.5,-47 463.5,-41 463.5,-41 463.5,-12 463.5,-12 463.5,-6 469.5,0 475.5,0 475.5,0 625.5,0 625.5,0 631.5,0 637.5,-6 637.5,-12 637.5,-12 637.5,-41 637.5,-41 637.5,-47 631.5,-53 625.5,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"471.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.089</text>\n",
       "<text text-anchor=\"start\" x=\"502\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 22</text>\n",
       "<text text-anchor=\"start\" x=\"500.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.773</text>\n",
       "</g>\n",
       "<!-- 73&#45;&gt;74 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>73&#45;&gt;74</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M612.92,-88.95C603.54,-79.71 593.34,-69.67 583.96,-60.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"586.23,-57.76 576.65,-53.24 581.32,-62.75 586.23,-57.76\"/>\n",
       "</g>\n",
       "<!-- 75 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>75</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M817.5,-53C817.5,-53 667.5,-53 667.5,-53 661.5,-53 655.5,-47 655.5,-41 655.5,-41 655.5,-12 655.5,-12 655.5,-6 661.5,0 667.5,0 667.5,0 817.5,0 817.5,0 823.5,0 829.5,-6 829.5,-12 829.5,-12 829.5,-41 829.5,-41 829.5,-47 823.5,-53 817.5,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"663.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.037</text>\n",
       "<text text-anchor=\"start\" x=\"694\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12</text>\n",
       "<text text-anchor=\"start\" x=\"692.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.157</text>\n",
       "</g>\n",
       "<!-- 73&#45;&gt;75 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>73&#45;&gt;75</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M680.08,-88.95C689.46,-79.71 699.66,-69.67 709.04,-60.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"711.68,-62.75 716.35,-53.24 706.77,-57.76 711.68,-62.75\"/>\n",
       "</g>\n",
       "<!-- 86 -->\n",
       "<g id=\"node87\" class=\"node\">\n",
       "<title>86</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1524.5,-1509C1524.5,-1509 1374.5,-1509 1374.5,-1509 1368.5,-1509 1362.5,-1503 1362.5,-1497 1362.5,-1497 1362.5,-1453 1362.5,-1453 1362.5,-1447 1368.5,-1441 1374.5,-1441 1374.5,-1441 1524.5,-1441 1524.5,-1441 1530.5,-1441 1536.5,-1447 1536.5,-1453 1536.5,-1453 1536.5,-1497 1536.5,-1497 1536.5,-1503 1530.5,-1509 1524.5,-1509\"/>\n",
       "<text text-anchor=\"start\" x=\"1372.5\" y=\"-1493.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesSent_1 â‰¤ &#45;0.443</text>\n",
       "<text text-anchor=\"start\" x=\"1370.5\" y=\"-1478.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.289</text>\n",
       "<text text-anchor=\"start\" x=\"1396.5\" y=\"-1463.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 136</text>\n",
       "<text text-anchor=\"start\" x=\"1399.5\" y=\"-1448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.712</text>\n",
       "</g>\n",
       "<!-- 85&#45;&gt;86 -->\n",
       "<g id=\"edge86\" class=\"edge\">\n",
       "<title>85&#45;&gt;86</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1449.5,-1544.88C1449.5,-1536.78 1449.5,-1527.98 1449.5,-1519.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1453,-1519.3 1449.5,-1509.3 1446,-1519.3 1453,-1519.3\"/>\n",
       "</g>\n",
       "<!-- 101 -->\n",
       "<g id=\"node102\" class=\"node\">\n",
       "<title>101</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1716.5,-1501.5C1716.5,-1501.5 1566.5,-1501.5 1566.5,-1501.5 1560.5,-1501.5 1554.5,-1495.5 1554.5,-1489.5 1554.5,-1489.5 1554.5,-1460.5 1554.5,-1460.5 1554.5,-1454.5 1560.5,-1448.5 1566.5,-1448.5 1566.5,-1448.5 1716.5,-1448.5 1716.5,-1448.5 1722.5,-1448.5 1728.5,-1454.5 1728.5,-1460.5 1728.5,-1460.5 1728.5,-1489.5 1728.5,-1489.5 1728.5,-1495.5 1722.5,-1501.5 1716.5,-1501.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1562.5\" y=\"-1486.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.171</text>\n",
       "<text text-anchor=\"start\" x=\"1597.5\" y=\"-1471.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"1591.5\" y=\"-1456.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.348</text>\n",
       "</g>\n",
       "<!-- 85&#45;&gt;101 -->\n",
       "<g id=\"edge101\" class=\"edge\">\n",
       "<title>85&#45;&gt;101</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1511.84,-1544.88C1535.23,-1532.46 1561.68,-1518.41 1584.56,-1506.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1586.27,-1509.3 1593.46,-1501.52 1582.99,-1503.12 1586.27,-1509.3\"/>\n",
       "</g>\n",
       "<!-- 87 -->\n",
       "<g id=\"node88\" class=\"node\">\n",
       "<title>87</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1521.5,-1405C1521.5,-1405 1371.5,-1405 1371.5,-1405 1365.5,-1405 1359.5,-1399 1359.5,-1393 1359.5,-1393 1359.5,-1349 1359.5,-1349 1359.5,-1343 1365.5,-1337 1371.5,-1337 1371.5,-1337 1521.5,-1337 1521.5,-1337 1527.5,-1337 1533.5,-1343 1533.5,-1349 1533.5,-1349 1533.5,-1393 1533.5,-1393 1533.5,-1399 1527.5,-1405 1521.5,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"1383.5\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_2 â‰¤ 0.082</text>\n",
       "<text text-anchor=\"start\" x=\"1367.5\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.259</text>\n",
       "<text text-anchor=\"start\" x=\"1393.5\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 132</text>\n",
       "<text text-anchor=\"start\" x=\"1396.5\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.679</text>\n",
       "</g>\n",
       "<!-- 86&#45;&gt;87 -->\n",
       "<g id=\"edge87\" class=\"edge\">\n",
       "<title>86&#45;&gt;87</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1448.53,-1440.88C1448.29,-1432.78 1448.03,-1423.98 1447.78,-1415.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1451.27,-1415.19 1447.48,-1405.3 1444.27,-1415.4 1451.27,-1415.19\"/>\n",
       "</g>\n",
       "<!-- 100 -->\n",
       "<g id=\"node101\" class=\"node\">\n",
       "<title>100</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1713.5,-1397.5C1713.5,-1397.5 1563.5,-1397.5 1563.5,-1397.5 1557.5,-1397.5 1551.5,-1391.5 1551.5,-1385.5 1551.5,-1385.5 1551.5,-1356.5 1551.5,-1356.5 1551.5,-1350.5 1557.5,-1344.5 1563.5,-1344.5 1563.5,-1344.5 1713.5,-1344.5 1713.5,-1344.5 1719.5,-1344.5 1725.5,-1350.5 1725.5,-1356.5 1725.5,-1356.5 1725.5,-1385.5 1725.5,-1385.5 1725.5,-1391.5 1719.5,-1397.5 1713.5,-1397.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1559.5\" y=\"-1382.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.041</text>\n",
       "<text text-anchor=\"start\" x=\"1594.5\" y=\"-1367.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"1588.5\" y=\"-1352.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.807</text>\n",
       "</g>\n",
       "<!-- 86&#45;&gt;100 -->\n",
       "<g id=\"edge100\" class=\"edge\">\n",
       "<title>86&#45;&gt;100</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1510.86,-1440.88C1533.79,-1428.51 1559.69,-1414.53 1582.15,-1402.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1584.08,-1405.35 1591.21,-1397.52 1580.75,-1399.19 1584.08,-1405.35\"/>\n",
       "</g>\n",
       "<!-- 88 -->\n",
       "<g id=\"node89\" class=\"node\">\n",
       "<title>88</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1512.5,-1293.5C1512.5,-1293.5 1362.5,-1293.5 1362.5,-1293.5 1356.5,-1293.5 1350.5,-1287.5 1350.5,-1281.5 1350.5,-1281.5 1350.5,-1252.5 1350.5,-1252.5 1350.5,-1246.5 1356.5,-1240.5 1362.5,-1240.5 1362.5,-1240.5 1512.5,-1240.5 1512.5,-1240.5 1518.5,-1240.5 1524.5,-1246.5 1524.5,-1252.5 1524.5,-1252.5 1524.5,-1281.5 1524.5,-1281.5 1524.5,-1287.5 1518.5,-1293.5 1512.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1358.5\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.348</text>\n",
       "<text text-anchor=\"start\" x=\"1389\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 20</text>\n",
       "<text text-anchor=\"start\" x=\"1387.5\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.232</text>\n",
       "</g>\n",
       "<!-- 87&#45;&gt;88 -->\n",
       "<g id=\"edge88\" class=\"edge\">\n",
       "<title>87&#45;&gt;88</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1443.58,-1336.88C1442.64,-1326.22 1441.59,-1314.35 1440.63,-1303.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.12,-1303.17 1439.75,-1293.52 1437.14,-1303.79 1444.12,-1303.17\"/>\n",
       "</g>\n",
       "<!-- 89 -->\n",
       "<g id=\"node90\" class=\"node\">\n",
       "<title>89</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1704.5,-1301C1704.5,-1301 1554.5,-1301 1554.5,-1301 1548.5,-1301 1542.5,-1295 1542.5,-1289 1542.5,-1289 1542.5,-1245 1542.5,-1245 1542.5,-1239 1548.5,-1233 1554.5,-1233 1554.5,-1233 1704.5,-1233 1704.5,-1233 1710.5,-1233 1716.5,-1239 1716.5,-1245 1716.5,-1245 1716.5,-1289 1716.5,-1289 1716.5,-1295 1710.5,-1301 1704.5,-1301\"/>\n",
       "<text text-anchor=\"start\" x=\"1578\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_4 â‰¤ 2.158</text>\n",
       "<text text-anchor=\"start\" x=\"1550.5\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.201</text>\n",
       "<text text-anchor=\"start\" x=\"1576.5\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 112</text>\n",
       "<text text-anchor=\"start\" x=\"1579.5\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.758</text>\n",
       "</g>\n",
       "<!-- 87&#45;&gt;89 -->\n",
       "<g id=\"edge89\" class=\"edge\">\n",
       "<title>87&#45;&gt;89</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1505.91,-1336.88C1523.58,-1327.04 1543.08,-1316.17 1561.26,-1306.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1563.04,-1309.05 1570.07,-1301.12 1559.63,-1302.94 1563.04,-1309.05\"/>\n",
       "</g>\n",
       "<!-- 90 -->\n",
       "<g id=\"node91\" class=\"node\">\n",
       "<title>90</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1561.5,-1197C1561.5,-1197 1411.5,-1197 1411.5,-1197 1405.5,-1197 1399.5,-1191 1399.5,-1185 1399.5,-1185 1399.5,-1141 1399.5,-1141 1399.5,-1135 1405.5,-1129 1411.5,-1129 1411.5,-1129 1561.5,-1129 1561.5,-1129 1567.5,-1129 1573.5,-1135 1573.5,-1141 1573.5,-1141 1573.5,-1185 1573.5,-1185 1573.5,-1191 1567.5,-1197 1561.5,-1197\"/>\n",
       "<text text-anchor=\"start\" x=\"1436.5\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTT_4 â‰¤ &#45;0.12</text>\n",
       "<text text-anchor=\"start\" x=\"1407.5\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.166</text>\n",
       "<text text-anchor=\"start\" x=\"1433.5\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 109</text>\n",
       "<text text-anchor=\"start\" x=\"1436.5\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.791</text>\n",
       "</g>\n",
       "<!-- 89&#45;&gt;90 -->\n",
       "<g id=\"edge90\" class=\"edge\">\n",
       "<title>89&#45;&gt;90</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1583.07,-1232.88C1569.83,-1223.44 1555.29,-1213.06 1541.59,-1203.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1543.36,-1200.26 1533.18,-1197.3 1539.29,-1205.96 1543.36,-1200.26\"/>\n",
       "</g>\n",
       "<!-- 99 -->\n",
       "<g id=\"node100\" class=\"node\">\n",
       "<title>99</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1753.5,-1189.5C1753.5,-1189.5 1603.5,-1189.5 1603.5,-1189.5 1597.5,-1189.5 1591.5,-1183.5 1591.5,-1177.5 1591.5,-1177.5 1591.5,-1148.5 1591.5,-1148.5 1591.5,-1142.5 1597.5,-1136.5 1603.5,-1136.5 1603.5,-1136.5 1753.5,-1136.5 1753.5,-1136.5 1759.5,-1136.5 1765.5,-1142.5 1765.5,-1148.5 1765.5,-1148.5 1765.5,-1177.5 1765.5,-1177.5 1765.5,-1183.5 1759.5,-1189.5 1753.5,-1189.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1599.5\" y=\"-1174.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.037</text>\n",
       "<text text-anchor=\"start\" x=\"1634.5\" y=\"-1159.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"1628.5\" y=\"-1144.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.578</text>\n",
       "</g>\n",
       "<!-- 89&#45;&gt;99 -->\n",
       "<g id=\"edge99\" class=\"edge\">\n",
       "<title>89&#45;&gt;99</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1645.41,-1232.88C1650.64,-1222 1656.47,-1209.86 1661.76,-1198.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1665.07,-1200.05 1666.24,-1189.52 1658.76,-1197.02 1665.07,-1200.05\"/>\n",
       "</g>\n",
       "<!-- 91 -->\n",
       "<g id=\"node92\" class=\"node\">\n",
       "<title>91</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1424,-1085.5C1424,-1085.5 1283,-1085.5 1283,-1085.5 1277,-1085.5 1271,-1079.5 1271,-1073.5 1271,-1073.5 1271,-1044.5 1271,-1044.5 1271,-1038.5 1277,-1032.5 1283,-1032.5 1283,-1032.5 1424,-1032.5 1424,-1032.5 1430,-1032.5 1436,-1038.5 1436,-1044.5 1436,-1044.5 1436,-1073.5 1436,-1073.5 1436,-1079.5 1430,-1085.5 1424,-1085.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1279\" y=\"-1070.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.13</text>\n",
       "<text text-anchor=\"start\" x=\"1309.5\" y=\"-1055.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"1303.5\" y=\"-1040.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.957</text>\n",
       "</g>\n",
       "<!-- 90&#45;&gt;91 -->\n",
       "<g id=\"edge91\" class=\"edge\">\n",
       "<title>90&#45;&gt;91</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1443.32,-1128.88C1427.83,-1117.01 1410.41,-1103.65 1395.06,-1091.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1396.84,-1088.83 1386.78,-1085.52 1392.58,-1094.38 1396.84,-1088.83\"/>\n",
       "</g>\n",
       "<!-- 92 -->\n",
       "<g id=\"node93\" class=\"node\">\n",
       "<title>92</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1607,-1093C1607,-1093 1466,-1093 1466,-1093 1460,-1093 1454,-1087 1454,-1081 1454,-1081 1454,-1037 1454,-1037 1454,-1031 1460,-1025 1466,-1025 1466,-1025 1607,-1025 1607,-1025 1613,-1025 1619,-1031 1619,-1037 1619,-1037 1619,-1081 1619,-1081 1619,-1087 1613,-1093 1607,-1093\"/>\n",
       "<text text-anchor=\"start\" x=\"1473.5\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar_1 â‰¤ 0.542</text>\n",
       "<text text-anchor=\"start\" x=\"1462\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.14</text>\n",
       "<text text-anchor=\"start\" x=\"1483.5\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n",
       "<text text-anchor=\"start\" x=\"1486.5\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.823</text>\n",
       "</g>\n",
       "<!-- 90&#45;&gt;92 -->\n",
       "<g id=\"edge92\" class=\"edge\">\n",
       "<title>90&#45;&gt;92</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1502.73,-1128.88C1506.88,-1120.42 1511.4,-1111.21 1515.74,-1102.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1518.92,-1103.82 1520.18,-1093.3 1512.63,-1100.74 1518.92,-1103.82\"/>\n",
       "</g>\n",
       "<!-- 93 -->\n",
       "<g id=\"node94\" class=\"node\">\n",
       "<title>93</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1419.5,-981.5C1419.5,-981.5 1269.5,-981.5 1269.5,-981.5 1263.5,-981.5 1257.5,-975.5 1257.5,-969.5 1257.5,-969.5 1257.5,-940.5 1257.5,-940.5 1257.5,-934.5 1263.5,-928.5 1269.5,-928.5 1269.5,-928.5 1419.5,-928.5 1419.5,-928.5 1425.5,-928.5 1431.5,-934.5 1431.5,-940.5 1431.5,-940.5 1431.5,-969.5 1431.5,-969.5 1431.5,-975.5 1425.5,-981.5 1419.5,-981.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1265.5\" y=\"-966.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.119</text>\n",
       "<text text-anchor=\"start\" x=\"1296\" y=\"-951.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n",
       "<text text-anchor=\"start\" x=\"1294.5\" y=\"-936.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.578</text>\n",
       "</g>\n",
       "<!-- 92&#45;&gt;93 -->\n",
       "<g id=\"edge93\" class=\"edge\">\n",
       "<title>92&#45;&gt;93</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1474.16,-1024.88C1450.77,-1012.46 1424.32,-998.41 1401.44,-986.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1403.01,-983.12 1392.54,-981.52 1399.73,-989.3 1403.01,-983.12\"/>\n",
       "</g>\n",
       "<!-- 94 -->\n",
       "<g id=\"node95\" class=\"node\">\n",
       "<title>94</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1611.5,-989C1611.5,-989 1461.5,-989 1461.5,-989 1455.5,-989 1449.5,-983 1449.5,-977 1449.5,-977 1449.5,-933 1449.5,-933 1449.5,-927 1455.5,-921 1461.5,-921 1461.5,-921 1611.5,-921 1611.5,-921 1617.5,-921 1623.5,-927 1623.5,-933 1623.5,-933 1623.5,-977 1623.5,-977 1623.5,-983 1617.5,-989 1611.5,-989\"/>\n",
       "<text text-anchor=\"start\" x=\"1463.5\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BusyTime_2 â‰¤ 0.511</text>\n",
       "<text text-anchor=\"start\" x=\"1457.5\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.115</text>\n",
       "<text text-anchor=\"start\" x=\"1488\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 75</text>\n",
       "<text text-anchor=\"start\" x=\"1486.5\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.921</text>\n",
       "</g>\n",
       "<!-- 92&#45;&gt;94 -->\n",
       "<g id=\"edge94\" class=\"edge\">\n",
       "<title>92&#45;&gt;94</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1536.5,-1024.88C1536.5,-1016.78 1536.5,-1007.98 1536.5,-999.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1540,-999.3 1536.5,-989.3 1533,-999.3 1540,-999.3\"/>\n",
       "</g>\n",
       "<!-- 95 -->\n",
       "<g id=\"node96\" class=\"node\">\n",
       "<title>95</title>\n",
       "<path fill=\"#fffffe\" stroke=\"black\" d=\"M1419.5,-877.5C1419.5,-877.5 1269.5,-877.5 1269.5,-877.5 1263.5,-877.5 1257.5,-871.5 1257.5,-865.5 1257.5,-865.5 1257.5,-836.5 1257.5,-836.5 1257.5,-830.5 1263.5,-824.5 1269.5,-824.5 1269.5,-824.5 1419.5,-824.5 1419.5,-824.5 1425.5,-824.5 1431.5,-830.5 1431.5,-836.5 1431.5,-836.5 1431.5,-865.5 1431.5,-865.5 1431.5,-871.5 1425.5,-877.5 1419.5,-877.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1265.5\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.066</text>\n",
       "<text text-anchor=\"start\" x=\"1296\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 68</text>\n",
       "<text text-anchor=\"start\" x=\"1294.5\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 5.865</text>\n",
       "</g>\n",
       "<!-- 94&#45;&gt;95 -->\n",
       "<g id=\"edge95\" class=\"edge\">\n",
       "<title>94&#45;&gt;95</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1474.16,-920.88C1450.77,-908.46 1424.32,-894.41 1401.44,-882.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1403.01,-879.12 1392.54,-877.52 1399.73,-885.3 1403.01,-879.12\"/>\n",
       "</g>\n",
       "<!-- 96 -->\n",
       "<g id=\"node97\" class=\"node\">\n",
       "<title>96</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1611.5,-885C1611.5,-885 1461.5,-885 1461.5,-885 1455.5,-885 1449.5,-879 1449.5,-873 1449.5,-873 1449.5,-829 1449.5,-829 1449.5,-823 1455.5,-817 1461.5,-817 1461.5,-817 1611.5,-817 1611.5,-817 1617.5,-817 1623.5,-823 1623.5,-829 1623.5,-829 1623.5,-873 1623.5,-873 1623.5,-879 1617.5,-885 1611.5,-885\"/>\n",
       "<text text-anchor=\"start\" x=\"1481.5\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RTTVar â‰¤ 0.897</text>\n",
       "<text text-anchor=\"start\" x=\"1457.5\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.268</text>\n",
       "<text text-anchor=\"start\" x=\"1492.5\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 7</text>\n",
       "<text text-anchor=\"start\" x=\"1486.5\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.462</text>\n",
       "</g>\n",
       "<!-- 94&#45;&gt;96 -->\n",
       "<g id=\"edge96\" class=\"edge\">\n",
       "<title>94&#45;&gt;96</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1536.5,-920.88C1536.5,-912.78 1536.5,-903.98 1536.5,-895.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1540,-895.3 1536.5,-885.3 1533,-895.3 1540,-895.3\"/>\n",
       "</g>\n",
       "<!-- 97 -->\n",
       "<g id=\"node98\" class=\"node\">\n",
       "<title>97</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1467.5,-773.5C1467.5,-773.5 1317.5,-773.5 1317.5,-773.5 1311.5,-773.5 1305.5,-767.5 1305.5,-761.5 1305.5,-761.5 1305.5,-732.5 1305.5,-732.5 1305.5,-726.5 1311.5,-720.5 1317.5,-720.5 1317.5,-720.5 1467.5,-720.5 1467.5,-720.5 1473.5,-720.5 1479.5,-726.5 1479.5,-732.5 1479.5,-732.5 1479.5,-761.5 1479.5,-761.5 1479.5,-767.5 1473.5,-773.5 1467.5,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1313.5\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.061</text>\n",
       "<text text-anchor=\"start\" x=\"1348.5\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"1342.5\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.166</text>\n",
       "</g>\n",
       "<!-- 96&#45;&gt;97 -->\n",
       "<g id=\"edge97\" class=\"edge\">\n",
       "<title>96&#45;&gt;97</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1489.75,-816.88C1472.82,-804.9 1453.77,-791.4 1437.04,-779.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1438.71,-776.44 1428.53,-773.52 1434.67,-782.16 1438.71,-776.44\"/>\n",
       "</g>\n",
       "<!-- 98 -->\n",
       "<g id=\"node99\" class=\"node\">\n",
       "<title>98</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1659.5,-773.5C1659.5,-773.5 1509.5,-773.5 1509.5,-773.5 1503.5,-773.5 1497.5,-767.5 1497.5,-761.5 1497.5,-761.5 1497.5,-732.5 1497.5,-732.5 1497.5,-726.5 1503.5,-720.5 1509.5,-720.5 1509.5,-720.5 1659.5,-720.5 1659.5,-720.5 1665.5,-720.5 1671.5,-726.5 1671.5,-732.5 1671.5,-732.5 1671.5,-761.5 1671.5,-761.5 1671.5,-767.5 1665.5,-773.5 1659.5,-773.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1505.5\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.017</text>\n",
       "<text text-anchor=\"start\" x=\"1540.5\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"1534.5\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.203</text>\n",
       "</g>\n",
       "<!-- 96&#45;&gt;98 -->\n",
       "<g id=\"edge98\" class=\"edge\">\n",
       "<title>96&#45;&gt;98</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1552.08,-816.88C1557.21,-806 1562.92,-793.86 1568.1,-782.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1571.4,-784.06 1572.49,-773.52 1565.07,-781.08 1571.4,-784.06\"/>\n",
       "</g>\n",
       "<!-- 103 -->\n",
       "<g id=\"node104\" class=\"node\">\n",
       "<title>103</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1917,-1613C1917,-1613 1750,-1613 1750,-1613 1744,-1613 1738,-1607 1738,-1601 1738,-1601 1738,-1557 1738,-1557 1738,-1551 1744,-1545 1750,-1545 1750,-1545 1917,-1545 1917,-1545 1923,-1545 1929,-1551 1929,-1557 1929,-1557 1929,-1601 1929,-1601 1929,-1607 1923,-1613 1917,-1613\"/>\n",
       "<text text-anchor=\"start\" x=\"1746\" y=\"-1597.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesRetrans_2 â‰¤ &#45;0.223</text>\n",
       "<text text-anchor=\"start\" x=\"1754.5\" y=\"-1582.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.674</text>\n",
       "<text text-anchor=\"start\" x=\"1780.5\" y=\"-1567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 155</text>\n",
       "<text text-anchor=\"start\" x=\"1788\" y=\"-1552.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.53</text>\n",
       "</g>\n",
       "<!-- 102&#45;&gt;103 -->\n",
       "<g id=\"edge103\" class=\"edge\">\n",
       "<title>102&#45;&gt;103</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1833.5,-1648.88C1833.5,-1640.78 1833.5,-1631.98 1833.5,-1623.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1837,-1623.3 1833.5,-1613.3 1830,-1623.3 1837,-1623.3\"/>\n",
       "</g>\n",
       "<!-- 110 -->\n",
       "<g id=\"node111\" class=\"node\">\n",
       "<title>110</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M2109.5,-1605.5C2109.5,-1605.5 1959.5,-1605.5 1959.5,-1605.5 1953.5,-1605.5 1947.5,-1599.5 1947.5,-1593.5 1947.5,-1593.5 1947.5,-1564.5 1947.5,-1564.5 1947.5,-1558.5 1953.5,-1552.5 1959.5,-1552.5 1959.5,-1552.5 2109.5,-1552.5 2109.5,-1552.5 2115.5,-1552.5 2121.5,-1558.5 2121.5,-1564.5 2121.5,-1564.5 2121.5,-1593.5 2121.5,-1593.5 2121.5,-1599.5 2115.5,-1605.5 2109.5,-1605.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1955.5\" y=\"-1590.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.475</text>\n",
       "<text text-anchor=\"start\" x=\"1986\" y=\"-1575.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 19</text>\n",
       "<text text-anchor=\"start\" x=\"1984.5\" y=\"-1560.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 8.362</text>\n",
       "</g>\n",
       "<!-- 102&#45;&gt;110 -->\n",
       "<g id=\"edge110\" class=\"edge\">\n",
       "<title>102&#45;&gt;110</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1898.76,-1648.88C1923.36,-1636.4 1951.18,-1622.28 1975.2,-1610.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1976.88,-1613.17 1984.21,-1605.52 1973.71,-1606.92 1976.88,-1613.17\"/>\n",
       "</g>\n",
       "<!-- 104 -->\n",
       "<g id=\"node105\" class=\"node\">\n",
       "<title>104</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M1908.5,-1501.5C1908.5,-1501.5 1758.5,-1501.5 1758.5,-1501.5 1752.5,-1501.5 1746.5,-1495.5 1746.5,-1489.5 1746.5,-1489.5 1746.5,-1460.5 1746.5,-1460.5 1746.5,-1454.5 1752.5,-1448.5 1758.5,-1448.5 1758.5,-1448.5 1908.5,-1448.5 1908.5,-1448.5 1914.5,-1448.5 1920.5,-1454.5 1920.5,-1460.5 1920.5,-1460.5 1920.5,-1489.5 1920.5,-1489.5 1920.5,-1495.5 1914.5,-1501.5 1908.5,-1501.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1754.5\" y=\"-1486.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.687</text>\n",
       "<text text-anchor=\"start\" x=\"1780.5\" y=\"-1471.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 116</text>\n",
       "<text text-anchor=\"start\" x=\"1783.5\" y=\"-1456.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.361</text>\n",
       "</g>\n",
       "<!-- 103&#45;&gt;104 -->\n",
       "<g id=\"edge104\" class=\"edge\">\n",
       "<title>103&#45;&gt;104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1833.5,-1544.88C1833.5,-1534.33 1833.5,-1522.6 1833.5,-1511.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1837,-1511.52 1833.5,-1501.52 1830,-1511.52 1837,-1511.52\"/>\n",
       "</g>\n",
       "<!-- 105 -->\n",
       "<g id=\"node106\" class=\"node\">\n",
       "<title>105</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M2100.5,-1509C2100.5,-1509 1950.5,-1509 1950.5,-1509 1944.5,-1509 1938.5,-1503 1938.5,-1497 1938.5,-1497 1938.5,-1453 1938.5,-1453 1938.5,-1447 1944.5,-1441 1950.5,-1441 1950.5,-1441 2100.5,-1441 2100.5,-1441 2106.5,-1441 2112.5,-1447 2112.5,-1453 2112.5,-1453 2112.5,-1497 2112.5,-1497 2112.5,-1503 2106.5,-1509 2100.5,-1509\"/>\n",
       "<text text-anchor=\"start\" x=\"1951.5\" y=\"-1493.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BytesAcked â‰¤ &#45;0.441</text>\n",
       "<text text-anchor=\"start\" x=\"1946.5\" y=\"-1478.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.298</text>\n",
       "<text text-anchor=\"start\" x=\"1977\" y=\"-1463.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 39</text>\n",
       "<text text-anchor=\"start\" x=\"1975.5\" y=\"-1448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.031</text>\n",
       "</g>\n",
       "<!-- 103&#45;&gt;105 -->\n",
       "<g id=\"edge105\" class=\"edge\">\n",
       "<title>103&#45;&gt;105</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1895.84,-1544.88C1914.46,-1534.99 1935.02,-1524.07 1954.17,-1513.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1955.96,-1516.91 1963.15,-1509.12 1952.67,-1510.73 1955.96,-1516.91\"/>\n",
       "</g>\n",
       "<!-- 106 -->\n",
       "<g id=\"node107\" class=\"node\">\n",
       "<title>106</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M2002.5,-1397.5C2002.5,-1397.5 1852.5,-1397.5 1852.5,-1397.5 1846.5,-1397.5 1840.5,-1391.5 1840.5,-1385.5 1840.5,-1385.5 1840.5,-1356.5 1840.5,-1356.5 1840.5,-1350.5 1846.5,-1344.5 1852.5,-1344.5 1852.5,-1344.5 2002.5,-1344.5 2002.5,-1344.5 2008.5,-1344.5 2014.5,-1350.5 2014.5,-1356.5 2014.5,-1356.5 2014.5,-1385.5 2014.5,-1385.5 2014.5,-1391.5 2008.5,-1397.5 2002.5,-1397.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1848.5\" y=\"-1382.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.147</text>\n",
       "<text text-anchor=\"start\" x=\"1879\" y=\"-1367.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\n",
       "<text text-anchor=\"start\" x=\"1877.5\" y=\"-1352.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.524</text>\n",
       "</g>\n",
       "<!-- 105&#45;&gt;106 -->\n",
       "<g id=\"edge106\" class=\"edge\">\n",
       "<title>105&#45;&gt;106</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1993.68,-1440.88C1982.59,-1429.34 1970.15,-1416.39 1959.07,-1404.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1961.47,-1402.31 1952.02,-1397.52 1956.42,-1407.16 1961.47,-1402.31\"/>\n",
       "</g>\n",
       "<!-- 107 -->\n",
       "<g id=\"node108\" class=\"node\">\n",
       "<title>107</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M2206,-1405C2206,-1405 2045,-1405 2045,-1405 2039,-1405 2033,-1399 2033,-1393 2033,-1393 2033,-1349 2033,-1349 2033,-1343 2039,-1337 2045,-1337 2045,-1337 2206,-1337 2206,-1337 2212,-1337 2218,-1343 2218,-1349 2218,-1349 2218,-1393 2218,-1393 2218,-1399 2212,-1405 2206,-1405\"/>\n",
       "<text text-anchor=\"start\" x=\"2041\" y=\"-1389.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">RWndLimited_3 â‰¤ 0.036</text>\n",
       "<text text-anchor=\"start\" x=\"2051\" y=\"-1374.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.18</text>\n",
       "<text text-anchor=\"start\" x=\"2077\" y=\"-1359.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 26</text>\n",
       "<text text-anchor=\"start\" x=\"2075.5\" y=\"-1344.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.284</text>\n",
       "</g>\n",
       "<!-- 105&#45;&gt;107 -->\n",
       "<g id=\"edge107\" class=\"edge\">\n",
       "<title>105&#45;&gt;107</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2057.97,-1440.88C2066.79,-1431.89 2076.44,-1422.04 2085.62,-1412.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2088.35,-1414.89 2092.85,-1405.3 2083.35,-1409.99 2088.35,-1414.89\"/>\n",
       "</g>\n",
       "<!-- 108 -->\n",
       "<g id=\"node109\" class=\"node\">\n",
       "<title>108</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M2104.5,-1293.5C2104.5,-1293.5 1954.5,-1293.5 1954.5,-1293.5 1948.5,-1293.5 1942.5,-1287.5 1942.5,-1281.5 1942.5,-1281.5 1942.5,-1252.5 1942.5,-1252.5 1942.5,-1246.5 1948.5,-1240.5 1954.5,-1240.5 1954.5,-1240.5 2104.5,-1240.5 2104.5,-1240.5 2110.5,-1240.5 2116.5,-1246.5 2116.5,-1252.5 2116.5,-1252.5 2116.5,-1281.5 2116.5,-1281.5 2116.5,-1287.5 2110.5,-1293.5 2104.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"1950.5\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.036</text>\n",
       "<text text-anchor=\"start\" x=\"1981\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12</text>\n",
       "<text text-anchor=\"start\" x=\"1979.5\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 6.967</text>\n",
       "</g>\n",
       "<!-- 107&#45;&gt;108 -->\n",
       "<g id=\"edge108\" class=\"edge\">\n",
       "<title>107&#45;&gt;108</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2094.33,-1336.88C2083.46,-1325.34 2071.28,-1312.39 2060.43,-1300.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2062.92,-1298.4 2053.52,-1293.52 2057.82,-1303.2 2062.92,-1298.4\"/>\n",
       "</g>\n",
       "<!-- 109 -->\n",
       "<g id=\"node110\" class=\"node\">\n",
       "<title>109</title>\n",
       "<path fill=\"#fffefe\" stroke=\"black\" d=\"M2296.5,-1293.5C2296.5,-1293.5 2146.5,-1293.5 2146.5,-1293.5 2140.5,-1293.5 2134.5,-1287.5 2134.5,-1281.5 2134.5,-1281.5 2134.5,-1252.5 2134.5,-1252.5 2134.5,-1246.5 2140.5,-1240.5 2146.5,-1240.5 2146.5,-1240.5 2296.5,-1240.5 2296.5,-1240.5 2302.5,-1240.5 2308.5,-1246.5 2308.5,-1252.5 2308.5,-1252.5 2308.5,-1281.5 2308.5,-1281.5 2308.5,-1287.5 2302.5,-1293.5 2296.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"2142.5\" y=\"-1278.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.145</text>\n",
       "<text text-anchor=\"start\" x=\"2173\" y=\"-1263.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 14</text>\n",
       "<text text-anchor=\"start\" x=\"2171.5\" y=\"-1248.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 7.556</text>\n",
       "</g>\n",
       "<!-- 107&#45;&gt;109 -->\n",
       "<g id=\"edge109\" class=\"edge\">\n",
       "<title>107&#45;&gt;109</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2156.67,-1336.88C2167.54,-1325.34 2179.72,-1312.39 2190.57,-1300.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2193.18,-1303.2 2197.48,-1293.52 2188.08,-1298.4 2193.18,-1303.2\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7f24bb2f1db0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(\n",
    "    pruned_dt,\n",
    "    feature_names=features.columns,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "display(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
